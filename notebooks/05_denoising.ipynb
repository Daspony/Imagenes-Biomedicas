{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Denoising en CT - LDCT a NDCT\n",
    "\n",
    "Este notebook implementa tecnicas de reduccion de ruido (denoising) en imagenes CT pulmonares.\n",
    "\n",
    "## Contenido:\n",
    "1. **Dataset Mayo Clinic (TCIA)**: Pares reales NDCT/LDCT (descarga automatica)\n",
    "2. **Alternativa**: Simulacion de ruido LDCT desde LUNA16\n",
    "3. **Tecnicas de Denoising**: Filtros clasicos vs Deep Learning (DnCNN)\n",
    "4. **Metricas de Calidad**: PSNR, SSIM, MSE\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets\n",
    "\n",
    "| Dataset | Tipo | Tamano | Descarga |\n",
    "|---------|------|--------|----------|\n",
    "| **Mayo Clinic LDCT** | Pares reales NDCT/LDCT | ~200 GB (Chest) | Automatica via tcia_utils |\n",
    "| **LUNA16** | Solo NDCT (simular ruido) | ~13 GB | Automatica |\n",
    "\n",
    "**Flujo del notebook:**\n",
    "1. Intentar descargar/usar datos reales Mayo Clinic\n",
    "2. Si no disponible, usar LUNA16 con ruido simulado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion del entorno\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACION\n",
    "# ============================================================\n",
    "USE_MAYO_PROJECTIONS = True  # Usar proyecciones/sinogramas Mayo\n",
    "USE_MAYO_IMAGES = True       # Usar imagenes CT reconstruidas Mayo\n",
    "MAX_MAYO_PATIENTS = 5        # Pacientes a usar (None = todos)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Ejecutando en Google Colab\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Instalar dependencias\n",
    "    import subprocess\n",
    "    paquetes = ['SimpleITK', 'scikit-image', 'tcia_utils', 'pydicom']\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + paquetes)\n",
    "\n",
    "    # Clonar repositorio\n",
    "    repo_url = \"https://github.com/Daspony/Imagenes-Biomedicas.git\"\n",
    "    if not os.path.exists(\"/content/Imagenes-Biomedicas\"):\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], cwd=\"/content\", check=True)\n",
    "    sys.path.insert(0, \"/content/Imagenes-Biomedicas\")\n",
    "\n",
    "    project_root = \"/content/Imagenes-Biomedicas\"\n",
    "\n",
    "    # Montar Drive para guardar datos y modelos\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Rutas en Drive (persistentes)\n",
    "    drive_data = \"/content/drive/MyDrive/CT_Denoising\"\n",
    "    os.makedirs(drive_data, exist_ok=True)\n",
    "\n",
    "    # Rutas Mayo separadas\n",
    "    MAYO_PROJ_PATH = os.path.join(drive_data, \"Mayo_LDCT\")    # Proyecciones/sinogramas\n",
    "    MAYO_IMG_PATH = os.path.join(drive_data, \"Mayo_Images\")   # Imagenes CT reconstruidas\n",
    "    weights_dir = os.path.join(drive_data, \"weights\")\n",
    "\n",
    "    # LUNA16 para fallback\n",
    "    LUNA16_PATH = os.path.join(project_root, \"LUNA16\")\n",
    "\n",
    "    print(f\"[OK] Datos se guardaran en: {drive_data}\")\n",
    "else:\n",
    "    print(\"Ejecutando localmente\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Instalar tcia_utils si no esta\n",
    "    try:\n",
    "        import tcia_utils\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tcia_utils\", \"pydicom\"])\n",
    "\n",
    "    parent_dir = os.path.abspath('..')\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.insert(0, parent_dir)\n",
    "\n",
    "    project_root = parent_dir\n",
    "\n",
    "    # Rutas Mayo separadas (local)\n",
    "    MAYO_PROJ_PATH = os.path.join(project_root, \"Mayo_LDCT\")\n",
    "    MAYO_IMG_PATH = os.path.join(project_root, \"Mayo_Images\")\n",
    "\n",
    "    LUNA16_PATH = os.path.join(project_root, \"LUNA16\")\n",
    "    weights_dir = os.path.join(project_root, \"weights\")\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# Verificar disponibilidad de cada dataset\n",
    "MAYO_PROJ_AVAILABLE = os.path.exists(MAYO_PROJ_PATH) and any(Path(MAYO_PROJ_PATH).rglob(\"*.dcm\"))\n",
    "MAYO_IMG_AVAILABLE = os.path.exists(MAYO_IMG_PATH) and any(Path(MAYO_IMG_PATH).rglob(\"*.dcm\"))\n",
    "\n",
    "# Para compatibilidad, usar ruido simulado si no hay datos reales\n",
    "USE_SIMULATED_NOISE = not (MAYO_PROJ_AVAILABLE or MAYO_IMG_AVAILABLE)\n",
    "\n",
    "print(f\"\\n[INFO] Estado de datos:\")\n",
    "print(f\"  - Proyecciones Mayo: {'Disponible' if MAYO_PROJ_AVAILABLE else 'No disponible'}\")\n",
    "print(f\"  - Imagenes CT Mayo: {'Disponible' if MAYO_IMG_AVAILABLE else 'No disponible'}\")\n",
    "print(f\"  - Ruido simulado: {'Activo' if USE_SIMULATED_NOISE else 'No necesario'}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DESCARGA AUTOMATICA - MAYO CLINIC LDCT (TCIA)\n",
    "# ============================================================\n",
    "from tcia_utils import nbia\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_mayo_ldct(download_path, body_part=\"CHEST\", max_patients=5):\n",
    "    \"\"\"\n",
    "    Descarga pares NDCT/LDCT del dataset Mayo Clinic (TCIA)\n",
    "    Con barra de progreso para monitorear la descarga.\n",
    "    \"\"\"\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    \n",
    "    # Verificar si ya hay datos\n",
    "    existing_files = list(Path(download_path).rglob(\"*.dcm\"))\n",
    "    if len(existing_files) > 100:\n",
    "        print(f\"[OK] Dataset Mayo ya existe: {len(existing_files)} archivos DICOM\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"[INFO] Buscando series de {body_part} en TCIA...\")\n",
    "    \n",
    "    try:\n",
    "        # Obtener series del dataset Mayo LDCT\n",
    "        series = nbia.getSeries(collection=\"LDCT-and-Projection-data\")\n",
    "        \n",
    "        if series is None or len(series) == 0:\n",
    "            print(\"[WARNING] No se encontraron series en TCIA\")\n",
    "            return False\n",
    "        \n",
    "        # Convertir a DataFrame para filtrar\n",
    "        df = pd.DataFrame(series)\n",
    "        print(f\"[INFO] Series totales: {len(df)}\")\n",
    "        \n",
    "        # Filtrar por body part\n",
    "        if 'BodyPartExamined' in df.columns:\n",
    "            df_filtered = df[df['BodyPartExamined'].str.upper() == body_part.upper()]\n",
    "            print(f\"[INFO] Series de {body_part}: {len(df_filtered)}\")\n",
    "        else:\n",
    "            df_filtered = df\n",
    "        \n",
    "        if len(df_filtered) == 0:\n",
    "            print(f\"[WARNING] No hay series de {body_part}\")\n",
    "            return False\n",
    "        \n",
    "        # Limitar numero de pacientes\n",
    "        if max_patients and 'PatientID' in df_filtered.columns:\n",
    "            unique_patients = df_filtered['PatientID'].unique()[:max_patients]\n",
    "            df_filtered = df_filtered[df_filtered['PatientID'].isin(unique_patients)]\n",
    "            print(f\"[INFO] Limitando a {len(unique_patients)} pacientes ({len(df_filtered)} series)\")\n",
    "        \n",
    "        # Descargar serie por serie con progreso\n",
    "        print(f\"\\n[INFO] Descargando {len(df_filtered)} series...\")\n",
    "        series_list = df_filtered.to_dict('records')\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        \n",
    "        for i, serie in enumerate(tqdm(series_list, desc=\"Descargando series\")):\n",
    "            try:\n",
    "                # Descargar una serie a la vez\n",
    "                nbia.downloadSeries(\n",
    "                    [serie],  # Lista con una sola serie\n",
    "                    path=download_path,\n",
    "                    csv_filename=None  # No crear CSV por cada serie\n",
    "                )\n",
    "                successful += 1\n",
    "            except Exception as e:\n",
    "                failed += 1\n",
    "                print(f\"\\n[WARNING] Error en serie {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"\\n[OK] Descarga completada: {successful} exitosas, {failed} fallidas\")\n",
    "        print(f\"[OK] Datos en: {download_path}\")\n",
    "        return successful > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error descargando Mayo: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# INTENTAR OBTENER DATOS MAYO\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET MAYO CLINIC LDCT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "MAYO_AVAILABLE = False\n",
    "\n",
    "if USE_MAYO_DATA:\n",
    "    # Verificar si ya existe\n",
    "    existing_dcm = list(Path(MAYO_PATH).rglob(\"*.dcm\")) if os.path.exists(MAYO_PATH) else []\n",
    "    \n",
    "    if len(existing_dcm) > 100:\n",
    "        MAYO_AVAILABLE = True\n",
    "        print(f\"[OK] Mayo LDCT encontrado: {len(existing_dcm)} archivos DICOM\")\n",
    "    else:\n",
    "        print(\"[INFO] Intentando descargar Mayo LDCT...\")\n",
    "        MAYO_AVAILABLE = download_mayo_ldct(\n",
    "            download_path=MAYO_PATH,\n",
    "            body_part=\"CHEST\",\n",
    "            max_patients=MAX_MAYO_PATIENTS\n",
    "        )\n",
    "else:\n",
    "    print(\"[INFO] USE_MAYO_DATA=False, usando ruido simulado\")\n",
    "\n",
    "print(f\"\\nMayo LDCT disponible: {MAYO_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pydicom\n",
    "\n",
    "# Procesamiento de imagenes\n",
    "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral, denoise_nl_means\n",
    "from skimage.filters import gaussian\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Importar modulos del proyecto\n",
    "from utils import LUNA16DataLoader, download_luna16\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURAR FUENTE DE DATOS\n",
    "# ============================================================\n",
    "USE_SIMULATED_NOISE = not MAYO_AVAILABLE\n",
    "\n",
    "if MAYO_AVAILABLE:\n",
    "    print(\"\\n[OK] Usando datos REALES de Mayo Clinic (NDCT/LDCT)\")\n",
    "else:\n",
    "    print(\"\\n[INFO] Mayo no disponible - Usando LUNA16 con ruido SIMULADO\")\n",
    "    \n",
    "    # Configurar LUNA16\n",
    "    DATA_PATH = os.path.join(LUNA16_PATH, 'subset0')\n",
    "    ANNOTATIONS_PATH = os.path.join(LUNA16_PATH, 'annotations.csv')\n",
    "    \n",
    "    # Descargar si no existe\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(\"[INFO] Descargando LUNA16 subset0...\")\n",
    "        download_luna16(subsets=[0], download_dir=LUNA16_PATH)\n",
    "    \n",
    "    mhd_count = len(list(Path(DATA_PATH).glob(\"*.mhd\"))) if os.path.exists(DATA_PATH) else 0\n",
    "    print(f\"  LUNA16 path: {DATA_PATH}\")\n",
    "    print(f\"  Scans disponibles: {mhd_count}\")\n",
    "\n",
    "print(\"\\nLibrerias importadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGAR DATOS DE EJEMPLO\n",
    "# ============================================================\n",
    "\n",
    "if USE_SIMULATED_NOISE:\n",
    "    # Cargar un escaneo de LUNA16 para demo\n",
    "    loader = LUNA16DataLoader(DATA_PATH, ANNOTATIONS_PATH)\n",
    "    mhd_files = list(Path(DATA_PATH).glob(\"*.mhd\"))\n",
    "    print(f\"Escaneos LUNA16 disponibles: {len(mhd_files)}\")\n",
    "    \n",
    "    # Cargar escaneo\n",
    "    sample_path = str(mhd_files[0])\n",
    "    ct_scan, origin, spacing = loader.load_itk_image(sample_path)\n",
    "    print(f\"Volumen cargado: {ct_scan.shape}\")\n",
    "    print(f\"Spacing: {spacing}\")\n",
    "    \n",
    "    # Seleccionar un slice central\n",
    "    slice_idx = ct_scan.shape[0] // 2\n",
    "    original_slice = ct_scan[slice_idx].copy()\n",
    "    print(f\"Slice seleccionado: {slice_idx}\")\n",
    "    print(f\"Rango HU: [{original_slice.min():.0f}, {original_slice.max():.0f}]\")\n",
    "else:\n",
    "    # Cargar un escaneo de Mayo para demo (se hara en celdas posteriores)\n",
    "    print(\"[INFO] Datos Mayo se cargaran en seccion dedicada\")\n",
    "    original_slice = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modelo DnCNN y Funciones Comunes\n",
    "\n",
    "Estas definiciones son compartidas por ambas partes (ruido simulado y datos reales).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Arquitectura DnCNN\n\n**DnCNN** (Denoising Convolutional Neural Network) es una arquitectura clásica para denoising:\n\n- Aprende el **residuo** (ruido) en lugar de la imagen limpia\n- Usa batch normalization y ReLU\n- Típicamente 17-20 capas convolucionales\n\n$$\\hat{x} = y - f(y; \\theta)$$\n\ndonde $y$ es la imagen ruidosa, $f$ predice el ruido, y $\\hat{x}$ es la imagen denoised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DnCNN para denoising de imágenes CT\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Conv + ReLU (primera capa)\n",
    "    - (Conv + BN + ReLU) x (depth-2) capas intermedias\n",
    "    - Conv (última capa)\n",
    "    \n",
    "    El modelo predice el residuo (ruido) que se resta de la entrada.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_features=64, depth=17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Primera capa: Conv + ReLU\n",
    "        layers.append(nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Capas intermedias: Conv + BN + ReLU\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(num_features))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Última capa: Conv (sin activación)\n",
    "        layers.append(nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "        \n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Predice el residuo y lo resta de la entrada\"\"\"\n",
    "        residual = self.dncnn(x)\n",
    "        return x - residual  # Imagen denoised\n",
    "\n",
    "\n",
    "# Crear modelo\n",
    "model_dncnn = DnCNN(in_channels=1, out_channels=1, num_features=64, depth=17).to(device)\n",
    "\n",
    "print(\"DnCNN creado\")\n",
    "print(f\"Parámetros totales: {sum(p.numel() for p in model_dncnn.parameters()):,}\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Funcion de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_denoiser(model, dataloader, epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Entrena el modelo de denoising\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo DnCNN\n",
    "        dataloader: DataLoader con pares (ruidosa, limpia)\n",
    "        epochs: Número de épocas\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        history: Diccionario con historial de entrenamiento\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    history = {'loss': [], 'psnr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_psnr = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f'Época {epoch+1}/{epochs}')\n",
    "        \n",
    "        for noisy, clean in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            denoised = model(noisy)\n",
    "            loss = criterion(denoised, clean)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Métricas\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calcular PSNR\n",
    "            with torch.no_grad():\n",
    "                mse = F.mse_loss(denoised, clean)\n",
    "                batch_psnr = 10 * torch.log10(1.0 / mse)\n",
    "                epoch_psnr += batch_psnr.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}', 'PSNR': f'{batch_psnr.item():.2f}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_psnr = epoch_psnr / num_batches\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['psnr'].append(avg_psnr)\n",
    "        \n",
    "        print(f'Época {epoch+1}: Loss = {avg_loss:.6f}, PSNR = {avg_psnr:.2f} dB')\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Función de entrenamiento definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Funcion de Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_denoiser(model, image_clean_hu, dose_ratio=0.25, I0=1e5):\n",
    "    \"\"\"\n",
    "    Evalua el modelo de denoising en una imagen.\n",
    "    Usa simulacion de ruido en dominio del sinograma (Yu et al. 2012).\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        image_clean_hu: Imagen limpia en valores HU\n",
    "        dose_ratio: Ratio de dosis para simular LDCT\n",
    "        I0: Fotones incidentes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados con imagenes y metricas\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Simular LDCT en dominio del sinograma\n",
    "    noisy_hu = add_ldct_noise_yu2012(image_clean_hu, dose_ratio=dose_ratio, I0=I0)\n",
    "    \n",
    "    # Normalizar para el modelo\n",
    "    def normalize(img):\n",
    "        img_clipped = np.clip(img, -1000, 400)\n",
    "        return (img_clipped - (-1000)) / (400 - (-1000))\n",
    "    \n",
    "    clean = normalize(image_clean_hu)\n",
    "    noisy = normalize(noisy_hu)\n",
    "    \n",
    "    # Preparar tensor\n",
    "    noisy_tensor = torch.FloatTensor(noisy).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inferencia\n",
    "    with torch.no_grad():\n",
    "        denoised_tensor = model(noisy_tensor)\n",
    "    \n",
    "    denoised = denoised_tensor.squeeze().cpu().numpy()\n",
    "    denoised = np.clip(denoised, 0, 1)\n",
    "    \n",
    "    # Calcular metricas\n",
    "    psnr_noisy = psnr(clean, noisy, data_range=1.0)\n",
    "    psnr_denoised = psnr(clean, denoised, data_range=1.0)\n",
    "    \n",
    "    ssim_noisy = ssim(clean, noisy, data_range=1.0)\n",
    "    ssim_denoised = ssim(clean, denoised, data_range=1.0)\n",
    "    \n",
    "    return {\n",
    "        'clean': clean,\n",
    "        'noisy': noisy,\n",
    "        'denoised': denoised,\n",
    "        'psnr_noisy': psnr_noisy,\n",
    "        'psnr_denoised': psnr_denoised,\n",
    "        'ssim_noisy': ssim_noisy,\n",
    "        'ssim_denoised': ssim_denoised,\n",
    "        'psnr_gain': psnr_denoised - psnr_noisy,\n",
    "        'ssim_gain': ssim_denoised - ssim_noisy\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion de evaluacion definida (usa ruido en sinograma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE A: Ruido Simulado \n",
    "Esta seccion permite trabajar con **ruido sintetico** simulado en el dominio del sinograma.\n",
    "Util para desarrollo y pruebas sin necesidad de descargar datasets externos.\n",
    "\n",
    "**Contenido:**\n",
    "- Modelo de ruido CT (Yu et al. 2012)\n",
    "- Variacion de niveles de dosis\n",
    "- Tecnicas clasicas de denoising\n",
    "- DnCNN con ruido simulado\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Modelo de Ruido en CT (Yu et al. 2012)\n",
    "\n",
    "El ruido en CT se origina en las **proyecciones** (sinograma), no en la imagen reconstruida.\n",
    "\n",
    "### Modelo Fisico\n",
    "\n",
    "Para un rayo de rayos X que atraviesa el objeto:\n",
    "\n",
    "$$P = -\\ln\\left(\\frac{N}{N_0}\\right) = \\int \\mu(x) \\, dx$$\n",
    "\n",
    "Donde:\n",
    "- $N_0$ = fotones incidentes\n",
    "- $N$ = fotones detectados (sigue distribucion Poisson)\n",
    "- $P$ = proyeccion (line integral de atenuacion)\n",
    "- $\\mu$ = coeficiente de atenuacion\n",
    "\n",
    "### Simulacion de Dosis Reducida (Eq. 6 del paper)\n",
    "\n",
    "Para simular una imagen con dosis reducida $a$ (ej: $a=0.25$ para 25%):\n",
    "\n",
    "$$\\tilde{P}_B \\approx P_A + \\sqrt{\\frac{1-a}{a} \\cdot \\frac{\\exp(P_A)}{N_{0A}}} \\cdot x$$\n",
    "\n",
    "Donde $x \\sim N(0,1)$ es ruido gaussiano.\n",
    "\n",
    "### Con Ruido Electronico (Eq. 11)\n",
    "\n",
    "$$\\tilde{P}_B \\approx P_A + \\sqrt{\\frac{1-a}{a} \\cdot \\frac{\\exp(P_A)}{N_{0A}} \\cdot \\left(1 + \\frac{1+a}{a} \\cdot \\frac{N_e \\cdot \\exp(P_A)}{N_{0A}}\\right)} \\cdot x$$\n",
    "\n",
    "### Pipeline de Simulacion\n",
    "\n",
    "```\n",
    "Imagen NDCT -> Sinograma (Radon) -> +Ruido (Eq.6) -> Reconstruccion (FBP) -> Imagen LDCT\n",
    "```\n",
    "\n",
    "**Referencia**: Yu L, et al. \"Development and Validation of a Practical Lower-Dose-Simulation Tool for Optimizing CT Scan Protocols\". J Comput Assist Tomogr 2012;36:477-487."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMULACION DE RUIDO LDCT - FIEL AL PAPER YU ET AL. 2012\n",
    "# ============================================================\n",
    "# Referencia: Yu et al. \"Development and Validation of a Practical \n",
    "# Lower-Dose-Simulation Tool for Optimizing CT Scan Protocols\"\n",
    "# J Comput Assist Tomogr 2012;36:477-487\n",
    "#\n",
    "# El ruido se simula en el dominio del SINOGRAMA, no en la imagen.\n",
    "# ============================================================\n",
    "\n",
    "from skimage.transform import radon, iradon\n",
    "\n",
    "def image_to_sinogram(image, theta=None):\n",
    "    \"\"\"\n",
    "    Convierte imagen CT a sinograma usando transformada de Radon.\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen CT 2D (valores HU o normalizados)\n",
    "        theta: Angulos de proyeccion (default: 0-180 grados)\n",
    "    \n",
    "    Returns:\n",
    "        sinogram: Sinograma (proyecciones)\n",
    "        theta: Angulos utilizados\n",
    "    \"\"\"\n",
    "    if theta is None:\n",
    "        theta = np.linspace(0., 180., max(image.shape), endpoint=False)\n",
    "    \n",
    "    sinogram = radon(image, theta=theta, circle=True)\n",
    "    return sinogram, theta\n",
    "\n",
    "\n",
    "def sinogram_to_image(sinogram, theta, output_size=None):\n",
    "    \"\"\"\n",
    "    Reconstruye imagen desde sinograma usando retroproyeccion filtrada.\n",
    "    \n",
    "    Args:\n",
    "        sinogram: Sinograma\n",
    "        theta: Angulos de proyeccion\n",
    "        output_size: Tamano de salida (default: automatico)\n",
    "    \n",
    "    Returns:\n",
    "        image: Imagen reconstruida\n",
    "    \"\"\"\n",
    "    image = iradon(sinogram, theta=theta, output_size=output_size, circle=True)\n",
    "    return image\n",
    "\n",
    "\n",
    "def add_noise_sinogram_domain(image, dose_ratio=0.25, I0=1e5, include_electronic=True, Ne=8.2):\n",
    "    \"\"\"\n",
    "    Simula LDCT anadiendo ruido en el dominio del sinograma.\n",
    "    Implementacion fiel al paper Yu et al. 2012 (Eq. 6 y 11).\n",
    "    \n",
    "    El modelo de ruido es:\n",
    "        P_B = P_A + sqrt((1-a)/a * exp(P_A)/N0_A) * x      (Eq. 6)\n",
    "    \n",
    "    Con ruido electronico (Eq. 11):\n",
    "        P_B = P_A + sqrt((1-a)/a * exp(P_A)/N0_A * (1 + (1+a)/a * Ne*exp(P_A)/N0_A)) * x\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen NDCT original (valores HU)\n",
    "        dose_ratio: Ratio de dosis simulada (a). Ej: 0.25 = 25% de dosis original\n",
    "        I0: Numero de fotones incidentes (N0) - parametro del escaner\n",
    "        include_electronic: Incluir ruido electronico\n",
    "        Ne: Ruido electronico equivalente (calibrado en paper como ~8.2)\n",
    "    \n",
    "    Returns:\n",
    "        noisy_image: Imagen LDCT simulada\n",
    "        sinogram_noisy: Sinograma con ruido (para visualizacion)\n",
    "        sinogram_clean: Sinograma original (para visualizacion)\n",
    "    \"\"\"\n",
    "    # 1. Normalizar imagen a coeficientes de atenuacion positivos\n",
    "    #    HU = 1000 * (mu - mu_water) / mu_water\n",
    "    #    => mu/mu_water = HU/1000 + 1\n",
    "    #    Usamos valores positivos para el sinograma\n",
    "    mu_water = 0.02  # cm^-1 aproximado para agua a ~70 keV\n",
    "    \n",
    "    # Convertir HU a coeficientes de atenuacion relativos (positivos)\n",
    "    # Asumimos que la imagen esta en HU\n",
    "    image_hu = image.copy()\n",
    "    \n",
    "    # Escalar a valores de atenuacion (0 = aire, 1 = agua, >1 = hueso)\n",
    "    image_atten = (image_hu / 1000.0) + 1.0\n",
    "    image_atten = np.clip(image_atten, 0.001, None)  # Evitar valores negativos/cero\n",
    "    \n",
    "    # 2. Calcular sinograma (transformada de Radon)\n",
    "    #    El sinograma representa la integral de linea de atenuacion\n",
    "    sinogram_clean, theta = image_to_sinogram(image_atten)\n",
    "    \n",
    "    # 3. Convertir a proyecciones P = -ln(N/N0) = integral de mu\n",
    "    #    En realidad, radon ya nos da la integral, que es proporcional a P\n",
    "    #    P = sum(mu * dx) a lo largo del rayo\n",
    "    P_A = sinogram_clean.copy()\n",
    "    \n",
    "    # Escalar para que los valores sean realistas\n",
    "    # (el sinograma de radon no tiene unidades fisicas exactas)\n",
    "    P_A = P_A * 0.01  # Factor de escala empirico\n",
    "    \n",
    "    # 4. Calcular N0_A (fotones incidentes) para cada detector\n",
    "    #    Simplificacion: asumimos N0 uniforme (sin bowtie filter)\n",
    "    #    En un escaner real, esto varia con el detector (Fig. 3 del paper)\n",
    "    N0_A = I0\n",
    "    \n",
    "    # 5. Aplicar modelo de ruido (Eq. 6 del paper)\n",
    "    #    P_B = P_A + sqrt((1-a)/a * exp(P_A)/N0_A) * x\n",
    "    a = dose_ratio\n",
    "    \n",
    "    # Varianza del ruido segun Eq. 6\n",
    "    # var = (1-a)/a * exp(P_A) / N0_A\n",
    "    exp_P = np.exp(P_A)\n",
    "    variance_base = ((1.0 - a) / a) * (exp_P / N0_A)\n",
    "    \n",
    "    # 6. Incluir ruido electronico si se especifica (Eq. 11)\n",
    "    if include_electronic:\n",
    "        # var = (1-a)/a * exp(P)/N0 * (1 + (1+a)/a * Ne*exp(P)/N0)\n",
    "        electronic_term = 1.0 + ((1.0 + a) / a) * (Ne * exp_P / N0_A)\n",
    "        variance = variance_base * electronic_term\n",
    "    else:\n",
    "        variance = variance_base\n",
    "    \n",
    "    # Asegurar varianza no negativa\n",
    "    variance = np.clip(variance, 0, None)\n",
    "    \n",
    "    # 7. Generar ruido gaussiano y aplicar\n",
    "    x = np.random.randn(*P_A.shape)\n",
    "    noise = np.sqrt(variance) * x\n",
    "    \n",
    "    P_B = P_A + noise\n",
    "    \n",
    "    # 8. Convertir de vuelta a sinograma para reconstruccion\n",
    "    sinogram_noisy = P_B / 0.01  # Deshacer factor de escala\n",
    "    \n",
    "    # 9. Reconstruir imagen desde sinograma ruidoso\n",
    "    image_recon_atten = sinogram_to_image(sinogram_noisy, theta, output_size=image.shape[0])\n",
    "    \n",
    "    # 10. Convertir de atenuacion a HU\n",
    "    noisy_image = (image_recon_atten - 1.0) * 1000.0\n",
    "    \n",
    "    return noisy_image, sinogram_noisy, sinogram_clean\n",
    "\n",
    "\n",
    "def add_ldct_noise_yu2012(image, dose_ratio=0.25, I0=1e5):\n",
    "    \"\"\"\n",
    "    Wrapper simple para simular LDCT segun Yu et al. 2012.\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen NDCT en valores HU\n",
    "        dose_ratio: Fraccion de dosis (0.25 = 25% = quarter-dose)\n",
    "        I0: Fotones incidentes (ajustar segun nivel de ruido deseado)\n",
    "    \n",
    "    Returns:\n",
    "        noisy_image: Imagen LDCT simulada\n",
    "    \"\"\"\n",
    "    noisy_image, _, _ = add_noise_sinogram_domain(\n",
    "        image, \n",
    "        dose_ratio=dose_ratio, \n",
    "        I0=I0,\n",
    "        include_electronic=True,\n",
    "        Ne=8.2\n",
    "    )\n",
    "    return noisy_image.astype(image.dtype)\n",
    "\n",
    "\n",
    "print(\"Funciones de ruido implementadas (Yu et al. 2012):\")\n",
    "print(\"  - image_to_sinogram(): Imagen -> Sinograma (Radon)\")\n",
    "print(\"  - sinogram_to_image(): Sinograma -> Imagen (FBP)\")\n",
    "print(\"  - add_noise_sinogram_domain(): Ruido en sinograma (Eq. 6, 11)\")\n",
    "print(\"  - add_ldct_noise_yu2012(): Wrapper simple para LDCT\")\n",
    "print(\"\\nModelo: P_B = P_A + sqrt((1-a)/a * exp(P_A)/N0) * x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar simulacion LDCT en dominio del sinograma (Yu et al. 2012)\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    print(\"Simulando LDCT en dominio del sinograma...\")\n",
    "    print(\"Proceso: Imagen -> Sinograma -> +Ruido -> Reconstruccion\")\n",
    "    \n",
    "    # Simular diferentes niveles de dosis\n",
    "    ldct_25, sino_noisy_25, sino_clean = add_noise_sinogram_domain(\n",
    "        original_slice, dose_ratio=0.25, I0=1e5\n",
    "    )\n",
    "    ldct_50, sino_noisy_50, _ = add_noise_sinogram_domain(\n",
    "        original_slice, dose_ratio=0.50, I0=1e5\n",
    "    )\n",
    "    ldct_10, sino_noisy_10, _ = add_noise_sinogram_domain(\n",
    "        original_slice, dose_ratio=0.10, I0=1e5\n",
    "    )\n",
    "    \n",
    "    # Normalizar para visualizacion\n",
    "    def normalize_for_display(img, min_hu=-1000, max_hu=400):\n",
    "        img_clipped = np.clip(img, min_hu, max_hu)\n",
    "        return (img_clipped - min_hu) / (max_hu - min_hu)\n",
    "    \n",
    "    orig_display = normalize_for_display(original_slice)\n",
    "    ldct_25_display = normalize_for_display(ldct_25)\n",
    "    ldct_50_display = normalize_for_display(ldct_50)\n",
    "    ldct_10_display = normalize_for_display(ldct_10)\n",
    "    \n",
    "    # Para compatibilidad con celdas posteriores\n",
    "    noisy_ldct = ldct_25\n",
    "    ldct_display = ldct_25_display\n",
    "    \n",
    "    print(\"[OK] Simulacion LDCT completada\")\n",
    "    print(f\"  - 50% dosis: PSNR = {psnr(orig_display, ldct_50_display, data_range=1.0):.2f} dB\")\n",
    "    print(f\"  - 25% dosis: PSNR = {psnr(orig_display, ldct_25_display, data_range=1.0):.2f} dB\")\n",
    "    print(f\"  - 10% dosis: PSNR = {psnr(orig_display, ldct_10_display, data_range=1.0):.2f} dB\")\n",
    "else:\n",
    "    print(\"[INFO] Usando datos reales Mayo - no se simula ruido\")\n",
    "    \n",
    "    def normalize_for_display(img, min_hu=-1000, max_hu=400):\n",
    "        img_clipped = np.clip(img, min_hu, max_hu)\n",
    "        return (img_clipped - min_hu) / (max_hu - min_hu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el proceso de simulacion LDCT en dominio del sinograma\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Fila 1: Proceso de simulacion\n",
    "    axes[0, 0].imshow(orig_display, cmap='bone')\n",
    "    axes[0, 0].set_title('1. Imagen Original\\n(NDCT)', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(sino_clean, cmap='bone', aspect='auto')\n",
    "    axes[0, 1].set_title('2. Sinograma\\n(Radon Transform)', fontsize=12)\n",
    "    axes[0, 1].set_xlabel('Angulo')\n",
    "    axes[0, 1].set_ylabel('Detector')\n",
    "    \n",
    "    # Diferencia de sinogramas (ruido anadido)\n",
    "    sino_diff = sino_noisy_25 - sino_clean\n",
    "    axes[0, 2].imshow(sino_diff, cmap='RdBu', aspect='auto', \n",
    "                      vmin=-np.percentile(np.abs(sino_diff), 99),\n",
    "                      vmax=np.percentile(np.abs(sino_diff), 99))\n",
    "    axes[0, 2].set_title('3. Ruido Anadido\\n(Eq. 6 Yu et al.)', fontsize=12)\n",
    "    axes[0, 2].set_xlabel('Angulo')\n",
    "    axes[0, 2].set_ylabel('Detector')\n",
    "    \n",
    "    axes[0, 3].imshow(ldct_25_display, cmap='bone')\n",
    "    axes[0, 3].set_title('4. LDCT Reconstruido\\n(25% dosis)', fontsize=12)\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Fila 2: Comparacion de niveles de dosis\n",
    "    axes[1, 0].imshow(orig_display, cmap='bone')\n",
    "    axes[1, 0].set_title('Original (100% dosis)', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(ldct_50_display, cmap='bone')\n",
    "    psnr_50 = psnr(orig_display, ldct_50_display, data_range=1.0)\n",
    "    axes[1, 1].set_title(f'50% dosis\\nPSNR: {psnr_50:.1f} dB', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(ldct_25_display, cmap='bone')\n",
    "    psnr_25 = psnr(orig_display, ldct_25_display, data_range=1.0)\n",
    "    axes[1, 2].set_title(f'25% dosis\\nPSNR: {psnr_25:.1f} dB', fontsize=12)\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    axes[1, 3].imshow(ldct_10_display, cmap='bone')\n",
    "    psnr_10 = psnr(orig_display, ldct_10_display, data_range=1.0)\n",
    "    axes[1, 3].set_title(f'10% dosis\\nPSNR: {psnr_10:.1f} dB', fontsize=12)\n",
    "    axes[1, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Simulacion LDCT en Dominio del Sinograma (Yu et al. 2012)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizar zoom\n",
    "    fig2, axes2 = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    y1, y2, x1, x2 = 200, 300, 200, 300\n",
    "    \n",
    "    axes2[0].imshow(orig_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes2[0].set_title('Original', fontsize=12)\n",
    "    axes2[0].axis('off')\n",
    "    \n",
    "    axes2[1].imshow(ldct_50_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes2[1].set_title('50% dosis', fontsize=12)\n",
    "    axes2[1].axis('off')\n",
    "    \n",
    "    axes2[2].imshow(ldct_25_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes2[2].set_title('25% dosis', fontsize=12)\n",
    "    axes2[2].axis('off')\n",
    "    \n",
    "    axes2[3].imshow(ldct_10_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes2[3].set_title('10% dosis', fontsize=12)\n",
    "    axes2[3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Zoom - Detalle del Ruido', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"[SKIP] Visualizacion de ruido simulado - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular metricas de calidad (solo ruido simulado)\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    print(\"Metricas de Calidad de Imagen (respecto al original):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metrics = []\n",
    "    for name, noisy in [('Gaussiano', gauss_display), \n",
    "                        ('Poisson', poisson_display), \n",
    "                        ('LDCT Sim.', ldct_display)]:\n",
    "        psnr_val = psnr(orig_display, noisy, data_range=1.0)\n",
    "        ssim_val = ssim(orig_display, noisy, data_range=1.0)\n",
    "        mse_val = np.mean((orig_display - noisy)**2)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Tipo': name,\n",
    "            'PSNR (dB)': f'{psnr_val:.2f}',\n",
    "            'SSIM': f'{ssim_val:.4f}',\n",
    "            'MSE': f'{mse_val:.6f}'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  PSNR: {psnr_val:.2f} dB\")\n",
    "        print(f\"  SSIM: {ssim_val:.4f}\")\n",
    "        print(f\"  MSE:  {mse_val:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(metrics_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"[SKIP] Metricas de ruido simulado - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Variación del Nivel de Ruido\n",
    "\n",
    "Exploramos cómo diferentes niveles de dosis afectan la calidad de imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular diferentes niveles de dosis (solo con ruido simulado)\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    dose_ratios = [1.0, 0.5, 0.25, 0.125, 0.0625]  # 100%, 50%, 25%, 12.5%, 6.25%\n",
    "    dose_labels = ['100%', '50%', '25%', '12.5%', '6.25%']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "    for i, (dose, label) in enumerate(zip(dose_ratios, dose_labels)):\n",
    "        if dose == 1.0:\n",
    "            noisy = original_slice.copy()\n",
    "        else:\n",
    "            noisy = add_ct_realistic_noise(original_slice, dose_ratio=dose)\n",
    "        \n",
    "        noisy_display = normalize_for_display(noisy)\n",
    "        \n",
    "        # Imagen completa\n",
    "        axes[0, i].imshow(noisy_display, cmap='bone')\n",
    "        axes[0, i].set_title(f'Dosis: {label}', fontsize=12)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Zoom\n",
    "        axes[1, i].imshow(noisy_display[200:300, 200:300], cmap='bone')\n",
    "        \n",
    "        # Calcular PSNR solo para imagenes con ruido\n",
    "        if dose < 1.0:\n",
    "            psnr_val = psnr(orig_display, noisy_display, data_range=1.0)\n",
    "            axes[1, i].set_title(f'PSNR: {psnr_val:.1f} dB', fontsize=11)\n",
    "        else:\n",
    "            axes[1, i].set_title('Referencia', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Efecto del Nivel de Dosis en la Calidad de Imagen CT', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Visualizacion de niveles de dosis - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Técnicas Clásicas de Denoising\n",
    "\n",
    "Antes de deep learning, se usaban filtros clásicos:\n",
    "\n",
    "| Técnica | Descripción | Pros | Contras |\n",
    "|---------|-------------|------|----------|\n",
    "| Gaussiano | Suavizado isotrópico | Rápido | Pierde bordes |\n",
    "| Mediana | Filtro no lineal | Preserva bordes | Pierde detalles finos |\n",
    "| Bilateral | Preserva bordes | Buenos resultados | Lento |\n",
    "| TV (Total Variation) | Minimiza variación | Preserva bordes | Efecto \"cartoon\" |\n",
    "| NLM (Non-Local Means) | Usa parches similares | Muy buenos resultados | Muy lento |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classical_denoising(image_normalized):\n",
    "    \"\"\"\n",
    "    Aplica diferentes técnicas clásicas de denoising\n",
    "    \n",
    "    Args:\n",
    "        image_normalized: Imagen normalizada [0, 1]\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con resultados de cada técnica\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Filtro Gaussiano\n",
    "    results['Gaussiano'] = gaussian(image_normalized, sigma=1.5)\n",
    "    \n",
    "    # 2. Filtro Mediana\n",
    "    results['Mediana'] = median_filter(image_normalized, size=3)\n",
    "    \n",
    "    # 3. Filtro Bilateral\n",
    "    results['Bilateral'] = denoise_bilateral(\n",
    "        image_normalized, \n",
    "        sigma_color=0.1, \n",
    "        sigma_spatial=5,\n",
    "        channel_axis=None\n",
    "    )\n",
    "    \n",
    "    # 4. Total Variation\n",
    "    results['TV'] = denoise_tv_chambolle(image_normalized, weight=0.1)\n",
    "    \n",
    "    # 5. Non-Local Means (versión rápida)\n",
    "    results['NLM'] = denoise_nl_means(\n",
    "        image_normalized,\n",
    "        h=0.1,\n",
    "        patch_size=5,\n",
    "        patch_distance=6,\n",
    "        channel_axis=None\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Funciones de denoising clásico definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar denoising a imagen LDCT simulada (solo con ruido simulado)\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    print(\"Aplicando tecnicas de denoising...\")\n",
    "    print(\"(Esto puede tomar unos segundos)\")\n",
    "\n",
    "    denoised_results = apply_classical_denoising(ldct_display)\n",
    "\n",
    "    print(\"\\n[OK] Denoising completado!\")\n",
    "else:\n",
    "    denoised_results = None\n",
    "    print(\"[SKIP] Denoising clasico - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de denoising (solo con ruido simulado)\n",
    "if USE_SIMULATED_NOISE and denoised_results is not None:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "    # Fila 1: Imagenes completas\n",
    "    axes[0, 0].imshow(orig_display, cmap='bone')\n",
    "    axes[0, 0].set_title('Original (NDCT)', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "\n",
    "    axes[0, 1].imshow(ldct_display, cmap='bone')\n",
    "    psnr_noisy = psnr(orig_display, ldct_display, data_range=1.0)\n",
    "    axes[0, 1].set_title(f'LDCT Simulado\\nPSNR: {psnr_noisy:.1f} dB', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "\n",
    "    # Mejores tecnicas\n",
    "    best_techniques = ['Bilateral', 'NLM']\n",
    "    for i, tech in enumerate(best_techniques):\n",
    "        denoised = denoised_results[tech]\n",
    "        psnr_val = psnr(orig_display, denoised, data_range=1.0)\n",
    "        axes[0, i+2].imshow(denoised, cmap='bone')\n",
    "        axes[0, i+2].set_title(f'{tech}\\nPSNR: {psnr_val:.1f} dB', fontsize=12)\n",
    "        axes[0, i+2].axis('off')\n",
    "\n",
    "    # Fila 2: Zoom\n",
    "    y1, y2, x1, x2 = 200, 300, 200, 300\n",
    "\n",
    "    axes[1, 0].imshow(orig_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, 0].set_title('Zoom Original', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "\n",
    "    axes[1, 1].imshow(ldct_display[y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, 1].set_title('Zoom LDCT', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    for i, tech in enumerate(best_techniques):\n",
    "        denoised = denoised_results[tech]\n",
    "        axes[1, i+2].imshow(denoised[y1:y2, x1:x2], cmap='bone')\n",
    "        axes[1, i+2].set_title(f'Zoom {tech}', fontsize=12)\n",
    "        axes[1, i+2].axis('off')\n",
    "\n",
    "    plt.suptitle('Comparacion de Tecnicas de Denoising', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Visualizacion de denoising clasico - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa de todas las tecnicas (solo con ruido simulado)\n",
    "if USE_SIMULATED_NOISE and denoised_results is not None:\n",
    "    print(\"Comparacion de Tecnicas de Denoising\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    psnr_noisy = psnr(orig_display, ldct_display, data_range=1.0)\n",
    "    \n",
    "    comparison = []\n",
    "    comparison.append({\n",
    "        'Tecnica': 'LDCT (con ruido)',\n",
    "        'PSNR (dB)': psnr_noisy,\n",
    "        'SSIM': ssim(orig_display, ldct_display, data_range=1.0)\n",
    "    })\n",
    "\n",
    "    for tech, denoised in denoised_results.items():\n",
    "        comparison.append({\n",
    "            'Tecnica': tech,\n",
    "            'PSNR (dB)': psnr(orig_display, denoised, data_range=1.0),\n",
    "            'SSIM': ssim(orig_display, denoised, data_range=1.0)\n",
    "        })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    comparison_df['PSNR (dB)'] = comparison_df['PSNR (dB)'].round(2)\n",
    "    comparison_df['SSIM'] = comparison_df['SSIM'].round(4)\n",
    "\n",
    "    # Ordenar por PSNR\n",
    "    comparison_df = comparison_df.sort_values('PSNR (dB)', ascending=False)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"Mayor PSNR y SSIM = Mejor calidad de imagen\")\n",
    "else:\n",
    "    comparison_df = None\n",
    "    print(\"[SKIP] Tabla comparativa - usando datos reales Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset para entrenamiento con ruido simulado en dominio del sinograma\n",
    "if USE_SIMULATED_NOISE:\n",
    "    class CTDenoisingDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Dataset para entrenamiento de denoising en CT.\n",
    "        \n",
    "        Genera pares (LDCT, NDCT) simulando ruido en el dominio\n",
    "        del sinograma segun Yu et al. 2012.\n",
    "        \"\"\"\n",
    "        def __init__(self, data_path, annotations_path, patch_size=64, \n",
    "                     num_patches_per_scan=50, dose_ratio=0.25, I0=1e5):\n",
    "            self.patch_size = patch_size\n",
    "            self.num_patches = num_patches_per_scan\n",
    "            self.dose_ratio = dose_ratio\n",
    "            self.I0 = I0\n",
    "            \n",
    "            # Cargar datos\n",
    "            self.loader = LUNA16DataLoader(data_path, annotations_path)\n",
    "            self.mhd_files = list(Path(data_path).glob(\"*.mhd\"))\n",
    "            \n",
    "            # Cache de slices (pares NDCT, LDCT)\n",
    "            self._pairs_cache = []\n",
    "            self._load_and_simulate()\n",
    "        \n",
    "        def _load_and_simulate(self):\n",
    "            \"\"\"Carga slices y simula LDCT en dominio del sinograma\"\"\"\n",
    "            print(\"Cargando slices y simulando LDCT (Yu et al. 2012)...\")\n",
    "            print(\"  Proceso: Imagen -> Sinograma -> +Ruido -> Reconstruccion\")\n",
    "            \n",
    "            max_scans = min(5, len(self.mhd_files))\n",
    "            \n",
    "            for mhd_file in tqdm(self.mhd_files[:max_scans], desc=\"Procesando scans\"):\n",
    "                ct_scan, _, _ = self.loader.load_itk_image(str(mhd_file))\n",
    "                \n",
    "                # Tomar slices centrales\n",
    "                start_slice = ct_scan.shape[0] // 4\n",
    "                end_slice = 3 * ct_scan.shape[0] // 4\n",
    "                \n",
    "                for slice_idx in range(start_slice, end_slice, 10):  # Cada 10 slices\n",
    "                    ndct_slice = ct_scan[slice_idx]\n",
    "                    \n",
    "                    # Simular LDCT en dominio del sinograma\n",
    "                    ldct_slice = add_ldct_noise_yu2012(\n",
    "                        ndct_slice, \n",
    "                        dose_ratio=self.dose_ratio,\n",
    "                        I0=self.I0\n",
    "                    )\n",
    "                    \n",
    "                    # Normalizar ambas\n",
    "                    ndct_norm = self.loader.normalize_hu(ndct_slice, -1000, 400)\n",
    "                    ldct_norm = self.loader.normalize_hu(ldct_slice, -1000, 400)\n",
    "                    \n",
    "                    self._pairs_cache.append((ndct_norm, ldct_norm))\n",
    "            \n",
    "            print(f\"[OK] Pares NDCT/LDCT generados: {len(self._pairs_cache)}\")\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self._pairs_cache) * self.num_patches\n",
    "        \n",
    "        def _extract_random_patch(self, ndct, ldct):\n",
    "            \"\"\"Extrae patch aleatorio del mismo lugar en ambas imagenes\"\"\"\n",
    "            h, w = ndct.shape\n",
    "            \n",
    "            max_y = h - self.patch_size\n",
    "            max_x = w - self.patch_size\n",
    "            \n",
    "            if max_y <= 0 or max_x <= 0:\n",
    "                return ndct[:self.patch_size, :self.patch_size], \\\n",
    "                       ldct[:self.patch_size, :self.patch_size]\n",
    "            \n",
    "            y = np.random.randint(0, max_y)\n",
    "            x = np.random.randint(0, max_x)\n",
    "            \n",
    "            return ndct[y:y+self.patch_size, x:x+self.patch_size], \\\n",
    "                   ldct[y:y+self.patch_size, x:x+self.patch_size]\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Seleccionar par\n",
    "            pair_idx = idx // self.num_patches\n",
    "            ndct_slice, ldct_slice = self._pairs_cache[pair_idx]\n",
    "            \n",
    "            # Extraer patch aleatorio\n",
    "            ndct_patch, ldct_patch = self._extract_random_patch(ndct_slice, ldct_slice)\n",
    "            \n",
    "            # Clip para asegurar rango valido\n",
    "            ndct_patch = np.clip(ndct_patch, 0, 1)\n",
    "            ldct_patch = np.clip(ldct_patch, 0, 1)\n",
    "            \n",
    "            # Convertir a tensores\n",
    "            ndct_tensor = torch.FloatTensor(ndct_patch).unsqueeze(0)\n",
    "            ldct_tensor = torch.FloatTensor(ldct_patch).unsqueeze(0)\n",
    "            \n",
    "            # Retornar (ruidosa, limpia)\n",
    "            return ldct_tensor, ndct_tensor\n",
    "\n",
    "    print(\"[OK] Dataset con ruido en sinograma definido (Yu et al. 2012)\")\n",
    "else:\n",
    "    print(\"[INFO] Usando datos reales Mayo - dataset simulado no necesario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset y dataloader (solo ruido simulado)\n",
    "if USE_SIMULATED_NOISE:\n",
    "    denoise_dataset = CTDenoisingDataset(\n",
    "        DATA_PATH, \n",
    "        ANNOTATIONS_PATH,\n",
    "        patch_size=64,\n",
    "        num_patches_per_scan=100,\n",
    "        dose_ratio=0.25\n",
    "    )\n",
    "\n",
    "    denoise_loader = DataLoader(\n",
    "        denoise_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset size: {len(denoise_dataset)}\")\n",
    "    print(f\"Batches: {len(denoise_loader)}\")\n",
    "else:\n",
    "    denoise_dataset = None\n",
    "    denoise_loader = None\n",
    "    print(\"[INFO] Dataset simulado no creado - se usara Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar ejemplos del dataset (solo ruido simulado)\n",
    "if USE_SIMULATED_NOISE and denoise_loader is not None:\n",
    "    noisy_batch, clean_batch = next(iter(denoise_loader))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(noisy_batch[i, 0].numpy(), cmap='bone')\n",
    "        axes[0, i].set_title('Ruidosa (LDCT)', fontsize=11)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(clean_batch[i, 0].numpy(), cmap='bone')\n",
    "        axes[1, i].set_title('Limpia (NDCT)', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Ejemplos del Dataset de Denoising (Pares Ruidosa/Limpia)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Visualizacion de dataset simulado - se usara Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo (solo ruido simulado)\n",
    "if USE_SIMULATED_NOISE and denoise_loader is not None:\n",
    "    print(\"Iniciando entrenamiento con ruido simulado...\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    history = train_denoiser(model_dncnn, denoise_loader, epochs=5, lr=1e-3)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"[OK] Entrenamiento completado!\")\n",
    "else:\n",
    "    history = None\n",
    "    print(\"[INFO] Entrenamiento con ruido simulado omitido\")\n",
    "    print(\"       Se entrenara con datos reales Mayo en celdas posteriores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar historial de entrenamiento (solo ruido simulado)\n",
    "if history is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(history['loss'], 'b-o', linewidth=2, markersize=8)\n",
    "    axes[0].set_xlabel('Epoca', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "    axes[0].set_title('Perdida durante Entrenamiento', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # PSNR\n",
    "    axes[1].plot(history['psnr'], 'g-o', linewidth=2, markersize=8)\n",
    "    axes[1].set_xlabel('Epoca', fontsize=12)\n",
    "    axes[1].set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    axes[1].set_title('PSNR durante Entrenamiento', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Historial de Entrenamiento DnCNN', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Historial de entrenamiento - se mostrara despues de entrenar con Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en imagen de prueba (solo ruido simulado)\n",
    "if USE_SIMULATED_NOISE and original_slice is not None:\n",
    "    results = evaluate_denoiser(model_dncnn, orig_display, dose_ratio=0.25)\n",
    "\n",
    "    print(\"Metricas de Evaluacion:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nImagen LDCT (con ruido):\")\n",
    "    print(f\"  PSNR: {results['psnr_noisy']:.2f} dB\")\n",
    "    print(f\"  SSIM: {results['ssim_noisy']:.4f}\")\n",
    "\n",
    "    print(f\"\\nImagen Denoised (DnCNN):\")\n",
    "    print(f\"  PSNR: {results['psnr_denoised']:.2f} dB (+{results['psnr_gain']:.2f} dB)\")\n",
    "    print(f\"  SSIM: {results['ssim_denoised']:.4f} (+{results['ssim_gain']:.4f})\")\n",
    "else:\n",
    "    results = None\n",
    "    print(\"[INFO] Evaluacion con ruido simulado omitida\")\n",
    "    print(\"       Se evaluara despues de entrenar con datos Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de denoising con DnCNN (solo ruido simulado)\n",
    "if results is not None:\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "    # Fila 1: Imagenes completas\n",
    "    axes[0, 0].imshow(results['clean'], cmap='bone')\n",
    "    axes[0, 0].set_title('Original (NDCT)', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "\n",
    "    axes[0, 1].imshow(results['noisy'], cmap='bone')\n",
    "    axes[0, 1].set_title(f'LDCT Simulado\\nPSNR: {results[\"psnr_noisy\"]:.1f} dB', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "\n",
    "    axes[0, 2].imshow(results['denoised'], cmap='bone')\n",
    "    axes[0, 2].set_title(f'DnCNN Denoised\\nPSNR: {results[\"psnr_denoised\"]:.1f} dB', fontsize=12)\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "    # Diferencia (residuo aprendido)\n",
    "    residual = np.abs(results['noisy'] - results['denoised'])\n",
    "    axes[0, 3].imshow(residual, cmap='hot')\n",
    "    axes[0, 3].set_title('Ruido Removido\\n(Residuo)', fontsize=12)\n",
    "    axes[0, 3].axis('off')\n",
    "\n",
    "    # Fila 2: Zoom\n",
    "    y1, y2, x1, x2 = 200, 300, 200, 300\n",
    "\n",
    "    axes[1, 0].imshow(results['clean'][y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, 0].set_title('Zoom Original', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "\n",
    "    axes[1, 1].imshow(results['noisy'][y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, 1].set_title('Zoom LDCT', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    axes[1, 2].imshow(results['denoised'][y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, 2].set_title('Zoom DnCNN', fontsize=12)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    axes[1, 3].imshow(residual[y1:y2, x1:x2], cmap='hot')\n",
    "    axes[1, 3].set_title('Zoom Residuo', fontsize=12)\n",
    "    axes[1, 3].axis('off')\n",
    "\n",
    "    plt.suptitle('Resultados de Denoising con DnCNN', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Visualizacion de resultados - se mostrara despues de entrenar con Mayo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Comparación: Clásico vs Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparacion final de todas las tecnicas (solo ruido simulado)\n",
    "if results is not None and denoised_results is not None:\n",
    "    print(\"Comparacion Final: Clasico vs Deep Learning\")\n",
    "    print(\"=\"*65)\n",
    "\n",
    "    # Aplicar denoising clasico a la misma imagen ruidosa\n",
    "    classical_on_same = apply_classical_denoising(results['noisy'])\n",
    "\n",
    "    final_comparison = []\n",
    "\n",
    "    # LDCT (referencia)\n",
    "    final_comparison.append({\n",
    "        'Tecnica': 'LDCT (sin procesar)',\n",
    "        'Tipo': 'Referencia',\n",
    "        'PSNR (dB)': results['psnr_noisy'],\n",
    "        'SSIM': results['ssim_noisy']\n",
    "    })\n",
    "\n",
    "    # Tecnicas clasicas\n",
    "    for tech, denoised in classical_on_same.items():\n",
    "        final_comparison.append({\n",
    "            'Tecnica': tech,\n",
    "            'Tipo': 'Clasico',\n",
    "            'PSNR (dB)': psnr(results['clean'], denoised, data_range=1.0),\n",
    "            'SSIM': ssim(results['clean'], denoised, data_range=1.0)\n",
    "        })\n",
    "\n",
    "    # DnCNN\n",
    "    final_comparison.append({\n",
    "        'Tecnica': 'DnCNN',\n",
    "        'Tipo': 'Deep Learning',\n",
    "        'PSNR (dB)': results['psnr_denoised'],\n",
    "        'SSIM': results['ssim_denoised']\n",
    "    })\n",
    "\n",
    "    # Crear DataFrame y ordenar\n",
    "    final_df = pd.DataFrame(final_comparison)\n",
    "    final_df['PSNR (dB)'] = final_df['PSNR (dB)'].round(2)\n",
    "    final_df['SSIM'] = final_df['SSIM'].round(4)\n",
    "    final_df = final_df.sort_values('PSNR (dB)', ascending=False)\n",
    "\n",
    "    print(final_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "else:\n",
    "    final_df = None\n",
    "    print(\"[SKIP] Comparacion final - se mostrara despues de entrenar con Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de barras comparativo (solo ruido simulado)\n",
    "if final_df is not None and results is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Ordenar por PSNR para visualizacion\n",
    "    df_sorted = final_df.sort_values('PSNR (dB)', ascending=True)\n",
    "\n",
    "    # Colores por tipo\n",
    "    colors = ['gray' if t == 'Referencia' else 'steelblue' if t == 'Clasico' else 'coral' \n",
    "              for t in df_sorted['Tipo']]\n",
    "\n",
    "    # PSNR\n",
    "    axes[0].barh(df_sorted['Tecnica'], df_sorted['PSNR (dB)'], color=colors)\n",
    "    axes[0].set_xlabel('PSNR (dB)', fontsize=12)\n",
    "    axes[0].set_title('Comparacion de PSNR', fontsize=14)\n",
    "    axes[0].axvline(x=results['psnr_noisy'], color='red', linestyle='--', alpha=0.5, label='LDCT')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # SSIM\n",
    "    axes[1].barh(df_sorted['Tecnica'], df_sorted['SSIM'], color=colors)\n",
    "    axes[1].set_xlabel('SSIM', fontsize=12)\n",
    "    axes[1].set_title('Comparacion de SSIM', fontsize=14)\n",
    "    axes[1].axvline(x=results['ssim_noisy'], color='red', linestyle='--', alpha=0.5, label='LDCT')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # Leyenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='gray', label='Referencia'),\n",
    "        Patch(facecolor='steelblue', label='Clasico'),\n",
    "        Patch(facecolor='coral', label='Deep Learning')\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "    plt.suptitle('Comparacion de Tecnicas de Denoising', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[SKIP] Grafico comparativo - se mostrara despues de entrenar con Mayo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Guardar Modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar pesos del modelo\n",
    "# weights_dir ya esta definido en celda inicial (Drive en Colab, local en PC)\n",
    "\n",
    "model_path = os.path.join(weights_dir, 'dncnn_denoising.pth')\n",
    "\n",
    "if USE_SIMULATED_NOISE:\n",
    "    torch.save(model_dncnn.state_dict(), model_path)\n",
    "    print(f\"[OK] Modelo guardado en: {model_path}\")\n",
    "    print(f\"    Tamano: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")\n",
    "else:\n",
    "    print(\"[INFO] Modelo se guardara despues de entrenar con datos Mayo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para cargar modelo guardado\n",
    "def load_denoiser(weights_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Carga un modelo DnCNN entrenado\n",
    "    \n",
    "    Args:\n",
    "        weights_path: Ruta al archivo .pth\n",
    "        device: Dispositivo ('cuda' o 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo cargado listo para inferencia\n",
    "    \"\"\"\n",
    "    model = DnCNN(in_channels=1, out_channels=1, num_features=64, depth=17)\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Verificar que se puede cargar (solo si se entreno con ruido simulado)\n",
    "if USE_SIMULATED_NOISE and os.path.exists(model_path):\n",
    "    model_loaded = load_denoiser(model_path, device)\n",
    "    print(\"[OK] Modelo cargado correctamente!\")\n",
    "else:\n",
    "    model_loaded = None\n",
    "    print(\"[INFO] Modelo se cargara despues de entrenar con Mayo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTE B: Datos Reales Mayo Clinic (Google Colab)\n",
    "\n",
    "> **NOTA**: Esta seccion usa datos reales del dataset Mayo Clinic LDCT.\n",
    "> Se divide en dos subsecciones para entrenar modelos especializados:\n",
    ">\n",
    "> - **B.1**: Proyecciones/Sinogramas (~50GB) - Para denoising en dominio de proyecciones\n",
    "> - **B.2**: Imagenes CT Reconstruidas (~5GB) - Para denoising en dominio de imagen\n",
    ">\n",
    "> Si solo quieres trabajar con ruido simulado, puedes **colapsar esta seccion**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## B.1: Proyecciones/Sinogramas (Mayo_LDCT)\n",
    "\n",
    "El dataset LDCT-and-Projection-data contiene **proyecciones** (sinogramas) con diferentes niveles de dosis.\n",
    "\n",
    "### Caracteristicas de las proyecciones:\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|---------|\n",
    "| **Formato** | DICOM-CT-PD (Projection Data) |\n",
    "| **Tamano** | 736 x 64 por proyeccion |\n",
    "| **Dosis** | Full-dose (100%) + Low-dose (25%) |\n",
    "| **Tamano Total** | ~50 GB |\n",
    "\n",
    "### Estructura esperada:\n",
    "```\n",
    "Mayo_LDCT/\n",
    "    <SeriesInstanceUID>/    # Carpeta por serie\n",
    "        *.dcm               # Proyecciones DICOM\n",
    "    ...\n",
    "```\n",
    "\n",
    "> El SeriesDescription del DICOM indica \"Full dose projections\" o \"Low dose projections\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# B.1 - PREPARAR PROYECCIONES MAYO - COPIAR A LOCAL\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "if MAYO_PROJ_AVAILABLE:\n",
    "    # Copiar de Drive a local (mucho mas rapido para entrenamiento)\n",
    "    local_proj_path = \"/content/Mayo_LDCT_local\"\n",
    "\n",
    "    if IN_COLAB and not os.path.exists(local_proj_path):\n",
    "        print(\"Copiando proyecciones de Drive a almacenamiento local...\")\n",
    "        print(\"(Esto mejora velocidad de entrenamiento significativamente)\")\n",
    "        shutil.copytree(MAYO_PROJ_PATH, local_proj_path)\n",
    "        print(f\"[OK] Datos copiados a {local_proj_path}\")\n",
    "    else:\n",
    "        if IN_COLAB:\n",
    "            print(f\"[OK] Datos ya existen en {local_proj_path}\")\n",
    "        else:\n",
    "            local_proj_path = MAYO_PROJ_PATH\n",
    "            print(f\"[OK] Usando datos locales: {local_proj_path}\")\n",
    "\n",
    "    # Usar path local para el resto de la seccion\n",
    "    MAYO_PROJ_PATH_FAST = local_proj_path\n",
    "else:\n",
    "    MAYO_PROJ_PATH_FAST = None\n",
    "    print(\"[INFO] Proyecciones Mayo no disponibles, saltando seccion B.1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# LOADER PARA MAYO CLINIC - ADAPTADO A ESTRUCTURA TCIA\n",
    "# ============================================================\n",
    "import pydicom\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# B.1 - LOADER PARA PROYECCIONES MAYO\n",
    "# ============================================================\n",
    "if not MAYO_PROJ_AVAILABLE:\n",
    "    print(\"[INFO] Seccion B.1 saltada - proyecciones no disponibles\")\n",
    "else:\n",
    "    class MayoLDCTLoader:\n",
    "        \"\"\"\n",
    "        Cargador para dataset Mayo Clinic LDCT (TCIA)\n",
    "        Adaptado a la estructura real de descarga de TCIA.\n",
    "\n",
    "        Estructura TCIA:\n",
    "        - Cada serie es una carpeta con SeriesInstanceUID\n",
    "        - SeriesDescription indica \"Full dose projections\" o \"Low dose projections\"\n",
    "        - Cada DICOM es una proyeccion individual (736, 64)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, base_path):\n",
    "            self.base_path = Path(base_path)\n",
    "            self.series_info = self._scan_series()\n",
    "            self.patients = self._group_by_patient()\n",
    "\n",
    "        def _scan_series(self):\n",
    "            \"\"\"Escanea todas las series y extrae metadata\"\"\"\n",
    "            series = []\n",
    "\n",
    "            for series_dir in self.base_path.iterdir():\n",
    "                if not series_dir.is_dir():\n",
    "                    continue\n",
    "\n",
    "                dcm_files = list(series_dir.glob(\"*.dcm\"))\n",
    "                if not dcm_files:\n",
    "                    continue\n",
    "\n",
    "                # Leer metadata del primer DICOM\n",
    "                try:\n",
    "                    ds = pydicom.dcmread(str(dcm_files[0]), stop_before_pixels=True)\n",
    "                    series.append({\n",
    "                        'path': series_dir,\n",
    "                        'series_uid': series_dir.name,\n",
    "                        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
    "                        'series_desc': getattr(ds, 'SeriesDescription', ''),\n",
    "                        'n_slices': len(dcm_files)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARNING] Error leyendo {series_dir.name}: {e}\")\n",
    "\n",
    "            print(f\"[OK] {len(series)} series encontradas\")\n",
    "            return series\n",
    "\n",
    "        def _group_by_patient(self):\n",
    "            \"\"\"Agrupa series por paciente, identificando pares full/low dose\"\"\"\n",
    "            patients = defaultdict(dict)\n",
    "\n",
    "            for s in self.series_info:\n",
    "                pid = s['patient_id']\n",
    "                desc = s['series_desc'].lower()\n",
    "\n",
    "                if 'full' in desc:\n",
    "                    patients[pid]['full'] = s\n",
    "                elif 'low' in desc:\n",
    "                    patients[pid]['low'] = s\n",
    "\n",
    "            # Filtrar solo pacientes con ambas series\n",
    "            valid_patients = {\n",
    "                pid: data for pid, data in patients.items()\n",
    "                if 'full' in data and 'low' in data\n",
    "            }\n",
    "\n",
    "            print(f\"[OK] {len(valid_patients)} pacientes con pares completos\")\n",
    "            for pid, data in valid_patients.items():\n",
    "                print(f\"    {pid}: Full={data['full']['n_slices']}, Low={data['low']['n_slices']} projections\")\n",
    "\n",
    "            return valid_patients\n",
    "\n",
    "        def load_patient_pair(self, patient_id, slice_idx=None):\n",
    "            \"\"\"\n",
    "            Carga un par de sinogramas (full/low dose) para un paciente\n",
    "\n",
    "            Args:\n",
    "                patient_id: ID del paciente\n",
    "                slice_idx: Indice del slice/proyeccion (None = retorna paths)\n",
    "\n",
    "            Returns:\n",
    "                Si slice_idx es None: (full_files, low_files)\n",
    "                Si slice_idx es int: (full_array, low_array)\n",
    "            \"\"\"\n",
    "            if patient_id not in self.patients:\n",
    "                raise ValueError(f\"Paciente {patient_id} no encontrado o sin par completo\")\n",
    "\n",
    "            full_path = self.patients[patient_id]['full']['path']\n",
    "            low_path = self.patients[patient_id]['low']['path']\n",
    "\n",
    "            full_files = sorted(full_path.glob(\"*.dcm\"))\n",
    "            low_files = sorted(low_path.glob(\"*.dcm\"))\n",
    "\n",
    "            if slice_idx is None:\n",
    "                return full_files, low_files\n",
    "\n",
    "            # Cargar slice especifico\n",
    "            full_ds = pydicom.dcmread(str(full_files[slice_idx]))\n",
    "            low_ds = pydicom.dcmread(str(low_files[slice_idx]))\n",
    "\n",
    "            return full_ds.pixel_array, low_ds.pixel_array\n",
    "\n",
    "\n",
    "    # Crear loader\n",
    "    if MAYO_AVAILABLE:\n",
    "        mayo_loader = MayoLDCTLoader(MAYO_PATH_FAST)\n",
    "        print(f\"\\nPacientes disponibles: {list(mayo_loader.patients.keys())}\")\n",
    "    else:\n",
    "        mayo_loader = None\n",
    "        print(\"[INFO] Mayo loader no creado - usando ruido simulado\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# B.1 - DATASET DE SINOGRAMAS MAYO\n",
    "# ============================================================\n",
    "from skimage.transform import resize\n",
    "\n",
    "if not MAYO_PROJ_AVAILABLE:\n",
    "    train_loader_mayo = None\n",
    "    test_loader_mayo = None\n",
    "    print(\"[INFO] Dataset sinogramas saltado - proyecciones no disponibles\")\n",
    "else:\n",
    "    class MayoSinogramDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Dataset de pares de sinogramas Full/Low dose.\n",
    "        Usa sinogramas completos (no patches) con resize a tamano fijo.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, loader, patient_ids=None, samples_per_patient=200,\n",
    "                     target_size=(64, 64), augment=False):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                loader: MayoLDCTLoader\n",
    "                patient_ids: Lista de pacientes (None = todos)\n",
    "                samples_per_patient: Proyecciones a muestrear por paciente\n",
    "                target_size: Tamano de salida (resize)\n",
    "                augment: Aplicar data augmentation\n",
    "            \"\"\"\n",
    "            self.loader = loader\n",
    "            self.target_size = target_size\n",
    "            self.augment = augment\n",
    "\n",
    "            if patient_ids is None:\n",
    "                self.patient_ids = list(loader.patients.keys())\n",
    "            else:\n",
    "                self.patient_ids = [p for p in patient_ids if p in loader.patients]\n",
    "\n",
    "            # Crear lista de samples (pid, slice_idx)\n",
    "            self.samples = []\n",
    "            for pid in self.patient_ids:\n",
    "                n_slices = loader.patients[pid]['full']['n_slices']\n",
    "                # Muestrear uniformemente\n",
    "                indices = np.linspace(0, n_slices-1, min(samples_per_patient, n_slices), dtype=int)\n",
    "                for idx in indices:\n",
    "                    self.samples.append((pid, int(idx)))\n",
    "\n",
    "            print(f\"[OK] Dataset: {len(self.samples)} sinogramas de {len(self.patient_ids)} pacientes\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            pid, slice_idx = self.samples[idx]\n",
    "\n",
    "            # Cargar sinogramas\n",
    "            full_sino, low_sino = self.loader.load_patient_pair(pid, slice_idx)\n",
    "\n",
    "            # Convertir a float32\n",
    "            full_sino = full_sino.astype(np.float32)\n",
    "            low_sino = low_sino.astype(np.float32)\n",
    "\n",
    "            # Normalizar a [0, 1]\n",
    "            def normalize(x):\n",
    "                x_min, x_max = x.min(), x.max()\n",
    "                if x_max - x_min > 0:\n",
    "                    return (x - x_min) / (x_max - x_min)\n",
    "                return x\n",
    "\n",
    "            full_sino = normalize(full_sino)\n",
    "            low_sino = normalize(low_sino)\n",
    "\n",
    "            # Resize a tamano objetivo (usando sinograma completo)\n",
    "            if full_sino.shape != self.target_size:\n",
    "                full_sino = resize(full_sino, self.target_size, preserve_range=True, anti_aliasing=True)\n",
    "                low_sino = resize(low_sino, self.target_size, preserve_range=True, anti_aliasing=True)\n",
    "\n",
    "            # Data augmentation\n",
    "            if self.augment:\n",
    "                if np.random.rand() > 0.5:\n",
    "                    full_sino = np.fliplr(full_sino).copy()\n",
    "                    low_sino = np.fliplr(low_sino).copy()\n",
    "\n",
    "            # Convertir a tensor (C, H, W)\n",
    "            full_tensor = torch.from_numpy(full_sino).unsqueeze(0).float()\n",
    "            low_tensor = torch.from_numpy(low_sino).unsqueeze(0).float()\n",
    "\n",
    "            return low_tensor, full_tensor  # (noisy, clean)\n",
    "\n",
    "    # Instanciar loader con path local (rapido)\n",
    "    loader_proj = MayoLDCTLoader(MAYO_PROJ_PATH_FAST)\n",
    "\n",
    "    # Dividir pacientes en train/test\n",
    "    patient_ids = loader_proj.get_patient_ids()\n",
    "    n_train = max(1, int(len(patient_ids) * 0.8))\n",
    "    train_patients = patient_ids[:n_train]\n",
    "    test_patients = patient_ids[n_train:] if n_train < len(patient_ids) else patient_ids[:1]\n",
    "\n",
    "    print(f\"\\nDivision de datos proyecciones:\")\n",
    "    print(f\"  Train: {train_patients}\")\n",
    "    print(f\"  Test: {test_patients}\")\n",
    "\n",
    "    # Crear datasets\n",
    "    train_dataset_mayo = MayoSinogramDataset(\n",
    "        loader_proj,\n",
    "        patient_ids=train_patients,\n",
    "        samples_per_patient=200,\n",
    "        target_size=(64, 64),\n",
    "        augment=True\n",
    "    )\n",
    "\n",
    "    test_dataset_mayo = MayoSinogramDataset(\n",
    "        loader_proj,\n",
    "        patient_ids=test_patients,\n",
    "        samples_per_patient=50,\n",
    "        target_size=(64, 64),\n",
    "        augment=False\n",
    "    )\n",
    "\n",
    "    # Crear dataloaders\n",
    "    train_loader_mayo = DataLoader(train_dataset_mayo, batch_size=16, shuffle=True, num_workers=0)\n",
    "    test_loader_mayo = DataLoader(test_dataset_mayo, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"\\n[OK] DataLoaders creados:\")\n",
    "    print(f\"  Train: {len(train_loader_mayo)} batches\")\n",
    "    print(f\"  Test: {len(test_loader_mayo)} batches\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualizar ejemplos de sinogramas\n",
    "if train_loader_mayo is not None:\n",
    "    low_batch, full_batch = next(iter(train_loader_mayo))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "    for i in range(min(4, low_batch.size(0))):\n",
    "        axes[0, i].imshow(low_batch[i, 0].numpy(), cmap='gray', aspect='auto')\n",
    "        axes[0, i].set_title(f'Low-dose sinogram', fontsize=11)\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(full_batch[i, 0].numpy(), cmap='gray', aspect='auto')\n",
    "        axes[1, i].set_title(f'Full-dose sinogram', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Pares de Sinogramas Mayo Clinic (64x64)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar diferencia\n",
    "    diff = torch.abs(full_batch - low_batch)\n",
    "    print(f\"\\nEstadisticas:\")\n",
    "    print(f\"  Diferencia media: {diff.mean():.4f}\")\n",
    "    print(f\"  Diferencia max: {diff.max():.4f}\")\n",
    "else:\n",
    "    print(\"[INFO] No hay datos de proyecciones Mayo para visualizar\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Entrenamiento con Datos Reales Mayo Clinic\n",
    "\n",
    "Si el dataset Mayo está disponible, podemos entrenar el modelo DnCNN con pares reales NDCT/LDCT."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# B.1 - ENTRENAMIENTO CON CHECKPOINTS - SINOGRAMAS MAYO\n",
    "# ============================================================\n",
    "\n",
    "def train_denoiser_with_checkpoints(model, train_loader, test_loader=None,\n",
    "                                     epochs=10, lr=1e-3, save_dir=None):\n",
    "    \"\"\"\n",
    "    Entrena el modelo con guardado de checkpoints cada epoca.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    history = {'loss': [], 'psnr': [], 'val_psnr': []}\n",
    "    best_psnr = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_psnr = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoca {epoch+1}/{epochs}')\n",
    "\n",
    "        for noisy, clean in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            denoised = model(noisy)\n",
    "            loss = criterion(denoised, clean)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mse = F.mse_loss(denoised, clean)\n",
    "                batch_psnr = 10 * torch.log10(1.0 / (mse + 1e-10))\n",
    "                epoch_psnr += batch_psnr.item()\n",
    "\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}', 'PSNR': f'{batch_psnr.item():.2f}'})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_psnr = epoch_psnr / num_batches\n",
    "\n",
    "        history['loss'].append(avg_loss)\n",
    "        history['psnr'].append(avg_psnr)\n",
    "\n",
    "        # Validacion en test set\n",
    "        val_psnr = 0\n",
    "        if test_loader is not None:\n",
    "            model.eval()\n",
    "            val_psnr_list = []\n",
    "            with torch.no_grad():\n",
    "                for noisy, clean in test_loader:\n",
    "                    noisy = noisy.to(device)\n",
    "                    clean = clean.to(device)\n",
    "                    denoised = model(noisy)\n",
    "                    mse = F.mse_loss(denoised, clean)\n",
    "                    val_psnr_list.append(10 * torch.log10(1.0 / (mse + 1e-10)).item())\n",
    "            val_psnr = np.mean(val_psnr_list)\n",
    "            history['val_psnr'].append(val_psnr)\n",
    "            model.train()\n",
    "\n",
    "        print(f\"Epoca {epoch+1}: Loss={avg_loss:.6f}, PSNR={avg_psnr:.2f}dB, Val_PSNR={val_psnr:.2f}dB\")\n",
    "\n",
    "        # Guardar checkpoint\n",
    "        if save_dir:\n",
    "            checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'psnr': avg_psnr,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "            # Guardar mejor modelo\n",
    "            if avg_psnr > best_psnr:\n",
    "                best_psnr = avg_psnr\n",
    "                best_path = os.path.join(save_dir, 'best_model.pth')\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    return history, model\n",
    "\n",
    "\n",
    "# Entrenar modelo para sinogramas\n",
    "if train_loader_mayo is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENAMIENTO - MODELO DNCNN PARA SINOGRAMAS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Crear modelo\n",
    "    model_mayo = DnCNN(channels=1, num_layers=17, features=64).to(device)\n",
    "    print(f\"[OK] Modelo DnCNN creado en {device}\")\n",
    "\n",
    "    # Directorio para checkpoints\n",
    "    sino_save_dir = os.path.join(weights_dir, \"dncnn_sinograms_checkpoints\")\n",
    "    os.makedirs(sino_save_dir, exist_ok=True)\n",
    "\n",
    "    # Entrenar\n",
    "    history_mayo, model_mayo = train_denoiser_with_checkpoints(\n",
    "        model_mayo,\n",
    "        train_loader_mayo,\n",
    "        test_loader=test_loader_mayo,\n",
    "        epochs=15,\n",
    "        lr=1e-3,\n",
    "        save_dir=sino_save_dir\n",
    "    )\n",
    "\n",
    "    # Guardar modelo final\n",
    "    final_path = os.path.join(weights_dir, \"dncnn_sinograms_final.pth\")\n",
    "    torch.save(model_mayo.state_dict(), final_path)\n",
    "    print(f\"\\n[OK] Modelo sinogramas guardado: {final_path}\")\n",
    "else:\n",
    "    model_mayo = None\n",
    "    history_mayo = None\n",
    "    print(\"[INFO] Entrenamiento sinogramas saltado - datos no disponibles\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# B.1 - EVALUACION FINAL EN TEST SET - SINOGRAMAS\n",
    "# ============================================================\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "if model_mayo is not None and test_loader_mayo is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUACION EN TEST SET - MODELO SINOGRAMAS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model_mayo.eval()\n",
    "\n",
    "    test_psnr_ldct = []\n",
    "    test_psnr_restored = []\n",
    "    test_ssim_ldct = []\n",
    "    test_ssim_restored = []\n",
    "\n",
    "    examples_sino = {'ldct': [], 'ndct': [], 'restored': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ldct, ndct in tqdm(test_loader_mayo, desc=\"Evaluando test set sinogramas\"):\n",
    "            ldct = ldct.to(device)\n",
    "            ndct_gpu = ndct.to(device)\n",
    "            restored = model_mayo(ldct)\n",
    "\n",
    "            for i in range(ldct.size(0)):\n",
    "                ldct_np = ldct[i, 0].cpu().numpy()\n",
    "                ndct_np = ndct[i, 0].cpu().numpy()\n",
    "                restored_np = restored[i, 0].cpu().numpy()\n",
    "\n",
    "                test_psnr_ldct.append(psnr(ndct_np, ldct_np, data_range=1.0))\n",
    "                test_ssim_ldct.append(ssim(ndct_np, ldct_np, data_range=1.0))\n",
    "\n",
    "                test_psnr_restored.append(psnr(ndct_np, restored_np, data_range=1.0))\n",
    "                test_ssim_restored.append(ssim(ndct_np, restored_np, data_range=1.0))\n",
    "\n",
    "                if len(examples_sino['ldct']) < 8:\n",
    "                    examples_sino['ldct'].append(ldct_np)\n",
    "                    examples_sino['ndct'].append(ndct_np)\n",
    "                    examples_sino['restored'].append(restored_np)\n",
    "\n",
    "    # Resultados\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"METRICAS EN TEST SET - MODELO SINOGRAMAS\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Metrica':<20} {'LDCT (ruidosa)':<20} {'Restaurada':<20} {'Ganancia':<15}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    psnr_ldct_mean = np.mean(test_psnr_ldct)\n",
    "    psnr_restored_mean = np.mean(test_psnr_restored)\n",
    "    ssim_ldct_mean = np.mean(test_ssim_ldct)\n",
    "    ssim_restored_mean = np.mean(test_ssim_restored)\n",
    "\n",
    "    print(f\"{'PSNR (dB)':<20} {psnr_ldct_mean:<20.2f} {psnr_restored_mean:<20.2f} {'+' if psnr_restored_mean > psnr_ldct_mean else ''}{psnr_restored_mean - psnr_ldct_mean:.2f}\")\n",
    "    print(f\"{'SSIM':<20} {ssim_ldct_mean:<20.4f} {ssim_restored_mean:<20.4f} {'+' if ssim_restored_mean > ssim_ldct_mean else ''}{ssim_restored_mean - ssim_ldct_mean:.4f}\")\n",
    "\n",
    "    # Visualizar ejemplos\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "    for i in range(min(4, len(examples_sino['ldct']))):\n",
    "        axes[0, i].imshow(examples_sino['ldct'][i], cmap='gray', aspect='auto')\n",
    "        axes[0, i].set_title(f'Low-dose sinogram')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(examples_sino['restored'][i], cmap='gray', aspect='auto')\n",
    "        axes[1, i].set_title(f'Restaurado (DnCNN)')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        axes[2, i].imshow(examples_sino['ndct'][i], cmap='gray', aspect='auto')\n",
    "        axes[2, i].set_title(f'Full-dose (Ground Truth)')\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Evaluacion Modelo DnCNN-Sinogramas en Test Set', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] Evaluacion sinogramas saltada - modelo no disponible\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## B.2: Imagenes CT Reconstruidas (Mayo_Images)\n",
    "\n",
    "El dataset tambien contiene **imagenes CT reconstruidas** que son mas faciles de interpretar visualmente.\n",
    "\n",
    "### Caracteristicas de las imagenes:\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|---------|\n",
    "| **Formato** | DICOM estandar (CT) |\n",
    "| **Tamano** | 512 x 512 por slice |\n",
    "| **Dosis** | Full Dose Images + Low Dose Images |\n",
    "| **Tamano Total** | ~5 GB |\n",
    "\n",
    "### Estructura esperada:\n",
    "```\n",
    "Mayo_Images/\n",
    "    <SeriesInstanceUID>/    # Carpeta por serie\n",
    "        *.dcm               # Slices CT DICOM\n",
    "    ...\n",
    "```\n",
    "\n",
    "> El SeriesDescription del DICOM indica \"Full Dose Images\" o \"Low Dose Images\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.2 - PREPARAR IMAGENES CT MAYO - COPIAR A LOCAL\n",
    "# ============================================================\n",
    "\n",
    "if MAYO_IMG_AVAILABLE:\n",
    "    # Copiar de Drive a local\n",
    "    local_img_path = \"/content/Mayo_Images_local\"\n",
    "\n",
    "    if IN_COLAB and not os.path.exists(local_img_path):\n",
    "        print(\"Copiando imagenes CT de Drive a almacenamiento local...\")\n",
    "        shutil.copytree(MAYO_IMG_PATH, local_img_path)\n",
    "        print(f\"[OK] Datos copiados a {local_img_path}\")\n",
    "    else:\n",
    "        if IN_COLAB:\n",
    "            print(f\"[OK] Datos ya existen en {local_img_path}\")\n",
    "        else:\n",
    "            local_img_path = MAYO_IMG_PATH\n",
    "            print(f\"[OK] Usando datos locales: {local_img_path}\")\n",
    "\n",
    "    MAYO_IMG_PATH_FAST = local_img_path\n",
    "else:\n",
    "    MAYO_IMG_PATH_FAST = None\n",
    "    print(\"[INFO] Imagenes CT Mayo no disponibles, saltando seccion B.2\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.2 - LOADER PARA IMAGENES CT MAYO\n",
    "# ============================================================\n",
    "if not MAYO_IMG_AVAILABLE:\n",
    "    print(\"[INFO] Seccion B.2 saltada - imagenes CT no disponibles\")\n",
    "    loader_img = None\n",
    "else:\n",
    "    class MayoCTImageLoader:\n",
    "        \"\"\"\n",
    "        Cargador para imagenes CT reconstruidas Mayo Clinic (TCIA)\n",
    "\n",
    "        Estructura TCIA:\n",
    "        - Cada serie es una carpeta con SeriesInstanceUID\n",
    "        - SeriesDescription indica \"Full Dose Images\" o \"Low Dose Images\"\n",
    "        - Cada DICOM es un slice CT (512x512)\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, base_path):\n",
    "            self.base_path = Path(base_path)\n",
    "            self.series_info = self._scan_series()\n",
    "            self.patients = self._group_by_patient()\n",
    "\n",
    "        def _scan_series(self):\n",
    "            \"\"\"Escanea todas las series y extrae metadata\"\"\"\n",
    "            series = []\n",
    "\n",
    "            for series_dir in self.base_path.iterdir():\n",
    "                if not series_dir.is_dir():\n",
    "                    continue\n",
    "\n",
    "                dcm_files = list(series_dir.glob(\"*.dcm\"))\n",
    "                if not dcm_files:\n",
    "                    continue\n",
    "\n",
    "                # Leer metadata del primer DICOM\n",
    "                try:\n",
    "                    ds = pydicom.dcmread(str(dcm_files[0]), stop_before_pixels=True)\n",
    "                    series.append({\n",
    "                        'path': series_dir,\n",
    "                        'series_uid': series_dir.name,\n",
    "                        'patient_id': getattr(ds, 'PatientID', 'Unknown'),\n",
    "                        'series_desc': getattr(ds, 'SeriesDescription', ''),\n",
    "                        'n_slices': len(dcm_files)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARNING] Error leyendo {series_dir.name}: {e}\")\n",
    "\n",
    "            print(f\"[OK] {len(series)} series de imagenes CT encontradas\")\n",
    "            return series\n",
    "\n",
    "        def _group_by_patient(self):\n",
    "            \"\"\"Agrupa series por paciente, identificando pares full/low dose\"\"\"\n",
    "            patients = defaultdict(dict)\n",
    "\n",
    "            for s in self.series_info:\n",
    "                pid = s['patient_id']\n",
    "                desc = s['series_desc'].lower()\n",
    "\n",
    "                # Imagenes CT usan \"Full Dose Images\" y \"Low Dose Images\"\n",
    "                if 'full' in desc and 'image' in desc:\n",
    "                    patients[pid]['full'] = s\n",
    "                elif 'low' in desc and 'image' in desc:\n",
    "                    patients[pid]['low'] = s\n",
    "\n",
    "            # Solo mantener pacientes con ambas series\n",
    "            complete = {pid: data for pid, data in patients.items()\n",
    "                       if 'full' in data and 'low' in data}\n",
    "\n",
    "            print(f\"[OK] {len(complete)} pacientes con pares completos de imagenes\")\n",
    "            return complete\n",
    "\n",
    "        def load_patient_pair(self, patient_id, slice_idx=0):\n",
    "            \"\"\"Carga un par de slices CT (full/low dose)\"\"\"\n",
    "            patient = self.patients.get(patient_id)\n",
    "            if not patient:\n",
    "                raise ValueError(f\"Paciente {patient_id} no encontrado\")\n",
    "\n",
    "            full_path = patient['full']['path']\n",
    "            low_path = patient['low']['path']\n",
    "\n",
    "            # Obtener archivos ordenados\n",
    "            full_files = sorted(full_path.glob(\"*.dcm\"))\n",
    "            low_files = sorted(low_path.glob(\"*.dcm\"))\n",
    "\n",
    "            # Ajustar indice\n",
    "            slice_idx = min(slice_idx, len(full_files)-1, len(low_files)-1)\n",
    "\n",
    "            # Cargar DICOMs\n",
    "            full_ds = pydicom.dcmread(str(full_files[slice_idx]))\n",
    "            low_ds = pydicom.dcmread(str(low_files[slice_idx]))\n",
    "\n",
    "            # Extraer pixel array y aplicar rescale\n",
    "            full_img = full_ds.pixel_array.astype(np.float32)\n",
    "            low_img = low_ds.pixel_array.astype(np.float32)\n",
    "\n",
    "            # Aplicar rescale slope/intercept si existen\n",
    "            if hasattr(full_ds, 'RescaleSlope'):\n",
    "                full_img = full_img * full_ds.RescaleSlope + full_ds.RescaleIntercept\n",
    "                low_img = low_img * low_ds.RescaleSlope + low_ds.RescaleIntercept\n",
    "\n",
    "            return full_img, low_img\n",
    "\n",
    "        def get_patient_ids(self):\n",
    "            return list(self.patients.keys())\n",
    "\n",
    "    # Instanciar loader\n",
    "    loader_img = MayoCTImageLoader(MAYO_IMG_PATH_FAST)\n",
    "\n",
    "    # Mostrar pacientes disponibles\n",
    "    print(f\"\\nPacientes disponibles: {loader_img.get_patient_ids()}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.2 - DATASET DE IMAGENES CT MAYO\n",
    "# ============================================================\n",
    "if not MAYO_IMG_AVAILABLE:\n",
    "    train_loader_img = None\n",
    "    test_loader_img = None\n",
    "else:\n",
    "    class MayoCTDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Dataset de pares de imagenes CT Full/Low dose.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, loader, patient_ids=None, slices_per_patient=50,\n",
    "                     target_size=(128, 128), augment=False):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                loader: MayoCTImageLoader\n",
    "                patient_ids: Lista de pacientes (None = todos)\n",
    "                slices_per_patient: Slices a muestrear por paciente\n",
    "                target_size: Tamano de salida (resize)\n",
    "                augment: Aplicar data augmentation\n",
    "            \"\"\"\n",
    "            self.loader = loader\n",
    "            self.target_size = target_size\n",
    "            self.augment = augment\n",
    "\n",
    "            if patient_ids is None:\n",
    "                self.patient_ids = list(loader.patients.keys())\n",
    "            else:\n",
    "                self.patient_ids = [p for p in patient_ids if p in loader.patients]\n",
    "\n",
    "            # Crear lista de samples (pid, slice_idx)\n",
    "            self.samples = []\n",
    "            for pid in self.patient_ids:\n",
    "                n_slices = loader.patients[pid]['full']['n_slices']\n",
    "                # Muestrear uniformemente\n",
    "                indices = np.linspace(0, n_slices-1, min(slices_per_patient, n_slices), dtype=int)\n",
    "                for idx in indices:\n",
    "                    self.samples.append((pid, int(idx)))\n",
    "\n",
    "            print(f\"[OK] Dataset CT: {len(self.samples)} slices de {len(self.patient_ids)} pacientes\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            pid, slice_idx = self.samples[idx]\n",
    "\n",
    "            # Cargar imagenes CT\n",
    "            full_img, low_img = self.loader.load_patient_pair(pid, slice_idx)\n",
    "\n",
    "            # Convertir a float32\n",
    "            full_img = full_img.astype(np.float32)\n",
    "            low_img = low_img.astype(np.float32)\n",
    "\n",
    "            # Normalizar HU a [0, 1] (ventana -1000 a 400 HU)\n",
    "            def normalize_hu(img, window_min=-1000, window_max=400):\n",
    "                img = np.clip(img, window_min, window_max)\n",
    "                return (img - window_min) / (window_max - window_min)\n",
    "\n",
    "            full_img = normalize_hu(full_img)\n",
    "            low_img = normalize_hu(low_img)\n",
    "\n",
    "            # Resize si es necesario\n",
    "            if full_img.shape != self.target_size:\n",
    "                full_img = resize(full_img, self.target_size, preserve_range=True, anti_aliasing=True)\n",
    "                low_img = resize(low_img, self.target_size, preserve_range=True, anti_aliasing=True)\n",
    "\n",
    "            # Data augmentation\n",
    "            if self.augment:\n",
    "                if np.random.rand() > 0.5:\n",
    "                    full_img = np.fliplr(full_img).copy()\n",
    "                    low_img = np.fliplr(low_img).copy()\n",
    "                if np.random.rand() > 0.5:\n",
    "                    full_img = np.flipud(full_img).copy()\n",
    "                    low_img = np.flipud(low_img).copy()\n",
    "\n",
    "            # Convertir a tensor (C, H, W)\n",
    "            full_tensor = torch.from_numpy(full_img).unsqueeze(0).float()\n",
    "            low_tensor = torch.from_numpy(low_img).unsqueeze(0).float()\n",
    "\n",
    "            return low_tensor, full_tensor  # (noisy, clean)\n",
    "\n",
    "    # Dividir pacientes en train/test\n",
    "    patient_ids = loader_img.get_patient_ids()\n",
    "    n_train = max(1, int(len(patient_ids) * 0.8))\n",
    "    train_patients = patient_ids[:n_train]\n",
    "    test_patients = patient_ids[n_train:] if n_train < len(patient_ids) else patient_ids[:1]\n",
    "\n",
    "    print(f\"\\nDivision de datos CT:\")\n",
    "    print(f\"  Train: {train_patients}\")\n",
    "    print(f\"  Test: {test_patients}\")\n",
    "\n",
    "    # Crear datasets\n",
    "    train_dataset_img = MayoCTDataset(\n",
    "        loader_img,\n",
    "        patient_ids=train_patients,\n",
    "        slices_per_patient=50,\n",
    "        target_size=(128, 128),\n",
    "        augment=True\n",
    "    )\n",
    "\n",
    "    test_dataset_img = MayoCTDataset(\n",
    "        loader_img,\n",
    "        patient_ids=test_patients,\n",
    "        slices_per_patient=20,\n",
    "        target_size=(128, 128),\n",
    "        augment=False\n",
    "    )\n",
    "\n",
    "    # Crear dataloaders\n",
    "    train_loader_img = DataLoader(train_dataset_img, batch_size=8, shuffle=True, num_workers=0)\n",
    "    test_loader_img = DataLoader(test_dataset_img, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"\\n[OK] DataLoaders CT creados:\")\n",
    "    print(f\"  Train: {len(train_loader_img)} batches\")\n",
    "    print(f\"  Test: {len(test_loader_img)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualizar ejemplos de imagenes CT\n",
    "if train_loader_img is not None:\n",
    "    low_batch, full_batch = next(iter(train_loader_img))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "    for i in range(min(4, low_batch.size(0))):\n",
    "        axes[0, i].imshow(low_batch[i, 0].numpy(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Low-dose CT', fontsize=11)\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(full_batch[i, 0].numpy(), cmap='gray')\n",
    "        axes[1, i].set_title(f'Full-dose CT', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Pares de Imagenes CT Mayo Clinic (128x128)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mostrar diferencia\n",
    "    diff = torch.abs(full_batch - low_batch)\n",
    "    print(f\"\\nEstadisticas:\")\n",
    "    print(f\"  Diferencia media: {diff.mean():.4f}\")\n",
    "    print(f\"  Diferencia max: {diff.max():.4f}\")\n",
    "else:\n",
    "    print(\"[INFO] No hay imagenes CT Mayo para visualizar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Entrenamiento con Imagenes CT Mayo\n",
    "\n",
    "Entrenamos un segundo modelo DnCNN especializado en imagenes CT reconstruidas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.2 - ENTRENAMIENTO CON IMAGENES CT\n",
    "# ============================================================\n",
    "\n",
    "if train_loader_img is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENAMIENTO - MODELO DNCNN PARA IMAGENES CT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Crear modelo para imagenes CT\n",
    "    model_ct = DnCNN(channels=1, num_layers=17, features=64).to(device)\n",
    "    print(f\"[OK] Modelo DnCNN-CT creado en {device}\")\n",
    "\n",
    "    # Directorio para checkpoints\n",
    "    ct_save_dir = os.path.join(weights_dir, \"dncnn_ct_checkpoints\")\n",
    "    os.makedirs(ct_save_dir, exist_ok=True)\n",
    "\n",
    "    # Entrenar\n",
    "    history_ct, model_ct = train_denoiser_with_checkpoints(\n",
    "        model_ct,\n",
    "        train_loader_img,\n",
    "        test_loader=test_loader_img,\n",
    "        epochs=15,\n",
    "        lr=1e-3,\n",
    "        save_dir=ct_save_dir\n",
    "    )\n",
    "\n",
    "    # Guardar modelo final\n",
    "    final_path = os.path.join(weights_dir, \"dncnn_ct_final.pth\")\n",
    "    torch.save(model_ct.state_dict(), final_path)\n",
    "    print(f\"\\n[OK] Modelo CT guardado: {final_path}\")\n",
    "else:\n",
    "    model_ct = None\n",
    "    history_ct = None\n",
    "    print(\"[INFO] Entrenamiento CT saltado - datos no disponibles\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.2 - EVALUACION MODELO CT EN TEST SET\n",
    "# ============================================================\n",
    "\n",
    "if model_ct is not None and test_loader_img is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUACION EN TEST SET - MODELO CT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model_ct.eval()\n",
    "\n",
    "    test_psnr_ldct = []\n",
    "    test_psnr_restored = []\n",
    "    test_ssim_ldct = []\n",
    "    test_ssim_restored = []\n",
    "\n",
    "    examples_ct = {'ldct': [], 'ndct': [], 'restored': []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ldct, ndct in tqdm(test_loader_img, desc=\"Evaluando test set CT\"):\n",
    "            ldct = ldct.to(device)\n",
    "            ndct_gpu = ndct.to(device)\n",
    "            restored = model_ct(ldct)\n",
    "\n",
    "            for i in range(ldct.size(0)):\n",
    "                ldct_np = ldct[i, 0].cpu().numpy()\n",
    "                ndct_np = ndct[i, 0].cpu().numpy()\n",
    "                restored_np = restored[i, 0].cpu().numpy()\n",
    "\n",
    "                test_psnr_ldct.append(psnr(ndct_np, ldct_np, data_range=1.0))\n",
    "                test_ssim_ldct.append(ssim(ndct_np, ldct_np, data_range=1.0))\n",
    "\n",
    "                test_psnr_restored.append(psnr(ndct_np, restored_np, data_range=1.0))\n",
    "                test_ssim_restored.append(ssim(ndct_np, restored_np, data_range=1.0))\n",
    "\n",
    "                if len(examples_ct['ldct']) < 8:\n",
    "                    examples_ct['ldct'].append(ldct_np)\n",
    "                    examples_ct['ndct'].append(ndct_np)\n",
    "                    examples_ct['restored'].append(restored_np)\n",
    "\n",
    "    # Resultados\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"METRICAS EN TEST SET - MODELO CT\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Metrica':<20} {'LDCT (ruidosa)':<20} {'Restaurada':<20} {'Ganancia':<15}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    psnr_ldct_mean = np.mean(test_psnr_ldct)\n",
    "    psnr_restored_mean = np.mean(test_psnr_restored)\n",
    "    ssim_ldct_mean = np.mean(test_ssim_ldct)\n",
    "    ssim_restored_mean = np.mean(test_ssim_restored)\n",
    "\n",
    "    print(f\"{'PSNR (dB)':<20} {psnr_ldct_mean:<20.2f} {psnr_restored_mean:<20.2f} {'+' if psnr_restored_mean > psnr_ldct_mean else ''}{psnr_restored_mean - psnr_ldct_mean:.2f}\")\n",
    "    print(f\"{'SSIM':<20} {ssim_ldct_mean:<20.4f} {ssim_restored_mean:<20.4f} {'+' if ssim_restored_mean > ssim_ldct_mean else ''}{ssim_restored_mean - ssim_ldct_mean:.4f}\")\n",
    "\n",
    "    # Visualizar ejemplos\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "    for i in range(min(4, len(examples_ct['ldct']))):\n",
    "        axes[0, i].imshow(examples_ct['ldct'][i], cmap='gray')\n",
    "        axes[0, i].set_title(f'Low-dose CT')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(examples_ct['restored'][i], cmap='gray')\n",
    "        axes[1, i].set_title(f'Restaurada (DnCNN)')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        axes[2, i].imshow(examples_ct['ndct'][i], cmap='gray')\n",
    "        axes[2, i].set_title(f'Full-dose (Ground Truth)')\n",
    "        axes[2, i].axis('off')\n",
    "\n",
    "    plt.suptitle('Evaluacion Modelo DnCNN-CT en Test Set', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] Evaluacion CT saltada - modelo no disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Comparacion de Modelos\n",
    "\n",
    "Comparamos los dos modelos entrenados:\n",
    "1. **DnCNN-Proyecciones**: Entrenado con sinogramas/proyecciones\n",
    "2. **DnCNN-CT**: Entrenado con imagenes CT reconstruidas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARACION DE MODELOS ENTRENADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DE MODELOS ENTRENADOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelos = []\n",
    "\n",
    "# Modelo para proyecciones\n",
    "if 'model_mayo' in dir() and model_mayo is not None:\n",
    "    modelos.append({\n",
    "        'Nombre': 'DnCNN-Proyecciones',\n",
    "        'Dominio': 'Sinogramas (736x64 -> 64x64)',\n",
    "        'Datos': 'Mayo_LDCT',\n",
    "        'Archivo': 'dncnn_sinograms_final.pth'\n",
    "    })\n",
    "    print(f\"[OK] DnCNN-Proyecciones entrenado\")\n",
    "\n",
    "# Modelo para imagenes CT\n",
    "if 'model_ct' in dir() and model_ct is not None:\n",
    "    modelos.append({\n",
    "        'Nombre': 'DnCNN-CT',\n",
    "        'Dominio': 'Imagenes CT (512x512 -> 128x128)',\n",
    "        'Datos': 'Mayo_Images',\n",
    "        'Archivo': 'dncnn_ct_final.pth'\n",
    "    })\n",
    "    print(f\"[OK] DnCNN-CT entrenado\")\n",
    "\n",
    "if modelos:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    df_modelos = pd.DataFrame(modelos)\n",
    "    print(df_modelos.to_string(index=False))\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    print(f\"\\nModelos guardados en: {weights_dir}\")\n",
    "else:\n",
    "    print(\"[INFO] No hay modelos entrenados para comparar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Resumen y Referencias\n",
    "\n",
    "### Logros de este notebook:\n",
    "\n",
    "1. **Simulacion de ruido LDCT** a partir de datos LUNA16 usando el modelo de Yu et al. 2012\n",
    "2. **Comparacion de tecnicas clasicas** de denoising (Gaussian, Bilateral, NLM, BM3D)\n",
    "3. **Implementacion de DnCNN** para denoising de CT\n",
    "4. **Entrenamiento con datos reales Mayo**:\n",
    "   - Modelo DnCNN-Proyecciones (sinogramas)\n",
    "   - Modelo DnCNN-CT (imagenes reconstruidas)\n",
    "\n",
    "### Referencias:\n",
    "\n",
    "- Yu et al. (2012) - \"Development and validation of a practical lower-dose-simulation tool for optimizing CT scan protocols\"\n",
    "- Zhang et al. (2017) - \"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising\" (DnCNN)\n",
    "- McCollough et al. (2017) - \"Low-dose CT for the detection and classification of metastatic liver cancer\" (Mayo Dataset)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL NOTEBOOK 05: DENOISING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. MODELOS DE RUIDO:\")\n",
    "print(\"   - Gaussiano (aditivo)\")\n",
    "print(\"   - Poisson (cuantico)\")\n",
    "print(\"   - LDCT realista (combinado)\")\n",
    "\n",
    "print(\"\\n2. TECNICAS DE DENOISING:\")\n",
    "print(\"   Clasicas: Gaussiano, Mediana, Bilateral, TV, NLM\")\n",
    "print(\"   Deep Learning: DnCNN (17 capas, residual learning)\")\n",
    "\n",
    "print(\"\\n3. DATASETS SOPORTADOS:\")\n",
    "if MAYO_AVAILABLE:\n",
    "    mayo_patients = len(mayo_loader.patients) if mayo_loader else 0\n",
    "    print(f\"   - Mayo Clinic: Pares reales NDCT/LDCT ({mayo_patients} pacientes)\")\n",
    "else:\n",
    "    print(\"   - LUNA16: Ruido simulado\")\n",
    "    print(\"   - Mayo Clinic: No descargado (ver instrucciones arriba)\")\n",
    "\n",
    "print(\"\\n4. RESULTADOS:\")\n",
    "if results is not None:\n",
    "    print(f\"   LDCT sin procesar: {results['psnr_noisy']:.2f} dB\")\n",
    "    print(f\"   DnCNN:             {results['psnr_denoised']:.2f} dB (+{results['psnr_gain']:.2f} dB)\")\n",
    "elif MAYO_AVAILABLE:\n",
    "    print(\"   Ver seccion de entrenamiento con datos Mayo\")\n",
    "else:\n",
    "    print(\"   Ejecutar celdas de entrenamiento primero\")\n",
    "\n",
    "print(\"\\n5. ARCHIVOS GENERADOS:\")\n",
    "print(f\"   - {weights_dir}/dncnn_denoising.pth (ruido simulado)\")\n",
    "if MAYO_AVAILABLE:\n",
    "    print(f\"   - {weights_dir}/dncnn_mayo_real.pth (datos reales)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Para mejorar resultados:\")\n",
    "print(\"  1. Descargar Mayo Clinic LDCT de TCIA (si no disponible)\")\n",
    "print(\"  2. Entrenar mas epocas (50-100)\")\n",
    "print(\"  3. Usar arquitecturas avanzadas (RED-CNN, WGAN)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}