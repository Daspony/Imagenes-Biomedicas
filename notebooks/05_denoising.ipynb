{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Simulación de Ruido y Denoising en CT\n",
    "\n",
    "Este notebook explora técnicas de simulación de ruido y reducción de ruido (denoising) en imágenes CT pulmonares.\n",
    "\n",
    "## Contenido:\n",
    "1. **Simulación de Ruido**: Convertir NDCT (Normal Dose) a LDCT (Low Dose)\n",
    "2. **Modelos de Ruido**: Poisson\n",
    "3. **Técnicas de Denoising**: Filtros clásicos vs Deep Learning\n",
    "4. **Métricas de Calidad**: PSNR, SSIM, MSE\n",
    "5. **Dataset Mayo Clinic (TCIA)**: Pares NDCT/LDCT simulados con un algoritmo validado\n",
    "\n",
    "---\n",
    "\n",
    "## Contexto: NDCT vs LDCT\n",
    "\n",
    "| Tipo | Dosis | Ruido | Uso Clínico |\n",
    "|------|-------|-------|-------------|\n",
    "| **NDCT** (Normal Dose CT) | ~definir | Bajo | Diagnóstico estándar |\n",
    "| **LDCT** (Low Dose CT) | ~definir | Alto | Screening, seguimiento |\n",
    "\n",
    "**Objetivo**: \n",
    "- Simular ruido LDCT a partir de NDCT para entrenar modelos de denoising\n",
    "- Usar datos reales del dataset Mayo Clinic LDCT (TCIA) para entrenar modelos de denoising\n",
    "- Comparar resultado de ambos\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets Disponibles\n",
    "\n",
    "| Dataset | Tipo | Tamaño | Acceso |\n",
    "|---------|------|--------|--------|\n",
    "| **LUNA16** | Solo NDCT (simular ruido) | ~13 GB | Automático |\n",
    "| **Mayo Clinic LDCT (TCIA)** | Pares NDCT/LDCT reales | ~1.32 TB | Manual (requiere descarga) |\n",
    "\n",
    "**Referencia**: \n",
    "- Chen et al. 2016 - \"An Open Library of CT Patient Projection Data\" (SPIE)\n",
    "- \n",
    "            \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar si estamos en Google Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Ejecutando en Google Colab\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Instalar dependencias\n",
    "    print(\"\\nInstalando dependencias...\")\n",
    "    import subprocess\n",
    "    paquetes = ['SimpleITK', 'scikit-image', 'requests', 'tqdm']\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + paquetes)\n",
    "    \n",
    "    # Clonar repositorio desde GitHub\n",
    "    print(\"\\nClonando repositorio desde GitHub...\")\n",
    "    repo_url = \"https://github.com/Daspony/Imagenes-Biomedicas.git\"\n",
    "    repo_name = \"Imagenes-Biomedicas\"\n",
    "    \n",
    "    if not os.path.exists(f\"/content/{repo_name}\"):\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], cwd=\"/content\", check=True)\n",
    "        print(f\"Repositorio clonado en /content/{repo_name}\")\n",
    "    else:\n",
    "        print(f\"Repositorio ya existe en /content/{repo_name}\")\n",
    "    \n",
    "    # Añadir al path\n",
    "    sys.path.insert(0, f\"/content/{repo_name}\")\n",
    "    \n",
    "    print(\"Configuración de Colab completada\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"Ejecutando localmente\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Añadir directorio padre al path para importar utils\n",
    "    parent_dir = os.path.abspath('..')\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.insert(0, parent_dir)\n",
    "    \n",
    "    print(f\"Directorio de trabajo: {os.getcwd()}\")\n",
    "    print(\"Configuración local completada\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Procesamiento de imágenes\n",
    "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral, denoise_nl_means\n",
    "from skimage.filters import gaussian, median\n",
    "from skimage.morphology import disk\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.ndimage import uniform_filter, median_filter\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Importar nuestros módulos\n",
    "from utils import LUNA16DataLoader, LungPreprocessor, LungVisualizer, download_luna16\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\nLibrerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "if IN_COLAB:\n",
    "    luna16_path = '/content/LUNA16'\n",
    "    download_luna16(subsets=0, include_csv=True, download_dir=luna16_path)\n",
    "    DATA_PATH = os.path.join(luna16_path, 'subset0')\n",
    "    ANNOTATIONS_PATH = os.path.join(luna16_path, 'annotations.csv')\n",
    "else:\n",
    "    project_root = os.path.abspath('..')\n",
    "    luna16_path = os.path.join(project_root, 'LUNA16')\n",
    "    download_luna16(subsets=0, include_csv=True, download_dir=luna16_path)\n",
    "    DATA_PATH = os.path.join(luna16_path, 'subset0')\n",
    "    ANNOTATIONS_PATH = os.path.join(luna16_path, 'annotations.csv')\n",
    "\n",
    "# Verificar rutas\n",
    "print(f\"Datos: {DATA_PATH}\")\n",
    "print(f\"Anotaciones: {ANNOTATIONS_PATH}\")\n",
    "print(f\"Anotaciones existe: {os.path.exists(ANNOTATIONS_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar un escaneo de ejemplo\n",
    "loader = LUNA16DataLoader(DATA_PATH, ANNOTATIONS_PATH)\n",
    "preprocessor = LungPreprocessor()\n",
    "\n",
    "# Obtener primer archivo MHD\n",
    "mhd_files = list(Path(DATA_PATH).glob(\"*.mhd\"))\n",
    "print(f\"Escaneos disponibles: {len(mhd_files)}\")\n",
    "\n",
    "# Cargar escaneo\n",
    "sample_path = str(mhd_files[0])\n",
    "ct_scan, origin, spacing = loader.load_itk_image(sample_path)\n",
    "print(f\"\\nVolumen cargado: {ct_scan.shape}\")\n",
    "print(f\"Spacing: {spacing}\")\n",
    "\n",
    "# Seleccionar un slice central\n",
    "slice_idx = ct_scan.shape[0] // 2\n",
    "original_slice = ct_scan[slice_idx].copy()\n",
    "print(f\"\\nSlice seleccionado: {slice_idx}\")\n",
    "print(f\"Rango HU: [{original_slice.min():.0f}, {original_slice.max():.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Modelos de Ruido en CT\n",
    "\n",
    "El ruido en imágenes CT tiene características específicas:\n",
    "\n",
    "1. **Ruido Cuántico (Poisson)**: Dominante en CT, proporcional a √(intensidad)\n",
    "2. **Ruido Electrónico (Gaussiano)**: Del detector, aditivo\n",
    "3. **Ruido Estructurado**: Artefactos de reconstrucción\n",
    "\n",
    "### Modelo simplificado para LDCT:\n",
    "$$I_{LDCT} = I_{NDCT} + \\sigma \\cdot \\sqrt{I_{NDCT}} \\cdot \\epsilon$$\n",
    "\n",
    "donde $\\epsilon \\sim N(0,1)$ y $\\sigma$ controla el nivel de ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image, sigma=25):\n",
    "    \"\"\"\n",
    "    Añade ruido gaussiano aditivo\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen original (valores HU)\n",
    "        sigma: Desviación estándar del ruido\n",
    "    \n",
    "    Returns:\n",
    "        Imagen con ruido gaussiano\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, sigma, image.shape)\n",
    "    noisy = image + noise\n",
    "    return noisy.astype(image.dtype)\n",
    "\n",
    "\n",
    "def add_poisson_noise(image, scale=1.0):\n",
    "    \"\"\"\n",
    "    Añade ruido Poisson (ruido cuántico)\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen original (valores HU)\n",
    "        scale: Factor de escala para intensidad del ruido\n",
    "    \n",
    "    Returns:\n",
    "        Imagen con ruido Poisson\n",
    "    \"\"\"\n",
    "    # Normalizar a valores positivos para Poisson\n",
    "    min_val = image.min()\n",
    "    image_shifted = image - min_val + 1  # Asegurar valores positivos\n",
    "    \n",
    "    # Escalar para controlar intensidad del ruido\n",
    "    image_scaled = image_shifted / scale\n",
    "    \n",
    "    # Aplicar ruido Poisson\n",
    "    noisy = np.random.poisson(image_scaled) * scale\n",
    "    \n",
    "    # Restaurar rango original\n",
    "    noisy = noisy + min_val - 1\n",
    "    \n",
    "    return noisy.astype(image.dtype)\n",
    "\n",
    "\n",
    "def add_ct_realistic_noise(image, dose_ratio=0.25, sigma_base=30):\n",
    "    \"\"\"\n",
    "    Simula ruido realista de CT de baja dosis\n",
    "    \n",
    "    Combina:\n",
    "    - Ruido dependiente de la señal (tipo Poisson)\n",
    "    - Ruido gaussiano de fondo\n",
    "    \n",
    "    Args:\n",
    "        image: Imagen NDCT original (valores HU)\n",
    "        dose_ratio: Ratio de dosis (0.25 = 1/4 de la dosis normal)\n",
    "        sigma_base: Nivel base de ruido gaussiano\n",
    "    \n",
    "    Returns:\n",
    "        Imagen simulando LDCT\n",
    "    \"\"\"\n",
    "    # Factor de ruido inversamente proporcional a la raíz de la dosis\n",
    "    noise_factor = 1.0 / np.sqrt(dose_ratio)\n",
    "    \n",
    "    # Ruido dependiente de la señal (aproximación al ruido cuántico)\n",
    "    # En regiones más densas (valores HU más altos), el ruido es mayor\n",
    "    image_normalized = (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
    "    signal_dependent_noise = np.random.normal(0, 1, image.shape) * np.sqrt(image_normalized + 0.1)\n",
    "    \n",
    "    # Ruido gaussiano de fondo\n",
    "    gaussian_noise = np.random.normal(0, 1, image.shape)\n",
    "    \n",
    "    # Combinar ruidos\n",
    "    sigma = sigma_base * noise_factor\n",
    "    total_noise = sigma * (0.7 * signal_dependent_noise + 0.3 * gaussian_noise)\n",
    "    \n",
    "    noisy = image + total_noise\n",
    "    return noisy.astype(image.dtype)\n",
    "\n",
    "\n",
    "print(\"Funciones de ruido definidas:\")\n",
    "print(\"  - add_gaussian_noise(): Ruido gaussiano aditivo\")\n",
    "print(\"  - add_poisson_noise(): Ruido Poisson (cuántico)\")\n",
    "print(\"  - add_ct_realistic_noise(): Simulación LDCT realista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar diferentes tipos de ruido\n",
    "noisy_gaussian = add_gaussian_noise(original_slice, sigma=50)\n",
    "noisy_poisson = add_poisson_noise(original_slice, scale=100)\n",
    "noisy_ldct = add_ct_realistic_noise(original_slice, dose_ratio=0.25)\n",
    "\n",
    "# Normalizar para visualización\n",
    "def normalize_for_display(img, min_hu=-1000, max_hu=400):\n",
    "    img_clipped = np.clip(img, min_hu, max_hu)\n",
    "    return (img_clipped - min_hu) / (max_hu - min_hu)\n",
    "\n",
    "orig_display = normalize_for_display(original_slice)\n",
    "gauss_display = normalize_for_display(noisy_gaussian)\n",
    "poisson_display = normalize_for_display(noisy_poisson)\n",
    "ldct_display = normalize_for_display(noisy_ldct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparación de tipos de ruido\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Fila 1: Imágenes completas\n",
    "axes[0, 0].imshow(orig_display, cmap='bone')\n",
    "axes[0, 0].set_title('Original (NDCT)', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(gauss_display, cmap='bone')\n",
    "axes[0, 1].set_title('Ruido Gaussiano\\n(σ=50)', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(poisson_display, cmap='bone')\n",
    "axes[0, 2].set_title('Ruido Poisson\\n(scale=100)', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(ldct_display, cmap='bone')\n",
    "axes[0, 3].set_title('LDCT Simulado\\n(dose=25%)', fontsize=12)\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Fila 2: Zoom en región de interés\n",
    "y1, y2, x1, x2 = 200, 300, 200, 300  # Región de zoom\n",
    "\n",
    "axes[1, 0].imshow(orig_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 0].set_title('Zoom Original', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(gauss_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 1].set_title('Zoom Gaussiano', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(poisson_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 2].set_title('Zoom Poisson', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(ldct_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 3].set_title('Zoom LDCT', fontsize=12)\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Comparación de Modelos de Ruido en CT', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de calidad\n",
    "print(\"Métricas de Calidad de Imagen (respecto al original):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usar imágenes normalizadas para métricas\n",
    "metrics = []\n",
    "\n",
    "for name, noisy in [('Gaussiano', gauss_display), \n",
    "                    ('Poisson', poisson_display), \n",
    "                    ('LDCT Sim.', ldct_display)]:\n",
    "    psnr_val = psnr(orig_display, noisy, data_range=1.0)\n",
    "    ssim_val = ssim(orig_display, noisy, data_range=1.0)\n",
    "    mse_val = np.mean((orig_display - noisy)**2)\n",
    "    \n",
    "    metrics.append({\n",
    "        'Tipo': name,\n",
    "        'PSNR (dB)': f'{psnr_val:.2f}',\n",
    "        'SSIM': f'{ssim_val:.4f}',\n",
    "        'MSE': f'{mse_val:.6f}'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  PSNR: {psnr_val:.2f} dB\")\n",
    "    print(f\"  SSIM: {ssim_val:.4f}\")\n",
    "    print(f\"  MSE:  {mse_val:.6f}\")\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Variación del Nivel de Ruido\n",
    "\n",
    "Exploramos cómo diferentes niveles de dosis afectan la calidad de imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular diferentes niveles de dosis\n",
    "dose_ratios = [1.0, 0.5, 0.25, 0.125, 0.0625]  # 100%, 50%, 25%, 12.5%, 6.25%\n",
    "dose_labels = ['100%', '50%', '25%', '12.5%', '6.25%']\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i, (dose, label) in enumerate(zip(dose_ratios, dose_labels)):\n",
    "    if dose == 1.0:\n",
    "        noisy = original_slice.copy()\n",
    "    else:\n",
    "        noisy = add_ct_realistic_noise(original_slice, dose_ratio=dose)\n",
    "    \n",
    "    noisy_display = normalize_for_display(noisy)\n",
    "    \n",
    "    # Imagen completa\n",
    "    axes[0, i].imshow(noisy_display, cmap='bone')\n",
    "    axes[0, i].set_title(f'Dosis: {label}', fontsize=12)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Zoom\n",
    "    axes[1, i].imshow(noisy_display[200:300, 200:300], cmap='bone')\n",
    "    \n",
    "    # Calcular PSNR solo para imágenes con ruido\n",
    "    if dose < 1.0:\n",
    "        psnr_val = psnr(orig_display, noisy_display, data_range=1.0)\n",
    "        axes[1, i].set_title(f'PSNR: {psnr_val:.1f} dB', fontsize=11)\n",
    "    else:\n",
    "        axes[1, i].set_title('Referencia', fontsize=11)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Efecto del Nivel de Dosis en la Calidad de Imagen CT', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Técnicas Clásicas de Denoising\n",
    "\n",
    "Antes de deep learning, se usaban filtros clásicos:\n",
    "\n",
    "| Técnica | Descripción | Pros | Contras |\n",
    "|---------|-------------|------|----------|\n",
    "| Gaussiano | Suavizado isotrópico | Rápido | Pierde bordes |\n",
    "| Mediana | Filtro no lineal | Preserva bordes | Pierde detalles finos |\n",
    "| Bilateral | Preserva bordes | Buenos resultados | Lento |\n",
    "| TV (Total Variation) | Minimiza variación | Preserva bordes | Efecto \"cartoon\" |\n",
    "| NLM (Non-Local Means) | Usa parches similares | Muy buenos resultados | Muy lento |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classical_denoising(image_normalized):\n",
    "    \"\"\"\n",
    "    Aplica diferentes técnicas clásicas de denoising\n",
    "    \n",
    "    Args:\n",
    "        image_normalized: Imagen normalizada [0, 1]\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con resultados de cada técnica\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Filtro Gaussiano\n",
    "    results['Gaussiano'] = gaussian(image_normalized, sigma=1.5)\n",
    "    \n",
    "    # 2. Filtro Mediana\n",
    "    results['Mediana'] = median_filter(image_normalized, size=3)\n",
    "    \n",
    "    # 3. Filtro Bilateral\n",
    "    results['Bilateral'] = denoise_bilateral(\n",
    "        image_normalized, \n",
    "        sigma_color=0.1, \n",
    "        sigma_spatial=5,\n",
    "        channel_axis=None\n",
    "    )\n",
    "    \n",
    "    # 4. Total Variation\n",
    "    results['TV'] = denoise_tv_chambolle(image_normalized, weight=0.1)\n",
    "    \n",
    "    # 5. Non-Local Means (versión rápida)\n",
    "    results['NLM'] = denoise_nl_means(\n",
    "        image_normalized,\n",
    "        h=0.1,\n",
    "        patch_size=5,\n",
    "        patch_distance=6,\n",
    "        channel_axis=None\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Funciones de denoising clásico definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar denoising a imagen LDCT simulada\n",
    "print(\"Aplicando técnicas de denoising...\")\n",
    "print(\"(Esto puede tomar unos segundos)\")\n",
    "\n",
    "denoised_results = apply_classical_denoising(ldct_display)\n",
    "\n",
    "print(\"\\nDenoising completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de denoising\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Fila 1: Imágenes completas\n",
    "axes[0, 0].imshow(orig_display, cmap='bone')\n",
    "axes[0, 0].set_title('Original (NDCT)', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(ldct_display, cmap='bone')\n",
    "psnr_noisy = psnr(orig_display, ldct_display, data_range=1.0)\n",
    "axes[0, 1].set_title(f'LDCT Simulado\\nPSNR: {psnr_noisy:.1f} dB', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Mejores técnicas\n",
    "best_techniques = ['Bilateral', 'NLM']\n",
    "for i, tech in enumerate(best_techniques):\n",
    "    denoised = denoised_results[tech]\n",
    "    psnr_val = psnr(orig_display, denoised, data_range=1.0)\n",
    "    axes[0, i+2].imshow(denoised, cmap='bone')\n",
    "    axes[0, i+2].set_title(f'{tech}\\nPSNR: {psnr_val:.1f} dB', fontsize=12)\n",
    "    axes[0, i+2].axis('off')\n",
    "\n",
    "# Fila 2: Zoom\n",
    "y1, y2, x1, x2 = 200, 300, 200, 300\n",
    "\n",
    "axes[1, 0].imshow(orig_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 0].set_title('Zoom Original', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(ldct_display[y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 1].set_title('Zoom LDCT', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "for i, tech in enumerate(best_techniques):\n",
    "    denoised = denoised_results[tech]\n",
    "    axes[1, i+2].imshow(denoised[y1:y2, x1:x2], cmap='bone')\n",
    "    axes[1, i+2].set_title(f'Zoom {tech}', fontsize=12)\n",
    "    axes[1, i+2].axis('off')\n",
    "\n",
    "plt.suptitle('Comparación de Técnicas de Denoising', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa de todas las técnicas\n",
    "print(\"Comparación de Técnicas de Denoising\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "comparison = []\n",
    "comparison.append({\n",
    "    'Técnica': 'LDCT (con ruido)',\n",
    "    'PSNR (dB)': psnr(orig_display, ldct_display, data_range=1.0),\n",
    "    'SSIM': ssim(orig_display, ldct_display, data_range=1.0)\n",
    "})\n",
    "\n",
    "for tech, denoised in denoised_results.items():\n",
    "    comparison.append({\n",
    "        'Técnica': tech,\n",
    "        'PSNR (dB)': psnr(orig_display, denoised, data_range=1.0),\n",
    "        'SSIM': ssim(orig_display, denoised, data_range=1.0)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "comparison_df['PSNR (dB)'] = comparison_df['PSNR (dB)'].round(2)\n",
    "comparison_df['SSIM'] = comparison_df['SSIM'].round(4)\n",
    "\n",
    "# Ordenar por PSNR\n",
    "comparison_df = comparison_df.sort_values('PSNR (dB)', ascending=False)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Mayor PSNR y SSIM = Mejor calidad de imagen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Denoising con Deep Learning: DnCNN\n",
    "\n",
    "**DnCNN** (Denoising Convolutional Neural Network) es una arquitectura clásica para denoising:\n",
    "\n",
    "- Aprende el **residuo** (ruido) en lugar de la imagen limpia\n",
    "- Usa batch normalization y ReLU\n",
    "- Típicamente 17-20 capas convolucionales\n",
    "\n",
    "$$\\hat{x} = y - f(y; \\theta)$$\n",
    "\n",
    "donde $y$ es la imagen ruidosa, $f$ predice el ruido, y $\\hat{x}$ es la imagen denoised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    DnCNN para denoising de imágenes CT\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Conv + ReLU (primera capa)\n",
    "    - (Conv + BN + ReLU) x (depth-2) capas intermedias\n",
    "    - Conv (última capa)\n",
    "    \n",
    "    El modelo predice el residuo (ruido) que se resta de la entrada.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_features=64, depth=17):\n",
    "        super(DnCNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Primera capa: Conv + ReLU\n",
    "        layers.append(nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Capas intermedias: Conv + BN + ReLU\n",
    "        for _ in range(depth - 2):\n",
    "            layers.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(num_features))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Última capa: Conv (sin activación)\n",
    "        layers.append(nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1, bias=False))\n",
    "        \n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Predice el residuo y lo resta de la entrada\"\"\"\n",
    "        residual = self.dncnn(x)\n",
    "        return x - residual  # Imagen denoised\n",
    "\n",
    "\n",
    "# Crear modelo\n",
    "model_dncnn = DnCNN(in_channels=1, out_channels=1, num_features=64, depth=17).to(device)\n",
    "\n",
    "print(\"DnCNN creado\")\n",
    "print(f\"Parámetros totales: {sum(p.numel() for p in model_dncnn.parameters()):,}\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDenoisingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset para entrenamiento de denoising en CT\n",
    "    \n",
    "    Genera pares (imagen_ruidosa, imagen_limpia) a partir de\n",
    "    escaneos CT aplicando ruido simulado.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, annotations_path, patch_size=64, \n",
    "                 num_patches_per_scan=50, dose_ratio=0.25):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches_per_scan\n",
    "        self.dose_ratio = dose_ratio\n",
    "        \n",
    "        # Cargar datos\n",
    "        self.loader = LUNA16DataLoader(data_path, annotations_path)\n",
    "        self.mhd_files = list(Path(data_path).glob(\"*.mhd\"))\n",
    "        \n",
    "        # Cache de slices\n",
    "        self._slices_cache = []\n",
    "        self._load_slices()\n",
    "    \n",
    "    def _load_slices(self):\n",
    "        \"\"\"Carga slices de algunos escaneos\"\"\"\n",
    "        print(\"Cargando slices para dataset de denoising...\")\n",
    "        \n",
    "        # Usar solo primeros N escaneos para demo\n",
    "        max_scans = min(5, len(self.mhd_files))\n",
    "        \n",
    "        for mhd_file in tqdm(self.mhd_files[:max_scans]):\n",
    "            ct_scan, _, _ = self.loader.load_itk_image(str(mhd_file))\n",
    "            \n",
    "            # Tomar slices centrales (evitar bordes)\n",
    "            start_slice = ct_scan.shape[0] // 4\n",
    "            end_slice = 3 * ct_scan.shape[0] // 4\n",
    "            \n",
    "            for slice_idx in range(start_slice, end_slice, 5):  # Cada 5 slices\n",
    "                ct_slice = ct_scan[slice_idx]\n",
    "                # Normalizar\n",
    "                ct_slice = self.loader.normalize_hu(ct_slice, -1000, 400)\n",
    "                self._slices_cache.append(ct_slice)\n",
    "        \n",
    "        print(f\"Slices cargados: {len(self._slices_cache)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._slices_cache) * self.num_patches\n",
    "    \n",
    "    def _extract_random_patch(self, image):\n",
    "        \"\"\"Extrae un patch aleatorio de la imagen\"\"\"\n",
    "        h, w = image.shape\n",
    "        \n",
    "        # Asegurar que el patch cabe\n",
    "        max_y = h - self.patch_size\n",
    "        max_x = w - self.patch_size\n",
    "        \n",
    "        if max_y <= 0 or max_x <= 0:\n",
    "            # Imagen muy pequeña, redimensionar\n",
    "            return image[:self.patch_size, :self.patch_size]\n",
    "        \n",
    "        y = np.random.randint(0, max_y)\n",
    "        x = np.random.randint(0, max_x)\n",
    "        \n",
    "        return image[y:y+self.patch_size, x:x+self.patch_size]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Seleccionar slice\n",
    "        slice_idx = idx // self.num_patches\n",
    "        clean_slice = self._slices_cache[slice_idx]\n",
    "        \n",
    "        # Extraer patch aleatorio\n",
    "        clean_patch = self._extract_random_patch(clean_slice)\n",
    "        \n",
    "        # Añadir ruido\n",
    "        # Convertir a HU aproximado para aplicar ruido realista\n",
    "        clean_hu = clean_patch * 1400 - 1000  # Desnormalizar\n",
    "        noisy_hu = add_ct_realistic_noise(clean_hu, dose_ratio=self.dose_ratio)\n",
    "        noisy_patch = (noisy_hu + 1000) / 1400  # Renormalizar\n",
    "        noisy_patch = np.clip(noisy_patch, 0, 1)\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        clean_tensor = torch.FloatTensor(clean_patch).unsqueeze(0)\n",
    "        noisy_tensor = torch.FloatTensor(noisy_patch).unsqueeze(0)\n",
    "        \n",
    "        return noisy_tensor, clean_tensor\n",
    "\n",
    "\n",
    "print(\"Dataset de denoising definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset y dataloader\n",
    "denoise_dataset = CTDenoisingDataset(\n",
    "    DATA_PATH, \n",
    "    ANNOTATIONS_PATH,\n",
    "    patch_size=64,\n",
    "    num_patches_per_scan=100,\n",
    "    dose_ratio=0.25\n",
    ")\n",
    "\n",
    "denoise_loader = DataLoader(\n",
    "    denoise_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(denoise_dataset)}\")\n",
    "print(f\"Batches: {len(denoise_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar ejemplos del dataset\n",
    "noisy_batch, clean_batch = next(iter(denoise_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(noisy_batch[i, 0].numpy(), cmap='bone')\n",
    "    axes[0, i].set_title('Ruidosa (LDCT)', fontsize=11)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(clean_batch[i, 0].numpy(), cmap='bone')\n",
    "    axes[1, i].set_title('Limpia (NDCT)', fontsize=11)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Ejemplos del Dataset de Denoising (Pares Ruidosa/Limpia)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Entrenamiento del Modelo DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_denoiser(model, dataloader, epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Entrena el modelo de denoising\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo DnCNN\n",
    "        dataloader: DataLoader con pares (ruidosa, limpia)\n",
    "        epochs: Número de épocas\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        history: Diccionario con historial de entrenamiento\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    history = {'loss': [], 'psnr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_psnr = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f'Época {epoch+1}/{epochs}')\n",
    "        \n",
    "        for noisy, clean in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            denoised = model(noisy)\n",
    "            loss = criterion(denoised, clean)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Métricas\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calcular PSNR\n",
    "            with torch.no_grad():\n",
    "                mse = F.mse_loss(denoised, clean)\n",
    "                batch_psnr = 10 * torch.log10(1.0 / mse)\n",
    "                epoch_psnr += batch_psnr.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.6f}', 'PSNR': f'{batch_psnr.item():.2f}'})\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_psnr = epoch_psnr / num_batches\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['psnr'].append(avg_psnr)\n",
    "        \n",
    "        print(f'Época {epoch+1}: Loss = {avg_loss:.6f}, PSNR = {avg_psnr:.2f} dB')\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Función de entrenamiento definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo (pocas épocas para demo)\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = train_denoiser(model_dncnn, denoise_loader, epochs=5, lr=1e-3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar historial de entrenamiento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['loss'], 'b-o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Época', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Pérdida durante Entrenamiento', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PSNR\n",
    "axes[1].plot(history['psnr'], 'g-o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Época', fontsize=12)\n",
    "axes[1].set_ylabel('PSNR (dB)', fontsize=12)\n",
    "axes[1].set_title('PSNR durante Entrenamiento', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Historial de Entrenamiento DnCNN', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_denoiser(model, image_clean, dose_ratio=0.25):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo de denoising en una imagen\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        image_clean: Imagen limpia normalizada [0, 1]\n",
    "        dose_ratio: Ratio de dosis para simular LDCT\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultados con imágenes y métricas\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Simular LDCT\n",
    "    clean_hu = image_clean * 1400 - 1000\n",
    "    noisy_hu = add_ct_realistic_noise(clean_hu, dose_ratio=dose_ratio)\n",
    "    noisy = (noisy_hu + 1000) / 1400\n",
    "    noisy = np.clip(noisy, 0, 1)\n",
    "    \n",
    "    # Preparar tensor\n",
    "    noisy_tensor = torch.FloatTensor(noisy).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inferencia\n",
    "    with torch.no_grad():\n",
    "        denoised_tensor = model(noisy_tensor)\n",
    "    \n",
    "    denoised = denoised_tensor.squeeze().cpu().numpy()\n",
    "    denoised = np.clip(denoised, 0, 1)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    psnr_noisy = psnr(image_clean, noisy, data_range=1.0)\n",
    "    psnr_denoised = psnr(image_clean, denoised, data_range=1.0)\n",
    "    \n",
    "    ssim_noisy = ssim(image_clean, noisy, data_range=1.0)\n",
    "    ssim_denoised = ssim(image_clean, denoised, data_range=1.0)\n",
    "    \n",
    "    return {\n",
    "        'clean': image_clean,\n",
    "        'noisy': noisy,\n",
    "        'denoised': denoised,\n",
    "        'psnr_noisy': psnr_noisy,\n",
    "        'psnr_denoised': psnr_denoised,\n",
    "        'ssim_noisy': ssim_noisy,\n",
    "        'ssim_denoised': ssim_denoised,\n",
    "        'psnr_gain': psnr_denoised - psnr_noisy,\n",
    "        'ssim_gain': ssim_denoised - ssim_noisy\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Función de evaluación definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en imagen de prueba\n",
    "results = evaluate_denoiser(model_dncnn, orig_display, dose_ratio=0.25)\n",
    "\n",
    "print(\"Métricas de Evaluación:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nImagen LDCT (con ruido):\")\n",
    "print(f\"  PSNR: {results['psnr_noisy']:.2f} dB\")\n",
    "print(f\"  SSIM: {results['ssim_noisy']:.4f}\")\n",
    "\n",
    "print(f\"\\nImagen Denoised (DnCNN):\")\n",
    "print(f\"  PSNR: {results['psnr_denoised']:.2f} dB (+{results['psnr_gain']:.2f} dB)\")\n",
    "print(f\"  SSIM: {results['ssim_denoised']:.4f} (+{results['ssim_gain']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar resultados de denoising con DnCNN\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Fila 1: Imágenes completas\n",
    "axes[0, 0].imshow(results['clean'], cmap='bone')\n",
    "axes[0, 0].set_title('Original (NDCT)', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(results['noisy'], cmap='bone')\n",
    "axes[0, 1].set_title(f'LDCT Simulado\\nPSNR: {results[\"psnr_noisy\"]:.1f} dB', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(results['denoised'], cmap='bone')\n",
    "axes[0, 2].set_title(f'DnCNN Denoised\\nPSNR: {results[\"psnr_denoised\"]:.1f} dB', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Diferencia (residuo aprendido)\n",
    "residual = np.abs(results['noisy'] - results['denoised'])\n",
    "axes[0, 3].imshow(residual, cmap='hot')\n",
    "axes[0, 3].set_title('Ruido Removido\\n(Residuo)', fontsize=12)\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Fila 2: Zoom\n",
    "y1, y2, x1, x2 = 200, 300, 200, 300\n",
    "\n",
    "axes[1, 0].imshow(results['clean'][y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 0].set_title('Zoom Original', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(results['noisy'][y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 1].set_title('Zoom LDCT', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(results['denoised'][y1:y2, x1:x2], cmap='bone')\n",
    "axes[1, 2].set_title('Zoom DnCNN', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(residual[y1:y2, x1:x2], cmap='hot')\n",
    "axes[1, 3].set_title('Zoom Residuo', fontsize=12)\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Resultados de Denoising con DnCNN', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Comparación: Clásico vs Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación final de todas las técnicas\n",
    "print(\"Comparación Final: Clásico vs Deep Learning\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Aplicar denoising clásico a la misma imagen ruidosa\n",
    "classical_on_same = apply_classical_denoising(results['noisy'])\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "# LDCT (referencia)\n",
    "final_comparison.append({\n",
    "    'Técnica': 'LDCT (sin procesar)',\n",
    "    'Tipo': 'Referencia',\n",
    "    'PSNR (dB)': results['psnr_noisy'],\n",
    "    'SSIM': results['ssim_noisy']\n",
    "})\n",
    "\n",
    "# Técnicas clásicas\n",
    "for tech, denoised in classical_on_same.items():\n",
    "    final_comparison.append({\n",
    "        'Técnica': tech,\n",
    "        'Tipo': 'Clásico',\n",
    "        'PSNR (dB)': psnr(results['clean'], denoised, data_range=1.0),\n",
    "        'SSIM': ssim(results['clean'], denoised, data_range=1.0)\n",
    "    })\n",
    "\n",
    "# DnCNN\n",
    "final_comparison.append({\n",
    "    'Técnica': 'DnCNN',\n",
    "    'Tipo': 'Deep Learning',\n",
    "    'PSNR (dB)': results['psnr_denoised'],\n",
    "    'SSIM': results['ssim_denoised']\n",
    "})\n",
    "\n",
    "# Crear DataFrame y ordenar\n",
    "final_df = pd.DataFrame(final_comparison)\n",
    "final_df['PSNR (dB)'] = final_df['PSNR (dB)'].round(2)\n",
    "final_df['SSIM'] = final_df['SSIM'].round(4)\n",
    "final_df = final_df.sort_values('PSNR (dB)', ascending=False)\n",
    "\n",
    "print(final_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras comparativo\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ordenar por PSNR para visualización\n",
    "df_sorted = final_df.sort_values('PSNR (dB)', ascending=True)\n",
    "\n",
    "# Colores por tipo\n",
    "colors = ['gray' if t == 'Referencia' else 'steelblue' if t == 'Clásico' else 'coral' \n",
    "          for t in df_sorted['Tipo']]\n",
    "\n",
    "# PSNR\n",
    "axes[0].barh(df_sorted['Técnica'], df_sorted['PSNR (dB)'], color=colors)\n",
    "axes[0].set_xlabel('PSNR (dB)', fontsize=12)\n",
    "axes[0].set_title('Comparación de PSNR', fontsize=14)\n",
    "axes[0].axvline(x=results['psnr_noisy'], color='red', linestyle='--', alpha=0.5, label='LDCT')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# SSIM\n",
    "axes[1].barh(df_sorted['Técnica'], df_sorted['SSIM'], color=colors)\n",
    "axes[1].set_xlabel('SSIM', fontsize=12)\n",
    "axes[1].set_title('Comparación de SSIM', fontsize=14)\n",
    "axes[1].axvline(x=results['ssim_noisy'], color='red', linestyle='--', alpha=0.5, label='LDCT')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Leyenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='gray', label='Referencia'),\n",
    "    Patch(facecolor='steelblue', label='Clásico'),\n",
    "    Patch(facecolor='coral', label='Deep Learning')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "\n",
    "plt.suptitle('Comparación de Técnicas de Denoising', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Guardar Modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar pesos del modelo\n",
    "if IN_COLAB:\n",
    "    weights_dir = '/content/weights'\n",
    "else:\n",
    "    weights_dir = os.path.join(project_root, 'weights')\n",
    "\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(weights_dir, 'dncnn_denoising.pth')\n",
    "torch.save(model_dncnn.state_dict(), model_path)\n",
    "\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "print(f\"Tamaño: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar modelo guardado\n",
    "def load_denoiser(weights_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Carga un modelo DnCNN entrenado\n",
    "    \n",
    "    Args:\n",
    "        weights_path: Ruta al archivo .pth\n",
    "        device: Dispositivo ('cuda' o 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo cargado listo para inferencia\n",
    "    \"\"\"\n",
    "    model = DnCNN(in_channels=1, out_channels=1, num_features=64, depth=17)\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Verificar que se puede cargar\n",
    "model_loaded = load_denoiser(model_path, device)\n",
    "print(\"Modelo cargado correctamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Dataset Mayo Clinic LDCT (TCIA) - Datos Reales\n",
    "\n",
    "El **AAPM Low Dose CT Grand Challenge** proporciona pares reales de imágenes NDCT/LDCT del Mayo Clinic.\n",
    "\n",
    "### Información del Dataset:\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|---------|\n",
    "| **Nombre** | LDCT-and-Projection-data |\n",
    "| **Fuente** | The Cancer Imaging Archive (TCIA) |\n",
    "| **Tamaño** | 1.32 TB (299 casos) |\n",
    "| **Formato** | DICOM estándar + DICOM-CT-PD (proyecciones) |\n",
    "| **Dosis** | Full-dose (100%) + Quarter-dose (25%) |\n",
    "| **Anatomía** | Cabeza, Tórax, Abdomen |\n",
    "| **DOI** | [10.7937/9npb-2637](https://doi.org/10.7937/9npb-2637) |\n",
    "\n",
    "### Cómo descargar:\n",
    "\n",
    "1. Visitar: https://www.cancerimagingarchive.net/collection/ldct-and-projection-data/\n",
    "2. Descargar el NBIA Data Retriever\n",
    "3. Seleccionar los casos deseados (Chest para pulmón)\n",
    "4. Descargar imágenes DICOM\n",
    "\n",
    "### Estructura esperada:\n",
    "```\n",
    "Mayo_LDCT/\n",
    "├── L001/                    # Paciente 001\n",
    "│   ├── full_dose/          # Imágenes NDCT (100% dosis)\n",
    "│   │   └── *.dcm\n",
    "│   └── quarter_dose/       # Imágenes LDCT (25% dosis)\n",
    "│       └── *.dcm\n",
    "├── L002/\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CARGA DE DATOS MAYO CLINIC LDCT (TCIA)\n",
    "# ============================================================\n",
    "\n",
    "# Configurar ruta al dataset Mayo Clinic (si está disponible)\n",
    "if IN_COLAB:\n",
    "    MAYO_PATH = '/content/Mayo_LDCT'\n",
    "else:\n",
    "    MAYO_PATH = os.path.join(project_root, 'Mayo_LDCT')\n",
    "\n",
    "MAYO_AVAILABLE = os.path.exists(MAYO_PATH)\n",
    "\n",
    "if MAYO_AVAILABLE:\n",
    "    print(f\"Dataset Mayo Clinic encontrado en: {MAYO_PATH}\")\n",
    "else:\n",
    "    print(f\"Dataset Mayo Clinic NO encontrado en: {MAYO_PATH}\")\n",
    "    print(\"\\nPara usar datos reales NDCT/LDCT:\")\n",
    "    print(\"1. Descargar de: https://www.cancerimagingarchive.net/collection/ldct-and-projection-data/\")\n",
    "    print(\"2. Colocar en la carpeta 'Mayo_LDCT' del proyecto\")\n",
    "    print(\"\\nContinuando con ruido simulado...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "from glob import glob\n",
    "\n",
    "class MayoLDCTLoader:\n",
    "    \"\"\"\n",
    "    Cargador para el dataset Mayo Clinic LDCT (TCIA)\n",
    "    \n",
    "    El dataset contiene pares de imágenes:\n",
    "    - Full-dose (NDCT): 100% de la dosis de radiación\n",
    "    - Quarter-dose (LDCT): 25% de la dosis (simulado con ruido Poisson)\n",
    "    \n",
    "    Referencia: Chen et al. 2016 - SPIE Medical Imaging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_path: Ruta base del dataset Mayo_LDCT\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.patients = self._find_patients()\n",
    "        \n",
    "    def _find_patients(self):\n",
    "        \"\"\"Encuentra todos los pacientes disponibles\"\"\"\n",
    "        patients = []\n",
    "        \n",
    "        # Buscar carpetas de pacientes (formato L001, L002, etc.)\n",
    "        for patient_dir in sorted(self.base_path.glob(\"L*\")):\n",
    "            if patient_dir.is_dir():\n",
    "                patients.append(patient_dir.name)\n",
    "        \n",
    "        # También buscar formato alternativo (carpetas con SeriesInstanceUID)\n",
    "        if len(patients) == 0:\n",
    "            for patient_dir in sorted(self.base_path.iterdir()):\n",
    "                if patient_dir.is_dir():\n",
    "                    patients.append(patient_dir.name)\n",
    "        \n",
    "        return patients\n",
    "    \n",
    "    def _find_dose_folders(self, patient_id):\n",
    "        \"\"\"\n",
    "        Encuentra las carpetas de full-dose y quarter-dose para un paciente\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (full_dose_path, quarter_dose_path) o (None, None) si no encuentra\n",
    "        \"\"\"\n",
    "        patient_path = self.base_path / patient_id\n",
    "        \n",
    "        full_dose = None\n",
    "        quarter_dose = None\n",
    "        \n",
    "        # Buscar por nombre de carpeta\n",
    "        for folder in patient_path.rglob(\"*\"):\n",
    "            if folder.is_dir():\n",
    "                folder_lower = folder.name.lower()\n",
    "                \n",
    "                # Detectar full dose\n",
    "                if any(x in folder_lower for x in ['full', '100', 'nd', 'normal']):\n",
    "                    full_dose = folder\n",
    "                # Detectar quarter dose  \n",
    "                elif any(x in folder_lower for x in ['quarter', '25', 'ld', 'low']):\n",
    "                    quarter_dose = folder\n",
    "                # Detectar por contenido de DICOM\n",
    "                elif not full_dose or not quarter_dose:\n",
    "                    dcm_files = list(folder.glob(\"*.dcm\"))\n",
    "                    if dcm_files:\n",
    "                        try:\n",
    "                            ds = pydicom.dcmread(str(dcm_files[0]), stop_before_pixels=True)\n",
    "                            # Intentar detectar por metadatos\n",
    "                            if hasattr(ds, 'SeriesDescription'):\n",
    "                                desc = ds.SeriesDescription.lower()\n",
    "                                if 'full' in desc or '100' in desc:\n",
    "                                    full_dose = folder\n",
    "                                elif 'quarter' in desc or '25' in desc:\n",
    "                                    quarter_dose = folder\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        return full_dose, quarter_dose\n",
    "    \n",
    "    def load_dicom_series(self, folder_path):\n",
    "        \"\"\"\n",
    "        Carga una serie DICOM completa y la convierte a volumen 3D\n",
    "        \n",
    "        Args:\n",
    "            folder_path: Ruta a la carpeta con archivos .dcm\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (volumen_3d, spacing, origin)\n",
    "        \"\"\"\n",
    "        folder_path = Path(folder_path)\n",
    "        dcm_files = sorted(folder_path.glob(\"*.dcm\"))\n",
    "        \n",
    "        if len(dcm_files) == 0:\n",
    "            # Buscar en subcarpetas\n",
    "            dcm_files = sorted(folder_path.rglob(\"*.dcm\"))\n",
    "        \n",
    "        if len(dcm_files) == 0:\n",
    "            raise ValueError(f\"No se encontraron archivos DICOM en {folder_path}\")\n",
    "        \n",
    "        # Leer todos los slices\n",
    "        slices = []\n",
    "        for dcm_file in dcm_files:\n",
    "            ds = pydicom.dcmread(str(dcm_file))\n",
    "            slices.append(ds)\n",
    "        \n",
    "        # Ordenar por posición Z\n",
    "        slices.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "        \n",
    "        # Extraer información espacial\n",
    "        pixel_spacing = slices[0].PixelSpacing\n",
    "        slice_thickness = slices[0].SliceThickness if hasattr(slices[0], 'SliceThickness') else 1.0\n",
    "        spacing = np.array([slice_thickness, pixel_spacing[0], pixel_spacing[1]])\n",
    "        origin = np.array(slices[0].ImagePositionPatient)\n",
    "        \n",
    "        # Construir volumen 3D\n",
    "        volume = np.stack([s.pixel_array for s in slices])\n",
    "        \n",
    "        # Convertir a valores HU\n",
    "        intercept = slices[0].RescaleIntercept if hasattr(slices[0], 'RescaleIntercept') else 0\n",
    "        slope = slices[0].RescaleSlope if hasattr(slices[0], 'RescaleSlope') else 1\n",
    "        volume = volume * slope + intercept\n",
    "        \n",
    "        return volume.astype(np.float32), spacing, origin\n",
    "    \n",
    "    def load_patient_pair(self, patient_id):\n",
    "        \"\"\"\n",
    "        Carga el par NDCT/LDCT para un paciente\n",
    "        \n",
    "        Args:\n",
    "            patient_id: ID del paciente (ej: 'L001')\n",
    "            \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'ndct': volumen full-dose,\n",
    "                'ldct': volumen quarter-dose,\n",
    "                'spacing': espaciado,\n",
    "                'patient_id': ID\n",
    "            }\n",
    "        \"\"\"\n",
    "        full_path, quarter_path = self._find_dose_folders(patient_id)\n",
    "        \n",
    "        if full_path is None or quarter_path is None:\n",
    "            raise ValueError(f\"No se encontraron carpetas full/quarter dose para {patient_id}\")\n",
    "        \n",
    "        print(f\"Cargando {patient_id}...\")\n",
    "        print(f\"  Full dose: {full_path}\")\n",
    "        print(f\"  Quarter dose: {quarter_path}\")\n",
    "        \n",
    "        ndct, spacing, origin = self.load_dicom_series(full_path)\n",
    "        ldct, _, _ = self.load_dicom_series(quarter_path)\n",
    "        \n",
    "        return {\n",
    "            'ndct': ndct,\n",
    "            'ldct': ldct,\n",
    "            'spacing': spacing,\n",
    "            'origin': origin,\n",
    "            'patient_id': patient_id\n",
    "        }\n",
    "    \n",
    "    def get_slice_pair(self, patient_id, slice_idx):\n",
    "        \"\"\"\n",
    "        Obtiene un par de slices NDCT/LDCT\n",
    "        \n",
    "        Args:\n",
    "            patient_id: ID del paciente\n",
    "            slice_idx: Índice del slice\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (ndct_slice, ldct_slice)\n",
    "        \"\"\"\n",
    "        data = self.load_patient_pair(patient_id)\n",
    "        return data['ndct'][slice_idx], data['ldct'][slice_idx]\n",
    "\n",
    "\n",
    "# Crear loader si el dataset está disponible\n",
    "if MAYO_AVAILABLE:\n",
    "    mayo_loader = MayoLDCTLoader(MAYO_PATH)\n",
    "    print(f\"\\nPacientes encontrados: {len(mayo_loader.patients)}\")\n",
    "    if len(mayo_loader.patients) > 0:\n",
    "        print(f\"Primeros pacientes: {mayo_loader.patients[:5]}\")\n",
    "else:\n",
    "    mayo_loader = None\n",
    "    print(\"\\nUsando datos simulados (LUNA16 + ruido sintético)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MayoLDCTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch para pares NDCT/LDCT del Mayo Clinic\n",
    "    \n",
    "    Extrae patches de pares reales full-dose/quarter-dose\n",
    "    para entrenamiento de modelos de denoising.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mayo_loader, patient_ids=None, patch_size=64, \n",
    "                 num_patches_per_volume=200, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mayo_loader: Instancia de MayoLDCTLoader\n",
    "            patient_ids: Lista de IDs de pacientes a usar (None = todos)\n",
    "            patch_size: Tamaño del patch cuadrado\n",
    "            num_patches_per_volume: Patches a extraer por volumen\n",
    "            transform: Transformaciones opcionales\n",
    "        \"\"\"\n",
    "        self.loader = mayo_loader\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches_per_volume\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Usar todos los pacientes si no se especifica\n",
    "        if patient_ids is None:\n",
    "            self.patient_ids = mayo_loader.patients\n",
    "        else:\n",
    "            self.patient_ids = patient_ids\n",
    "        \n",
    "        # Cache de datos\n",
    "        self._cache = {}\n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Carga los datos de todos los pacientes\"\"\"\n",
    "        print(f\"Cargando {len(self.patient_ids)} pacientes...\")\n",
    "        \n",
    "        for patient_id in tqdm(self.patient_ids):\n",
    "            try:\n",
    "                data = self.loader.load_patient_pair(patient_id)\n",
    "                self._cache[patient_id] = data\n",
    "            except Exception as e:\n",
    "                print(f\"Error cargando {patient_id}: {e}\")\n",
    "        \n",
    "        print(f\"Pacientes cargados: {len(self._cache)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._cache) * self.num_patches\n",
    "    \n",
    "    def _normalize(self, image, min_hu=-1000, max_hu=400):\n",
    "        \"\"\"Normaliza valores HU a [0, 1]\"\"\"\n",
    "        image = np.clip(image, min_hu, max_hu)\n",
    "        return (image - min_hu) / (max_hu - min_hu)\n",
    "    \n",
    "    def _extract_random_patch(self, ndct_vol, ldct_vol):\n",
    "        \"\"\"Extrae un patch aleatorio del mismo lugar en ambos volúmenes\"\"\"\n",
    "        z, h, w = ndct_vol.shape\n",
    "        \n",
    "        # Seleccionar slice aleatorio (evitar bordes)\n",
    "        z_idx = np.random.randint(z // 4, 3 * z // 4)\n",
    "        \n",
    "        # Seleccionar posición aleatoria\n",
    "        y = np.random.randint(0, h - self.patch_size)\n",
    "        x = np.random.randint(0, w - self.patch_size)\n",
    "        \n",
    "        # Extraer patches\n",
    "        ndct_patch = ndct_vol[z_idx, y:y+self.patch_size, x:x+self.patch_size]\n",
    "        ldct_patch = ldct_vol[z_idx, y:y+self.patch_size, x:x+self.patch_size]\n",
    "        \n",
    "        return ndct_patch, ldct_patch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Seleccionar paciente\n",
    "        patient_idx = idx // self.num_patches\n",
    "        patient_id = list(self._cache.keys())[patient_idx]\n",
    "        data = self._cache[patient_id]\n",
    "        \n",
    "        # Extraer patch\n",
    "        ndct_patch, ldct_patch = self._extract_random_patch(\n",
    "            data['ndct'], data['ldct']\n",
    "        )\n",
    "        \n",
    "        # Normalizar\n",
    "        ndct_patch = self._normalize(ndct_patch)\n",
    "        ldct_patch = self._normalize(ldct_patch)\n",
    "        \n",
    "        # Convertir a tensores\n",
    "        ndct_tensor = torch.FloatTensor(ndct_patch).unsqueeze(0)\n",
    "        ldct_tensor = torch.FloatTensor(ldct_patch).unsqueeze(0)\n",
    "        \n",
    "        # Aplicar transformaciones\n",
    "        if self.transform:\n",
    "            ndct_tensor = self.transform(ndct_tensor)\n",
    "            ldct_tensor = self.transform(ldct_tensor)\n",
    "        \n",
    "        # Retornar (ruidosa, limpia) para mantener consistencia con el dataset simulado\n",
    "        return ldct_tensor, ndct_tensor\n",
    "\n",
    "\n",
    "# Crear dataset si Mayo está disponible\n",
    "if MAYO_AVAILABLE and mayo_loader is not None and len(mayo_loader.patients) > 0:\n",
    "    print(\"Creando dataset con datos reales Mayo Clinic...\")\n",
    "    \n",
    "    # Usar primeros 5 pacientes para demo\n",
    "    demo_patients = mayo_loader.patients[:5]\n",
    "    mayo_dataset = MayoLDCTDataset(\n",
    "        mayo_loader, \n",
    "        patient_ids=demo_patients,\n",
    "        patch_size=64,\n",
    "        num_patches_per_volume=100\n",
    "    )\n",
    "    \n",
    "    mayo_dataloader = DataLoader(\n",
    "        mayo_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset Mayo creado:\")\n",
    "    print(f\"  Pacientes: {len(demo_patients)}\")\n",
    "    print(f\"  Total patches: {len(mayo_dataset)}\")\n",
    "else:\n",
    "    mayo_dataset = None\n",
    "    mayo_dataloader = None\n",
    "    print(\"\\nDataset Mayo no disponible, usar datos simulados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar ejemplos de datos reales (si están disponibles)\n",
    "if mayo_dataloader is not None:\n",
    "    print(\"Visualizando pares reales NDCT/LDCT del Mayo Clinic:\")\n",
    "    \n",
    "    ldct_batch, ndct_batch = next(iter(mayo_dataloader))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for i in range(4):\n",
    "        # LDCT (ruidosa)\n",
    "        axes[0, i].imshow(ldct_batch[i, 0].numpy(), cmap='bone')\n",
    "        axes[0, i].set_title('LDCT Real (25% dosis)', fontsize=11)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # NDCT (limpia)\n",
    "        axes[1, i].imshow(ndct_batch[i, 0].numpy(), cmap='bone')\n",
    "        axes[1, i].set_title('NDCT Real (100% dosis)', fontsize=11)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Pares Reales NDCT/LDCT - Mayo Clinic Dataset', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcular diferencia de ruido real\n",
    "    diff = np.abs(ldct_batch[0, 0].numpy() - ndct_batch[0, 0].numpy())\n",
    "    psnr_real = psnr(ndct_batch[0, 0].numpy(), ldct_batch[0, 0].numpy(), data_range=1.0)\n",
    "    \n",
    "    print(f\"\\nMétricas de ruido real (ejemplo):\")\n",
    "    print(f\"  PSNR LDCT vs NDCT: {psnr_real:.2f} dB\")\n",
    "    print(f\"  Diferencia media: {diff.mean():.4f}\")\n",
    "    print(f\"  Diferencia máx: {diff.max():.4f}\")\n",
    "else:\n",
    "    print(\"Dataset Mayo no disponible - usando datos simulados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Entrenamiento con Datos Reales Mayo Clinic\n",
    "\n",
    "Si el dataset Mayo está disponible, podemos entrenar el modelo DnCNN con pares reales NDCT/LDCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar con datos reales Mayo (si están disponibles)\n",
    "if mayo_dataloader is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"ENTRENAMIENTO CON DATOS REALES MAYO CLINIC\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear nuevo modelo para datos reales\n",
    "    model_mayo = DnCNN(in_channels=1, out_channels=1, num_features=64, depth=17).to(device)\n",
    "    \n",
    "    print(f\"\\nModelo DnCNN para Mayo Clinic:\")\n",
    "    print(f\"  Parámetros: {sum(p.numel() for p in model_mayo.parameters()):,}\")\n",
    "    \n",
    "    # Entrenar\n",
    "    print(\"\\nIniciando entrenamiento con datos reales...\")\n",
    "    history_mayo = train_denoiser(model_mayo, mayo_dataloader, epochs=10, lr=1e-3)\n",
    "    \n",
    "    # Guardar modelo\n",
    "    mayo_model_path = os.path.join(weights_dir, 'dncnn_mayo_real.pth')\n",
    "    torch.save(model_mayo.state_dict(), mayo_model_path)\n",
    "    print(f\"\\nModelo Mayo guardado en: {mayo_model_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Dataset Mayo no disponible\")\n",
    "    print(\"El modelo fue entrenado con ruido simulado (ver secciones anteriores)\")\n",
    "    print(\"\\nPara entrenar con datos reales:\")\n",
    "    print(\"1. Descargar de: https://www.cancerimagingarchive.net/collection/ldct-and-projection-data/\")\n",
    "    print(\"2. Colocar en carpeta 'Mayo_LDCT'\")\n",
    "    print(\"3. Re-ejecutar este notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Resumen y Referencias\n",
    "\n",
    "### Logros de este notebook:\n",
    "\n",
    "1. **Simulación de ruido LDCT** a partir de imágenes NDCT (Gaussiano, Poisson, CT realista)\n",
    "2. **Comparación de técnicas clásicas** de denoising (Bilateral, TV, NLM)\n",
    "3. **Implementación de DnCNN** para denoising con deep learning\n",
    "4. **Soporte para dataset Mayo Clinic** con pares NDCT/LDCT reales\n",
    "\n",
    "### Modelos guardados:\n",
    "\n",
    "| Modelo | Datos | Archivo |\n",
    "|--------|-------|---------|\n",
    "| DnCNN (simulado) | LUNA16 + ruido sintético | `weights/dncnn_denoising.pth` |\n",
    "| DnCNN (real) | Mayo Clinic LDCT | `weights/dncnn_mayo_real.pth` |\n",
    "\n",
    "### Referencias:\n",
    "\n",
    "1. **Paper del dataset Mayo Clinic**:\n",
    "   - Chen B. et al. (2016). \"An Open Library of CT Patient Projection Data\". SPIE Medical Imaging.\n",
    "   - DOI: [10.1117/12.2216823](https://doi.org/10.1117/12.2216823)\n",
    "\n",
    "2. **Dataset TCIA**:\n",
    "   - LDCT-and-Projection-data\n",
    "   - DOI: [10.7937/9npb-2637](https://doi.org/10.7937/9npb-2637)\n",
    "   - URL: https://www.cancerimagingarchive.net/collection/ldct-and-projection-data/\n",
    "\n",
    "3. **DnCNN**:\n",
    "   - Zhang K. et al. (2017). \"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising\". IEEE TIP.\n",
    "\n",
    "4. **AAPM Low Dose CT Grand Challenge**:\n",
    "   - https://www.aapm.org/grandchallenge/lowdosect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL NOTEBOOK 05: DENOISING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. MODELOS DE RUIDO:\")\n",
    "print(\"   - Gaussiano (aditivo)\")\n",
    "print(\"   - Poisson (cuántico)\")\n",
    "print(\"   - LDCT realista (combinado)\")\n",
    "\n",
    "print(\"\\n2. TÉCNICAS DE DENOISING:\")\n",
    "print(\"   Clásicas: Gaussiano, Mediana, Bilateral, TV, NLM\")\n",
    "print(\"   Deep Learning: DnCNN (17 capas, residual learning)\")\n",
    "\n",
    "print(\"\\n3. DATASETS SOPORTADOS:\")\n",
    "print(\"   - LUNA16: Ruido simulado (disponible)\")\n",
    "if MAYO_AVAILABLE:\n",
    "    print(f\"   - Mayo Clinic: Pares reales NDCT/LDCT (disponible, {len(mayo_loader.patients)} pacientes)\")\n",
    "else:\n",
    "    print(\"   - Mayo Clinic: No descargado (ver instrucciones arriba)\")\n",
    "\n",
    "print(\"\\n4. RESULTADOS (ruido simulado):\")\n",
    "print(f\"   LDCT sin procesar: {results['psnr_noisy']:.2f} dB\")\n",
    "print(f\"   DnCNN:             {results['psnr_denoised']:.2f} dB (+{results['psnr_gain']:.2f} dB)\")\n",
    "\n",
    "print(\"\\n5. ARCHIVOS GENERADOS:\")\n",
    "print(f\"   - weights/dncnn_denoising.pth (ruido simulado)\")\n",
    "if MAYO_AVAILABLE:\n",
    "    print(f\"   - weights/dncnn_mayo_real.pth (datos reales)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Para mejorar resultados:\")\n",
    "print(\"  1. Descargar Mayo Clinic LDCT de TCIA\")\n",
    "print(\"  2. Entrenar más épocas (50-100)\")\n",
    "print(\"  3. Usar arquitecturas avanzadas (RED-CNN, WGAN)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL NOTEBOOK 05: DENOISING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModelos de ruido implementados:\")\n",
    "print(\"  - Gaussiano (aditivo)\")\n",
    "print(\"  - Poisson (cuántico)\")\n",
    "print(\"  - LDCT realista (combinado)\")\n",
    "\n",
    "print(\"\\nTécnicas de denoising:\")\n",
    "print(\"  Clásicas: Gaussiano, Mediana, Bilateral, TV, NLM\")\n",
    "print(\"  Deep Learning: DnCNN\")\n",
    "\n",
    "print(\"\\nResultados (PSNR en imagen de prueba):\")\n",
    "print(f\"  LDCT sin procesar: {results['psnr_noisy']:.2f} dB\")\n",
    "print(f\"  DnCNN:             {results['psnr_denoised']:.2f} dB (+{results['psnr_gain']:.2f} dB)\")\n",
    "\n",
    "if model_dncnn is not None:\n",
    "    print(f\"\\nModelo DnCNN:\")\n",
    "    print(f\"  Parámetros: {sum(p.numel() for p in model_dncnn.parameters()):,}\")\n",
    "    print(f\"  Guardado en: {model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
