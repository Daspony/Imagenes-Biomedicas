{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 04: Clasificacion de Nodulos Pulmonares\n\nEste notebook entrena clasificadores de nodulos pulmonares (benigno/maligno) usando datos de LUNA16 + anotaciones LIDC-IDRI.\n\n## Problema: Dataset Muy Pequeno (~164 patches)\n\nCon tan pocas muestras, las redes profundas tienden a:\n- **Overfitting**: Memorizan el train set\n- **Alta varianza**: Metricas inestables entre splits\n\n## Estrategias Implementadas\n\n| Estrategia | Descripcion |\n|------------|-------------|\n| **K-Fold CV por paciente** | Evaluacion robusta, evita data leakage |\n| **Backbone congelado** | Solo entrenar cabeza lineal (~1000 params vs 4M) |\n| **Features + SVM/LogReg** | Clasificadores clasicos sobre embeddings |\n| **Radiomics baseline** | Features interpretables de textura/forma |\n\n## Contenido\n1. Extraccion de Patches (LUNA16 + LIDC-IDRI)\n2. **K-Fold Cross-Validation por paciente**\n3. **Backbone congelado + cabeza lineal**\n4. **Embeddings + clasificadores clasicos**\n5. **Radiomics baseline**\n6. Comparacion de todas las estrategias\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuracion del Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando localmente\n",
      "==================================================\n",
      "Directorio de pesos: c:\\Users\\Poney\\Desktop\\Imagenes Biomedicas\\weights\n"
     ]
    }
   ],
   "source": [
    "# Detectar entorno y configurar\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACION\n",
    "# ============================================================\n",
    "SAVE_TO_DRIVE = True  # Guardar modelo en Drive (solo Colab)\n",
    "SUBSETS_TO_USE = [0]  # Subsets de LUNA16 a usar [0] o [0,1,2,...,9]\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Ejecutando en Google Colab\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Instalar dependencias\n",
    "    import subprocess\n",
    "    paquetes = ['SimpleITK', 'pylidc', 'efficientnet_pytorch', 'timm']\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + paquetes)\n",
    "    \n",
    "    # Clonar repositorio\n",
    "    repo_url = \"https://github.com/Daspony/Imagenes-Biomedicas.git\"\n",
    "    if not os.path.exists(\"/content/Imagenes-Biomedicas\"):\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], cwd=\"/content\", check=True)\n",
    "    sys.path.insert(0, \"/content/Imagenes-Biomedicas\")\n",
    "    project_root = \"/content/Imagenes-Biomedicas\"\n",
    "    \n",
    "    # Montar Drive para guardar modelo\n",
    "    if SAVE_TO_DRIVE:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        weights_dir = \"/content/drive/MyDrive/lung_nodule_weights\"\n",
    "        os.makedirs(weights_dir, exist_ok=True)\n",
    "        print(f\"[OK] Pesos se guardaran en: {weights_dir}\")\n",
    "    else:\n",
    "        weights_dir = os.path.join(project_root, 'weights')\n",
    "else:\n",
    "    print(\"Ejecutando localmente\")\n",
    "    print(\"=\"*50)\n",
    "    parent_dir = os.path.abspath('..')\n",
    "    if parent_dir not in sys.path:\n",
    "        sys.path.insert(0, parent_dir)\n",
    "    project_root = parent_dir\n",
    "    weights_dir = os.path.join(project_root, 'weights')\n",
    "\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "print(f\"Directorio de pesos: {weights_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importar librerias\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport warnings\nimport json\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nimport timm  # Para EfficientNet y DenseNet\n\n# Sklearn - Clasificadores y metricas\nfrom sklearn.metrics import (classification_report, confusion_matrix, \n                             roc_auc_score, roc_curve, f1_score,\n                             accuracy_score, precision_score, recall_score)\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Utilidades del proyecto\nfrom utils import LUNA16DataLoader, download_luna16\nfrom utils.lidc_loader import LIDCAnnotationLoader\n\n# Configurar dispositivo\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Dispositivo: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# Seed para reproducibilidad\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n\nprint(\"\\nLibrerias importadas\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] subset0: 89 scans\n",
      "[OK] subset1: 89 scans\n",
      "[OK] subset2: 89 scans\n",
      "[OK] subset3: 89 scans\n",
      "\n",
      "Subsets disponibles: [0, 1, 2, 3]\n",
      "Total subsets: 4\n",
      "Anotaciones LUNA16: 1186 nodulos\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DETECTAR SUBSETS DISPONIBLES\n",
    "# ============================================================\n",
    "LUNA16_DIR = os.path.join(project_root, 'LUNA16')\n",
    "ANNOTATIONS_PATH = os.path.join(LUNA16_DIR, 'annotations.csv')\n",
    "\n",
    "# Detectar subsets disponibles automaticamente\n",
    "available_subsets = []\n",
    "for i in range(10):\n",
    "    subset_path = os.path.join(LUNA16_DIR, f'subset{i}')\n",
    "    if os.path.exists(subset_path):\n",
    "        mhd_files = list(Path(subset_path).glob(\"*.mhd\"))\n",
    "        if len(mhd_files) > 0:\n",
    "            available_subsets.append(i)\n",
    "            print(f\"[OK] subset{i}: {len(mhd_files)} scans\")\n",
    "\n",
    "if not available_subsets:\n",
    "    print(\"[WARNING] No se encontraron subsets de LUNA16\")\n",
    "    print(\"Descargando subset0...\")\n",
    "    download_luna16(subsets=[0], download_dir=LUNA16_DIR)\n",
    "    available_subsets = [0]\n",
    "\n",
    "print(f\"\\nSubsets disponibles: {available_subsets}\")\n",
    "print(f\"Total subsets: {len(available_subsets)}\")\n",
    "\n",
    "# Cargar anotaciones\n",
    "if os.path.exists(ANNOTATIONS_PATH):\n",
    "    annotations_df = pd.read_csv(ANNOTATIONS_PATH)\n",
    "    print(f\"Anotaciones LUNA16: {len(annotations_df)} nodulos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extraccion de Patches LUNA16 + LIDC\n",
    "\n",
    "Extraemos patches de 64x64 pixeles centrados en cada nodulo, con etiquetas de malignidad de LIDC-IDRI.\n",
    "\n",
    "**Criterio de binarizacion:**\n",
    "- Score 1-2 -> Benigno (0)\n",
    "- Score 4-5 -> Maligno (1)\n",
    "- Score 3 -> Excluido (indeterminado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 356 scans encontrados en 4 subsets\n"
     ]
    }
   ],
   "source": [
    "class LUNALIDCPatchExtractor:\n",
    "    \"\"\"\n",
    "    Extractor de patches de nodulos desde LUNA16 + LIDC-IDRI\n",
    "    \n",
    "    Extrae patches 2D centrados en nodulos con etiquetas de malignidad\n",
    "    basadas en el consenso de radiologos de LIDC-IDRI.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, luna16_dir, subsets, patch_size=64, min_radiologists=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            luna16_dir: Directorio raiz de LUNA16\n",
    "            subsets: Lista de subsets a usar (ej: [0] o [0,1,2,...,9])\n",
    "            patch_size: Tamano del patch cuadrado (default: 64)\n",
    "            min_radiologists: Minimo de radiologos para considerar un nodulo (default: 3)\n",
    "        \"\"\"\n",
    "        self.luna16_dir = Path(luna16_dir)\n",
    "        self.subsets = subsets\n",
    "        self.patch_size = patch_size\n",
    "        self.min_radiologists = min_radiologists\n",
    "        \n",
    "        # Inicializar LIDC loader\n",
    "        self.lidc_loader = LIDCAnnotationLoader(verbose=False)\n",
    "        \n",
    "        # Recolectar todos los scans\n",
    "        self.scan_paths = {}\n",
    "        for subset_id in subsets:\n",
    "            subset_path = self.luna16_dir / f'subset{subset_id}'\n",
    "            if subset_path.exists():\n",
    "                for mhd_file in subset_path.glob(\"*.mhd\"):\n",
    "                    seriesuid = mhd_file.stem\n",
    "                    self.scan_paths[seriesuid] = mhd_file\n",
    "        \n",
    "        print(f\"[OK] {len(self.scan_paths)} scans encontrados en {len(subsets)} subsets\")\n",
    "    \n",
    "    def extract_all_patches(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Extrae todos los patches con etiquetas de malignidad\n",
    "        \n",
    "        Returns:\n",
    "            patches: Lista de arrays (patch_size, patch_size)\n",
    "            labels: Lista de etiquetas (0=benigno, 1=maligno)\n",
    "            metadata: Lista de diccionarios con info adicional\n",
    "        \"\"\"\n",
    "        patches = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        n_excluded = 0  # Score 3 (indeterminado)\n",
    "        n_no_lidc = 0   # Sin anotaciones LIDC\n",
    "        \n",
    "        for seriesuid, mhd_path in tqdm(self.scan_paths.items(), \n",
    "                                         desc=\"Extrayendo patches\",\n",
    "                                         disable=not verbose):\n",
    "            try:\n",
    "                # Obtener nodulos con suficientes anotaciones\n",
    "                reliable_clusters = self.lidc_loader.get_reliable_nodules(\n",
    "                    seriesuid, min_annotations=self.min_radiologists\n",
    "                )\n",
    "                \n",
    "                if not reliable_clusters:\n",
    "                    n_no_lidc += 1\n",
    "                    continue\n",
    "                \n",
    "                # Cargar CT\n",
    "                import SimpleITK as sitk\n",
    "                ct_itk = sitk.ReadImage(str(mhd_path))\n",
    "                ct_array = sitk.GetArrayFromImage(ct_itk)\n",
    "                spacing = np.array(ct_itk.GetSpacing())[::-1]  # (z, y, x)\n",
    "                origin = np.array(ct_itk.GetOrigin())[::-1]    # (z, y, x)\n",
    "                \n",
    "                # Procesar cada nodulo\n",
    "                for cluster in reliable_clusters:\n",
    "                    # Obtener malignidad por consenso\n",
    "                    mal_info = self.lidc_loader.get_cluster_malignancy(cluster)\n",
    "                    mean_mal = mal_info['malignancy_mean']\n",
    "                    \n",
    "                    # Binarizar: 1-2 = benigno, 4-5 = maligno, 3 = excluir\n",
    "                    if mean_mal <= 2.0:\n",
    "                        label = 0  # Benigno\n",
    "                    elif mean_mal >= 4.0:\n",
    "                        label = 1  # Maligno\n",
    "                    else:\n",
    "                        n_excluded += 1\n",
    "                        continue  # Excluir score ~3\n",
    "                    \n",
    "                    # Obtener centro del nodulo desde la mascara\n",
    "                    result = self.lidc_loader.get_aligned_mask_for_cluster(\n",
    "                        cluster, origin, spacing, ct_array.shape, threshold=0.5\n",
    "                    )\n",
    "                    \n",
    "                    if result is None:\n",
    "                        continue\n",
    "                    \n",
    "                    mask, bbox = result\n",
    "                    \n",
    "                    # Calcular centro del nodulo\n",
    "                    z_center = (bbox[0].start + bbox[0].stop) // 2\n",
    "                    y_center = (bbox[1].start + bbox[1].stop) // 2\n",
    "                    x_center = (bbox[2].start + bbox[2].stop) // 2\n",
    "                    \n",
    "                    # Extraer patch 2D del slice central\n",
    "                    patch = self._extract_patch(ct_array, z_center, y_center, x_center)\n",
    "                    \n",
    "                    if patch is not None:\n",
    "                        patches.append(patch)\n",
    "                        labels.append(label)\n",
    "                        metadata.append({\n",
    "                            'seriesuid': seriesuid,\n",
    "                            'z': z_center,\n",
    "                            'y': y_center,\n",
    "                            'x': x_center,\n",
    "                            'malignancy_mean': mean_mal,\n",
    "                            'malignancy_std': mal_info['malignancy_std'],\n",
    "                            'num_radiologists': mal_info['num_radiologists']\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"[ERROR] {seriesuid}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[OK] Extraccion completada:\")\n",
    "            print(f\"    Patches extraidos: {len(patches)}\")\n",
    "            print(f\"    Benignos (0): {sum(1 for l in labels if l == 0)}\")\n",
    "            print(f\"    Malignos (1): {sum(1 for l in labels if l == 1)}\")\n",
    "            print(f\"    Excluidos (score ~3): {n_excluded}\")\n",
    "            print(f\"    Sin anotaciones LIDC: {n_no_lidc}\")\n",
    "        \n",
    "        return np.array(patches), np.array(labels), metadata\n",
    "    \n",
    "    def _extract_patch(self, ct_array, z, y, x):\n",
    "        \"\"\"Extrae un patch 2D centrado en (z, y, x)\"\"\"\n",
    "        half = self.patch_size // 2\n",
    "        \n",
    "        # Verificar limites\n",
    "        if (z < 0 or z >= ct_array.shape[0] or\n",
    "            y - half < 0 or y + half > ct_array.shape[1] or\n",
    "            x - half < 0 or x + half > ct_array.shape[2]):\n",
    "            return None\n",
    "        \n",
    "        patch = ct_array[z, y-half:y+half, x-half:x+half]\n",
    "        \n",
    "        if patch.shape != (self.patch_size, self.patch_size):\n",
    "            return None\n",
    "        \n",
    "        return patch.astype(np.float32)\n",
    "\n",
    "\n",
    "# Crear extractor\n",
    "extractor = LUNALIDCPatchExtractor(\n",
    "    luna16_dir=LUNA16_DIR,\n",
    "    subsets=available_subsets,\n",
    "    patch_size=64,\n",
    "    min_radiologists=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extrayendo patches (esto puede tardar unos minutos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:   3%|▎         | 12/356 [00:09<05:54,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  17%|█▋        | 62/356 [00:51<03:16,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  46%|████▌     | 163/356 [02:23<03:49,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  55%|█████▌    | 196/356 [02:54<01:57,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  63%|██████▎   | 224/356 [03:27<03:36,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  69%|██████▉   | 246/356 [03:55<01:41,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches:  99%|█████████▉| 354/356 [05:42<00:01,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to reduce all groups to <= 4 Annotations.\n",
      "Some nodules may be close and must be grouped manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo patches: 100%|██████████| 356/356 [05:44<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Extraccion completada:\n",
      "    Patches extraidos: 164\n",
      "    Benignos (0): 81\n",
      "    Malignos (1): 83\n",
      "    Excluidos (score ~3): 323\n",
      "    Sin anotaciones LIDC: 106\n",
      "[OK] Cache guardado en: c:\\Users\\Poney\\Desktop\\Imagenes Biomedicas\\data\\luna_lidc_patches.npz\n",
      "\n",
      "Dataset LUNA16+LIDC:\n",
      "  Total patches: 164\n",
      "  Benignos: 81 (49.4%)\n",
      "  Malignos: 83 (50.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraer patches\n",
    "# El cache evita re-extraer si ya existe\n",
    "cache_path = os.path.join(project_root, 'data', 'luna_lidc_patches.npz')\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(f\"[INFO] Cargando patches desde cache: {cache_path}\")\n",
    "    data = np.load(cache_path, allow_pickle=True)\n",
    "    X = data['patches']\n",
    "    y = data['labels']\n",
    "    metadata = data['metadata'].tolist()\n",
    "    print(f\"[OK] Cargados {len(X)} patches desde cache\")\n",
    "else:\n",
    "    print(\"[INFO] Extrayendo patches (esto puede tardar unos minutos)...\")\n",
    "    X, y, metadata = extractor.extract_all_patches()\n",
    "    \n",
    "    # Guardar cache\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "    np.savez(cache_path, patches=X, labels=y, metadata=np.array(metadata, dtype=object))\n",
    "    print(f\"[OK] Cache guardado en: {cache_path}\")\n",
    "\n",
    "print(f\"\\nDataset LUNA16+LIDC:\")\n",
    "print(f\"  Total patches: {len(X)}\")\n",
    "print(f\"  Benignos: {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
    "print(f\"  Malignos: {(y == 1).sum()} ({(y == 1).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PREPARAR DATOS PARA K-FOLD CROSS-VALIDATION POR PACIENTE\n# ============================================================\n# Usamos StratifiedGroupKFold para:\n# 1. Mantener proporcion de clases en cada fold (Stratified)\n# 2. Evitar que patches del mismo paciente esten en train y val (Group)\n\n# Obtener grupos (pacientes) y labels\ngroups = np.array([m['seriesuid'] for m in metadata])\nunique_groups = np.unique(groups)\n\nprint(f\"Total patches: {len(X)}\")\nprint(f\"Pacientes unicos: {len(unique_groups)}\")\nprint(f\"Benignos: {(y == 0).sum()} ({(y == 0).mean():.1%})\")\nprint(f\"Malignos: {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n\n# Configurar K-Fold\nN_FOLDS = 5\nskf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# Mostrar distribucion por fold\nprint(f\"\\nDistribucion por fold (K={N_FOLDS}):\")\nprint(\"-\" * 50)\n\nfolds_info = []\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y, groups)):\n    train_patients = len(np.unique(groups[train_idx]))\n    val_patients = len(np.unique(groups[val_idx]))\n    \n    info = {\n        'fold': fold_idx + 1,\n        'train_samples': len(train_idx),\n        'val_samples': len(val_idx),\n        'train_patients': train_patients,\n        'val_patients': val_patients,\n        'train_benign': (y[train_idx] == 0).sum(),\n        'train_malign': (y[train_idx] == 1).sum(),\n        'val_benign': (y[val_idx] == 0).sum(),\n        'val_malign': (y[val_idx] == 1).sum()\n    }\n    folds_info.append(info)\n    \n    print(f\"Fold {fold_idx+1}: Train {len(train_idx):3d} ({train_patients:2d} pac) | \"\n          f\"Val {len(val_idx):3d} ({val_patients:2d} pac) | \"\n          f\"Val dist: {info['val_benign']}B/{info['val_malign']}M\")\n\nprint(\"-\" * 50)\nprint(\"[OK] K-Fold CV configurado con separacion por paciente\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Dataset y Funciones Auxiliares\n\nDataset de PyTorch con data augmentation moderado para entrenamiento."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class NoduleDataset(Dataset):\n    \"\"\"Dataset de nodulos con data augmentation moderado\"\"\"\n    \n    def __init__(self, patches, labels, augment=False):\n        \"\"\"\n        Args:\n            patches: Array de patches (N, H, W)\n            labels: Array de labels (N,)\n            augment: Aplicar data augmentation\n        \"\"\"\n        self.patches = patches\n        self.labels = labels.astype(np.int64)\n        self.augment = augment\n        \n        # Transformaciones suaves (evitar overfitting en dataset pequeno)\n        if augment:\n            self.transform = transforms.Compose([\n                transforms.RandomRotation(15),  # Reducido de 180\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomVerticalFlip(p=0.5),\n            ])\n        else:\n            self.transform = None\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        patch = self.patches[idx]\n        label = self.labels[idx]\n        \n        # Normalizar a [0, 1]\n        patch = (patch - patch.min()) / (patch.max() - patch.min() + 1e-8)\n        \n        # Convertir a tensor con canal\n        patch = torch.FloatTensor(patch).unsqueeze(0)  # (1, H, W)\n        \n        # Aplicar augmentation\n        if self.transform:\n            patch = self.transform(patch)\n        \n        return patch, label\n\n\n# Funcion auxiliar para crear dataloaders de un fold\ndef create_fold_loaders(X, y, train_idx, val_idx, batch_size=16):\n    \"\"\"\n    Crea DataLoaders para un fold de K-Fold CV\n    \n    Returns:\n        train_loader, val_loader\n    \"\"\"\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    \n    train_dataset = NoduleDataset(X_train, y_train, augment=True)\n    val_dataset = NoduleDataset(X_val, y_val, augment=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    return train_loader, val_loader\n\n\nprint(\"[OK] Dataset y funciones auxiliares definidas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizar ejemplos del dataset\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\naxes = axes.flatten()\n\nclass_names = ['Benigno', 'Maligno']\ncolors = ['green', 'red']\n\n# Crear dataset temporal para visualizacion\ntemp_dataset = NoduleDataset(X, y, augment=False)\n\nfor i in range(10):\n    idx = i * (len(temp_dataset) // 10)\n    patch, label = temp_dataset[idx]\n    \n    axes[i].imshow(patch.squeeze().numpy(), cmap='bone')\n    axes[i].set_title(f'{class_names[label]}', color=colors[label], fontweight='bold')\n    axes[i].axis('off')\n\nplt.suptitle('Ejemplos del Dataset LUNA16+LIDC', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. Estrategia 1: Backbone Congelado + Cabeza Lineal\n\n**ResNet18 pre-entrenado con backbone congelado**:\n- Solo entrenamos la capa final (~1000 parametros vs 11M)\n- Evita overfitting severo en datasets pequenos\n- El backbone actua como extractor de features generalistas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_frozen_resnet18(num_classes=2, dropout=0.5):\n    \"\"\"\n    Crea ResNet18 con backbone congelado y cabeza lineal entrenable\n    \n    Solo ~1000 parametros entrenables (512*2 + 2 = 1026)\n    \"\"\"\n    # Cargar ResNet18 pre-entrenado adaptado para grayscale\n    model = timm.create_model('resnet18', pretrained=True, in_chans=1)\n    \n    # Congelar todo el backbone\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # Reemplazar cabeza con capa entrenable\n    num_features = model.fc.in_features  # 512 para ResNet18\n    model.fc = nn.Sequential(\n        nn.Dropout(dropout),\n        nn.Linear(num_features, num_classes)\n    )\n    \n    # La nueva cabeza SI tiene requires_grad=True por defecto\n    return model\n\n\n# Crear modelo y mostrar info\nmodel_demo = create_frozen_resnet18()\n\nn_total = sum(p.numel() for p in model_demo.parameters())\nn_trainable = sum(p.numel() for p in model_demo.parameters() if p.requires_grad)\nn_frozen = n_total - n_trainable\n\nprint(\"ResNet18 con Backbone Congelado:\")\nprint(f\"  Parametros totales: {n_total:,}\")\nprint(f\"  Parametros congelados: {n_frozen:,}\")\nprint(f\"  Parametros entrenables: {n_trainable:,}\")\nprint(f\"  Reduccion: {n_frozen/n_total:.1%} de parametros NO se entrenan\")\n\ndel model_demo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Entrenamiento con K-Fold Cross-Validation\n\nEntrenamos el modelo con K-Fold CV por paciente y reportamos media +/- std."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Entrena una epoca\"\"\"\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for patches, labels in loader:\n        patches = patches.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(patches)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    return total_loss / len(loader), correct / total\n\n\ndef evaluate(model, loader, device):\n    \"\"\"Evalua el modelo y retorna predicciones\"\"\"\n    model.eval()\n    all_probs = []\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for patches, labels in loader:\n            patches = patches.to(device)\n            \n            outputs = model(patches)\n            probs = F.softmax(outputs, dim=1)[:, 1]\n            _, predicted = outputs.max(1)\n            \n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_preds.extend(predicted.cpu().numpy())\n    \n    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n\n\ndef train_kfold_resnet(X, y, groups, n_folds=5, num_epochs=30, lr=1e-3, batch_size=16, verbose=True):\n    \"\"\"\n    Entrena ResNet18 congelado con K-Fold CV por paciente\n    \n    Returns:\n        fold_results: Lista de diccionarios con metricas por fold\n    \"\"\"\n    skf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n    fold_results = []\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y, groups)):\n        if verbose:\n            print(f\"\\n{'='*50}\")\n            print(f\"Fold {fold_idx+1}/{n_folds}\")\n            print(f\"{'='*50}\")\n        \n        # Crear dataloaders\n        train_loader, val_loader = create_fold_loaders(X, y, train_idx, val_idx, batch_size)\n        \n        # Crear modelo fresco para cada fold\n        model = create_frozen_resnet18().to(device)\n        \n        # Solo optimizar parametros de la cabeza\n        optimizer = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, model.parameters()), \n            lr=lr\n        )\n        criterion = nn.CrossEntropyLoss()\n        \n        best_auc = 0\n        patience = 5\n        patience_counter = 0\n        \n        for epoch in range(num_epochs):\n            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n            y_true, y_pred, y_prob = evaluate(model, val_loader, device)\n            \n            try:\n                val_auc = roc_auc_score(y_true, y_prob)\n            except:\n                val_auc = 0.5\n            \n            val_acc = accuracy_score(y_true, y_pred)\n            \n            if verbose and (epoch + 1) % 5 == 0:\n                print(f\"  Epoch {epoch+1:02d}: Train Loss={train_loss:.3f}, \"\n                      f\"Train Acc={train_acc:.1%}, Val Acc={val_acc:.1%}, Val AUC={val_auc:.3f}\")\n            \n            if val_auc > best_auc:\n                best_auc = val_auc\n                best_results = {'y_true': y_true, 'y_pred': y_pred, 'y_prob': y_prob}\n                patience_counter = 0\n            else:\n                patience_counter += 1\n            \n            if patience_counter >= patience:\n                if verbose:\n                    print(f\"  [INFO] Early stopping at epoch {epoch+1}\")\n                break\n        \n        # Calcular metricas del fold\n        y_true = best_results['y_true']\n        y_pred = best_results['y_pred']\n        y_prob = best_results['y_prob']\n        \n        fold_metrics = {\n            'fold': fold_idx + 1,\n            'auc': roc_auc_score(y_true, y_prob),\n            'accuracy': accuracy_score(y_true, y_pred),\n            'sensitivity': recall_score(y_true, y_pred, pos_label=1),\n            'specificity': recall_score(y_true, y_pred, pos_label=0),\n            'y_true': y_true,\n            'y_pred': y_pred,\n            'y_prob': y_prob\n        }\n        fold_results.append(fold_metrics)\n        \n        if verbose:\n            print(f\"  Fold {fold_idx+1} - AUC: {fold_metrics['auc']:.3f}, \"\n                  f\"Acc: {fold_metrics['accuracy']:.1%}\")\n    \n    return fold_results\n\n\nprint(\"[OK] Funciones de entrenamiento definidas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# ENTRENAR RESNET18 CONGELADO CON K-FOLD CV\n# ============================================================\nprint(\"Entrenando ResNet18 (backbone congelado) con K-Fold CV...\")\nprint(f\"  K = {N_FOLDS} folds\")\nprint(f\"  Epochs max = 30\")\nprint(f\"  Early stopping = 5 epochs\")\n\nresnet_results = train_kfold_resnet(\n    X, y, groups,\n    n_folds=N_FOLDS,\n    num_epochs=30,\n    lr=1e-3,\n    batch_size=16,\n    verbose=True\n)\n\n# Calcular estadisticas\nresnet_aucs = [r['auc'] for r in resnet_results]\nresnet_accs = [r['accuracy'] for r in resnet_results]\nresnet_sens = [r['sensitivity'] for r in resnet_results]\nresnet_spec = [r['specificity'] for r in resnet_results]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTADOS: ResNet18 Backbone Congelado\")\nprint(\"=\"*60)\nprint(f\"  AUC:         {np.mean(resnet_aucs):.3f} +/- {np.std(resnet_aucs):.3f}\")\nprint(f\"  Accuracy:    {np.mean(resnet_accs):.3f} +/- {np.std(resnet_accs):.3f}\")\nprint(f\"  Sensitivity: {np.mean(resnet_sens):.3f} +/- {np.std(resnet_sens):.3f}\")\nprint(f\"  Specificity: {np.mean(resnet_spec):.3f} +/- {np.std(resnet_spec):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 6. Estrategia 2: Features CNN + Clasificadores Clasicos\n\nExtraemos embeddings del backbone y entrenamos clasificadores clasicos:\n- **SVM (RBF)**: Bueno para datos no lineales\n- **Logistic Regression**: Regularizado para evitar overfitting\n- **Random Forest**: Ensemble con profundidad limitada"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def extract_features_resnet(X, batch_size=32):\n    \"\"\"\n    Extrae features usando ResNet18 backbone congelado\n    \n    Returns:\n        features: Array (N, 512) de embeddings\n    \"\"\"\n    # Crear modelo extractor (sin cabeza final)\n    backbone = timm.create_model('resnet18', pretrained=True, in_chans=1, num_classes=0)\n    backbone = backbone.to(device)\n    backbone.eval()\n    \n    # Dataset simple sin augmentation\n    dataset = NoduleDataset(X, np.zeros(len(X)), augment=False)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    features = []\n    with torch.no_grad():\n        for patches, _ in tqdm(loader, desc=\"Extrayendo features\"):\n            patches = patches.to(device)\n            feat = backbone(patches)  # (batch, 512)\n            features.append(feat.cpu().numpy())\n    \n    return np.vstack(features)\n\n\ndef train_kfold_classical(X_features, y, groups, n_folds=5, verbose=True):\n    \"\"\"\n    Entrena clasificadores clasicos con K-Fold CV\n    \n    Returns:\n        results: Dict con resultados por clasificador\n    \"\"\"\n    skf = StratifiedGroupKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n    \n    classifiers = {\n        'SVM (RBF)': SVC(kernel='rbf', C=1.0, probability=True, random_state=SEED),\n        'Logistic Regression': LogisticRegression(C=0.1, max_iter=1000, random_state=SEED),\n        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n    }\n    \n    results = {name: [] for name in classifiers.keys()}\n    \n    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_features, y, groups)):\n        X_train, y_train = X_features[train_idx], y[train_idx]\n        X_val, y_val = X_features[val_idx], y[val_idx]\n        \n        # Normalizar features\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        \n        for name, clf_template in classifiers.items():\n            # Crear nueva instancia del clasificador\n            if 'SVM' in name:\n                clf = SVC(kernel='rbf', C=1.0, probability=True, random_state=SEED)\n            elif 'Logistic' in name:\n                clf = LogisticRegression(C=0.1, max_iter=1000, random_state=SEED)\n            else:\n                clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n            \n            # Entrenar\n            clf.fit(X_train_scaled, y_train)\n            \n            # Evaluar\n            y_pred = clf.predict(X_val_scaled)\n            y_prob = clf.predict_proba(X_val_scaled)[:, 1]\n            \n            try:\n                auc = roc_auc_score(y_val, y_prob)\n            except:\n                auc = 0.5\n            \n            fold_metrics = {\n                'fold': fold_idx + 1,\n                'auc': auc,\n                'accuracy': accuracy_score(y_val, y_pred),\n                'sensitivity': recall_score(y_val, y_pred, pos_label=1),\n                'specificity': recall_score(y_val, y_pred, pos_label=0),\n                'y_true': y_val,\n                'y_pred': y_pred,\n                'y_prob': y_prob\n            }\n            results[name].append(fold_metrics)\n        \n        if verbose:\n            print(f\"Fold {fold_idx+1}/{n_folds} completado\")\n    \n    return results\n\n\nprint(\"[OK] Funciones de extraccion de features definidas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXTRAER FEATURES Y ENTRENAR CLASIFICADORES CLASICOS\n# ============================================================\nprint(\"Extrayendo features con ResNet18 backbone...\")\nX_features = extract_features_resnet(X)\nprint(f\"[OK] Features extraidas: {X_features.shape}\")\n\nprint(\"\\nEntrenando clasificadores clasicos con K-Fold CV...\")\nclassical_results = train_kfold_classical(X_features, y, groups, n_folds=N_FOLDS, verbose=True)\n\n# Mostrar resultados\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTADOS: Clasificadores Clasicos (Features CNN)\")\nprint(\"=\"*60)\n\nfor clf_name, fold_results in classical_results.items():\n    aucs = [r['auc'] for r in fold_results]\n    accs = [r['accuracy'] for r in fold_results]\n    sens = [r['sensitivity'] for r in fold_results]\n    spec = [r['specificity'] for r in fold_results]\n    \n    print(f\"\\n{clf_name}:\")\n    print(f\"  AUC:         {np.mean(aucs):.3f} +/- {np.std(aucs):.3f}\")\n    print(f\"  Accuracy:    {np.mean(accs):.3f} +/- {np.std(accs):.3f}\")\n    print(f\"  Sensitivity: {np.mean(sens):.3f} +/- {np.std(sens):.3f}\")\n    print(f\"  Specificity: {np.mean(spec):.3f} +/- {np.std(spec):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 7. Estrategia 3: Radiomics Baseline (Features interpretables)\n\nFeatures clasicas de imagen medica sin deep learning:\n- **Intensidad**: estadisticas basicas (media, std, percentiles)\n- **Textura GLCM**: contraste, homogeneidad, energia, correlacion\n- **Histograma**: skewness, kurtosis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from skimage.feature import graycomatrix, graycoprops\nfrom scipy.stats import skew, kurtosis\n\n\ndef extract_radiomics(patch):\n    \"\"\"\n    Extrae features radiomicas de un patch 2D\n    \n    Returns:\n        Dict con todas las features\n    \"\"\"\n    features = {}\n    \n    # Normalizar a 0-255 para GLCM\n    patch_norm = patch - patch.min()\n    if patch_norm.max() > 0:\n        patch_norm = patch_norm / patch_norm.max()\n    patch_uint8 = (patch_norm * 255).astype(np.uint8)\n    \n    # --- INTENSIDAD ---\n    features['intensity_mean'] = float(patch.mean())\n    features['intensity_std'] = float(patch.std())\n    features['intensity_min'] = float(patch.min())\n    features['intensity_max'] = float(patch.max())\n    features['intensity_p25'] = float(np.percentile(patch, 25))\n    features['intensity_p50'] = float(np.percentile(patch, 50))\n    features['intensity_p75'] = float(np.percentile(patch, 75))\n    \n    # --- HISTOGRAMA ---\n    features['skewness'] = float(skew(patch.flatten()))\n    features['kurtosis'] = float(kurtosis(patch.flatten()))\n    \n    # --- TEXTURA GLCM ---\n    # Calcular GLCM en 4 direcciones (0, 45, 90, 135 grados)\n    try:\n        glcm = graycomatrix(patch_uint8, distances=[1], \n                           angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n                           levels=256, symmetric=True, normed=True)\n        \n        features['glcm_contrast'] = float(graycoprops(glcm, 'contrast').mean())\n        features['glcm_homogeneity'] = float(graycoprops(glcm, 'homogeneity').mean())\n        features['glcm_energy'] = float(graycoprops(glcm, 'energy').mean())\n        features['glcm_correlation'] = float(graycoprops(glcm, 'correlation').mean())\n        features['glcm_dissimilarity'] = float(graycoprops(glcm, 'dissimilarity').mean())\n    except:\n        # Valores por defecto si falla\n        features['glcm_contrast'] = 0.0\n        features['glcm_homogeneity'] = 1.0\n        features['glcm_energy'] = 1.0\n        features['glcm_correlation'] = 0.0\n        features['glcm_dissimilarity'] = 0.0\n    \n    return features\n\n\ndef extract_all_radiomics(X):\n    \"\"\"Extrae features radiomicas de todos los patches\"\"\"\n    all_features = []\n    \n    for i, patch in enumerate(tqdm(X, desc=\"Extrayendo radiomics\")):\n        features = extract_radiomics(patch)\n        all_features.append(features)\n    \n    # Convertir a DataFrame\n    df = pd.DataFrame(all_features)\n    return df.values, list(df.columns)\n\n\nprint(\"[OK] Funciones de radiomics definidas\")\nprint(f\"Features por patch: {len(extract_radiomics(X[0]))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXTRAER RADIOMICS Y ENTRENAR CLASIFICADORES\n# ============================================================\nprint(\"Extrayendo features radiomicas...\")\nX_radiomics, feature_names = extract_all_radiomics(X)\nprint(f\"[OK] Features extraidas: {X_radiomics.shape}\")\nprint(f\"Features: {feature_names}\")\n\nprint(\"\\nEntrenando clasificadores con Radiomics...\")\nradiomics_results = train_kfold_classical(X_radiomics, y, groups, n_folds=N_FOLDS, verbose=True)\n\n# Mostrar resultados\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTADOS: Clasificadores con Radiomics\")\nprint(\"=\"*60)\n\nfor clf_name, fold_results in radiomics_results.items():\n    aucs = [r['auc'] for r in fold_results]\n    accs = [r['accuracy'] for r in fold_results]\n    sens = [r['sensitivity'] for r in fold_results]\n    spec = [r['specificity'] for r in fold_results]\n    \n    print(f\"\\n{clf_name}:\")\n    print(f\"  AUC:         {np.mean(aucs):.3f} +/- {np.std(aucs):.3f}\")\n    print(f\"  Accuracy:    {np.mean(accs):.3f} +/- {np.std(accs):.3f}\")\n    print(f\"  Sensitivity: {np.mean(sens):.3f} +/- {np.std(sens):.3f}\")\n    print(f\"  Specificity: {np.mean(spec):.3f} +/- {np.std(spec):.3f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 8. Comparacion de Todas las Estrategias\n\nTabla resumen con media +/- std de AUC sobre K folds.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# TABLA COMPARATIVA DE TODAS LAS ESTRATEGIAS\n# ============================================================\n\ndef summarize_results(results_list, name):\n    \"\"\"Resume metricas de una lista de resultados por fold\"\"\"\n    aucs = [r['auc'] for r in results_list]\n    accs = [r['accuracy'] for r in results_list]\n    sens = [r['sensitivity'] for r in results_list]\n    spec = [r['specificity'] for r in results_list]\n    return {\n        'Metodo': name,\n        'AUC': f\"{np.mean(aucs):.3f} +/- {np.std(aucs):.3f}\",\n        'Accuracy': f\"{np.mean(accs):.3f} +/- {np.std(accs):.3f}\",\n        'Sensitivity': f\"{np.mean(sens):.3f} +/- {np.std(sens):.3f}\",\n        'Specificity': f\"{np.mean(spec):.3f} +/- {np.std(spec):.3f}\",\n        'auc_mean': np.mean(aucs)  # Para ordenar\n    }\n\n# Recopilar todos los resultados\nall_results = []\n\n# 1. ResNet18 Backbone Congelado\nall_results.append(summarize_results(resnet_results, 'ResNet18 (frozen backbone)'))\n\n# 2. Clasificadores con features CNN\nfor clf_name, fold_results in classical_results.items():\n    all_results.append(summarize_results(fold_results, f'CNN Features + {clf_name}'))\n\n# 3. Clasificadores con Radiomics\nfor clf_name, fold_results in radiomics_results.items():\n    all_results.append(summarize_results(fold_results, f'Radiomics + {clf_name}'))\n\n# Ordenar por AUC descendente\nall_results.sort(key=lambda x: x['auc_mean'], reverse=True)\n\n# Crear DataFrame\ndf_results = pd.DataFrame(all_results)\ndf_results = df_results.drop(columns=['auc_mean'])\n\nprint(\"=\"*80)\nprint(\"COMPARACION DE TODAS LAS ESTRATEGIAS\")\nprint(f\"K-Fold Cross-Validation con K={N_FOLDS} (separacion por paciente)\")\nprint(\"=\"*80)\nprint()\nprint(df_results.to_string(index=False))\nprint()\nprint(\"-\"*80)\nprint(f\"Mejor metodo: {all_results[0]['Metodo']}\")\nprint(f\"Mejor AUC: {all_results[0]['AUC']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# VISUALIZACION: CURVAS ROC COMPARATIVAS\n# ============================================================\n\n# Preparar datos para ROC (usamos el ultimo fold como ejemplo representativo)\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# --- Panel 1: Deep Learning vs Clasificadores Clasicos ---\nax1 = axes[0]\n\n# ResNet18 (promediamos sobre folds)\ncolors_dl = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']\nmethods_dl = [\n    ('ResNet18 (frozen)', resnet_results),\n    ('CNN + SVM', classical_results['SVM (RBF)']),\n    ('CNN + LogReg', classical_results['Logistic Regression']),\n    ('CNN + RF', classical_results['Random Forest'])\n]\n\nfor i, (name, results) in enumerate(methods_dl):\n    # Concatenar y_true y y_prob de todos los folds\n    y_true_all = np.concatenate([r['y_true'] for r in results])\n    y_prob_all = np.concatenate([r['y_prob'] for r in results])\n    \n    fpr, tpr, _ = roc_curve(y_true_all, y_prob_all)\n    auc = roc_auc_score(y_true_all, y_prob_all)\n    ax1.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=colors_dl[i], linewidth=2)\n\nax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC=0.500)')\nax1.set_xlabel('False Positive Rate', fontsize=12)\nax1.set_ylabel('True Positive Rate', fontsize=12)\nax1.set_title('Deep Learning: Backbone Congelado vs Features+Clasificadores', fontsize=11)\nax1.legend(loc='lower right')\nax1.grid(True, alpha=0.3)\n\n# --- Panel 2: Radiomics Baseline ---\nax2 = axes[1]\n\ncolors_rad = ['#e67e22', '#16a085', '#8e44ad']\nmethods_rad = [\n    ('Radiomics + SVM', radiomics_results['SVM (RBF)']),\n    ('Radiomics + LogReg', radiomics_results['Logistic Regression']),\n    ('Radiomics + RF', radiomics_results['Random Forest'])\n]\n\nfor i, (name, results) in enumerate(methods_rad):\n    y_true_all = np.concatenate([r['y_true'] for r in results])\n    y_prob_all = np.concatenate([r['y_prob'] for r in results])\n    \n    fpr, tpr, _ = roc_curve(y_true_all, y_prob_all)\n    auc = roc_auc_score(y_true_all, y_prob_all)\n    ax2.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=colors_rad[i], linewidth=2)\n\nax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC=0.500)')\nax2.set_xlabel('False Positive Rate', fontsize=12)\nax2.set_ylabel('True Positive Rate', fontsize=12)\nax2.set_title('Radiomics Baseline (Features Interpretables)', fontsize=11)\nax2.legend(loc='lower right')\nax2.grid(True, alpha=0.3)\n\nplt.suptitle('Comparacion de Curvas ROC - Clasificacion de Nodulos Pulmonares', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n[OK] Notebook completado\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}